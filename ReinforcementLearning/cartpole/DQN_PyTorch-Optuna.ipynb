{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10,\n",
    "                 name='QNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_episodes = 300          # max number of episodes to learn from\\nmax_steps = 200                # max steps in an episode\\ngamma = 0.99                   # future reward discount\\n\\naction_size = 2\\n\\n# Exploration parameters\\nexplore_start = 0.2            # exploration probability at start\\nexplore_stop = 0.01            # minimum exploration probability \\ndecay_rate = 0.0001            # exponential decay rate for exploration prob\\n\\n# Network parameters\\nhidden_size = 16               # number of units in each Q-network hidden layer\\nlearning_rate = 0.0001         # Q-network learning rate\\n\\n# Memory parameters\\nmemory_size = 200000            # memory capacity\\nbatch_size = 512                # experience mini-batch size\\npretrain_length = batch_size   # number experiences to pretrain the memory\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_episodes = 300          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "action_size = 2\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.2            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 200000            # memory capacity\n",
    "batch_size = 512                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env.reset()\n",
    "values = []\n",
    "for i in range(100000):\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    values.append(state)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "values_array = np.array(values)\n",
    "means = values_array.mean(axis=0)\n",
    "stds = values_array.std(axis=0)\n",
    "print(means)\n",
    "print(stds)\n",
    "\n",
    "corr_matrix = np.corrcoef(((values_array-means)/stds).T)\n",
    "print(corr_matrix)\n",
    "eigen_values,eigen_vectors = np.linalg.eig(corr_matrix)\n",
    "print(eigen_values)\n",
    "print(eigen_vectors)\n",
    "\"\"\"\n",
    "means = [-0.00029514, -0.00455431, -0.00026341, 0.00259509]\n",
    "stds = [0.09655688, 0.5670828, 0.10286357, 0.85574364]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(observation, means, stds):\n",
    "    # return np.dot((np.array(observation) - means)/stds, eigen_vectors.T)/eigen_values\n",
    "    return (np.array(observation) - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit(env, train_episodes = 300, max_steps = 200, gamma = 0.99, action_size = 2,\n",
    "        explore_start = 0.2, explore_stop = 0.01, decay_rate = 0.0001,\n",
    "        hidden_size = 16, learning_rate = 0.0001,\n",
    "        memory_size = 200000,batch_size = 512):\n",
    "\n",
    "    means = [-0.00029514, -0.00455431, -0.00026341, 0.00259509]\n",
    "    stds = [0.09655688, 0.5670828, 0.10286357, 0.85574364]\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "    \n",
    "    # Initialize the simulation\n",
    "    env.reset()\n",
    "    # Take one random step to get the pole and cart moving\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    state = make_state(observation, means, stds)\n",
    "\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    for ii in range(batch_size - 1):\n",
    "        # Uncomment the line below to watch the simulation\n",
    "        # env.render()\n",
    "\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        next_state = make_state(next_observation, means, stds)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, 0, next_state, done))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "            state = make_state(observation, means, stds)\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, 0, next_state, done))\n",
    "            state = next_state\n",
    "            \n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    #saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    step = 0\n",
    "    opt = optim.Adam(mainQN.parameters(), learning_rate)\n",
    "\n",
    "    outputs = np.empty([1,6])\n",
    "\n",
    "    count_stop = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = make_state(observation, means, stds)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            step += 1\n",
    "\n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                Qs = mainQN(Variable(torch.FloatTensor(state))).data.numpy()\n",
    "                action = np.argmax(Qs)\n",
    "\n",
    "            result = np.hstack((state, mainQN(Variable(torch.FloatTensor(state))).data.numpy()))\n",
    "            outputs = np.vstack((outputs, result))\n",
    "\n",
    "            # Take action, get new state and reward\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            next_state = make_state(next_observation, means, stds)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, 0, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            ### ポイント！！！\n",
    "            # actionはスカラーなのでベクトルにする\n",
    "            # actionsはベクトルでなく、statesと同じ行列\n",
    "            actions = np.array([[each[1]] for each in batch])\n",
    "            ### ポイント終わり\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "\n",
    "            # Train network\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s==False, dones)), dtype=torch.uint8)\n",
    "            # 終端状態のQ値はその後の報酬が存在しないためゼロとする\n",
    "            target_maxQs = torch.zeros(batch_size)\n",
    "            target_maxQs[non_final_mask] = mainQN(Variable(torch.FloatTensor(next_states)[non_final_mask])).max(1)[0].detach()\n",
    "\n",
    "            #tutorial way\n",
    "            targets = (torch.FloatTensor(rewards) + gamma * target_maxQs).unsqueeze(1)\n",
    "\n",
    "            current_q_values = mainQN(Variable(torch.FloatTensor(states))).gather(1, torch.LongTensor(actions))\n",
    "            loss = torch.nn.SmoothL1Loss()(current_q_values, targets)\n",
    "            # backpropagation of loss to NN\n",
    "            # 勾配を初期化\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                \"\"\"\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss.data.numpy()),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                \"\"\"\n",
    "                rewards_list.append(total_reward)\n",
    "                break\n",
    "                \n",
    "        if np.array(rewards_list[-5:]).mean() > 195:\n",
    "            break\n",
    "            \n",
    "    return len(rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    # 変数xに-10から10までの値の中から最適な値を算出してもらう\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 2, 7)\n",
    "    hidden_size = 2**hidden_size\n",
    "    batch_size = trial.suggest_int('batch_size', 5, 10)\n",
    "    batch_size = 2**batch_size\n",
    "\n",
    "    return fit(env, train_episodes = 300, max_steps = 200, gamma = 0.99, action_size = 2,\n",
    "        explore_start = 0.2, explore_stop = 0.01, decay_rate = 0.0001,\n",
    "        hidden_size = hidden_size, learning_rate = learning_rate,\n",
    "        memory_size = 200000, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-10-06 08:00:40,444] Finished trial#20 resulted in value: 281.0. Current best value is 86.0 with parameters: {'hidden_size': 6, 'learning_rate': 3.566430055143271e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:06:15,237] Finished trial#21 resulted in value: 219.0. Current best value is 86.0 with parameters: {'hidden_size': 6, 'learning_rate': 3.566430055143271e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:07:41,760] Finished trial#22 resulted in value: 67.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:09:37,798] Finished trial#23 resulted in value: 124.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:15:48,282] Finished trial#24 resulted in value: 299.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:17:21,172] Finished trial#25 resulted in value: 299.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:18:37,711] Finished trial#26 resulted in value: 77.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:19:03,482] Finished trial#27 resulted in value: 299.0. Current best value is 67.0 with parameters: {'hidden_size': 6, 'learning_rate': 2.6265809914139044e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:19:28,387] Finished trial#28 resulted in value: 42.0. Current best value is 42.0 with parameters: {'hidden_size': 5, 'learning_rate': 0.00011728706029996088, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:19:46,379] Finished trial#29 resulted in value: 299.0. Current best value is 42.0 with parameters: {'hidden_size': 5, 'learning_rate': 0.00011728706029996088, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:20:42,596] Finished trial#30 resulted in value: 185.0. Current best value is 42.0 with parameters: {'hidden_size': 5, 'learning_rate': 0.00011728706029996088, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:21:06,395] Finished trial#31 resulted in value: 25.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:23:03,540] Finished trial#32 resulted in value: 119.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:23:49,057] Finished trial#33 resulted in value: 299.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:24:19,441] Finished trial#34 resulted in value: 299.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:25:14,005] Finished trial#35 resulted in value: 40.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:25:59,738] Finished trial#36 resulted in value: 299.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:26:49,575] Finished trial#37 resulted in value: 299.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:29:16,410] Finished trial#38 resulted in value: 150.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n",
      "[I 2019-10-06 08:29:46,712] Finished trial#39 resulted in value: 299.0. Current best value is 25.0 with parameters: {'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_{'hidden_size': 6, 'learning_rate': 8.106471739895323e-05, 'batch_size': 10}\n",
      "value_25.0\n"
     ]
    }
   ],
   "source": [
    "# optimizeの第一引数に対象のメソッドを指定、n_trialsにプログラムが試行錯誤する回数を指定\n",
    "#study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# プログラムが試行錯誤した結果最も良いパラメータを表示\n",
    "print(\"params_{}\".format(study.best_params))\n",
    "# 最も良いパラメータで実行したときの結果（返り値）を表示\n",
    "print(\"value_{}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
