{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym, time\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, \n",
    "                 action_size=1, hidden_size=10, batch_size=20,\n",
    "                 name='QNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, \n",
    "                 action_size=1, hidden_size=10, batch_size=20,\n",
    "                 name='PolicyNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 500          # max number of episodes to learn from\n",
    "max_steps = 1000                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "action_size = 1\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.2            # exploration probability at start\n",
    "explore_stop = 0.001            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.01         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 200000            # memory capacity\n",
    "batch_size = 512                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(memory_size)\n",
    "q_network = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate,batch_size=batch_size)\n",
    "policy_network = PolicyNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: -0.38817920569993836 Training loss: -0.0145\n",
      "Episode: 2 Total reward: -0.374665107816222 Training loss: -0.0148\n",
      "Episode: 3 Total reward: -0.4253035545667357 Training loss: -0.0088\n",
      "Episode: 4 Total reward: -0.6754640003929695 Training loss: 0.0117\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "#saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "step = 0\n",
    "opt_q = optim.Adam(q_network.parameters(), learning_rate/5.0)\n",
    "opt_policy = optim.Adam(policy_network.parameters(), learning_rate)\n",
    "\n",
    "outputs = deque(maxlen=10000)\n",
    "\n",
    "count_stop = 0\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    # Start new episode\n",
    "    state = env.reset()\n",
    "            \n",
    "    for t in range(max_steps):\n",
    "        action = policy_network(Variable(torch.FloatTensor(state))).data.numpy()\n",
    "        epsilon = max(explore_stop, explore_start*(50.0 - ep)/50.0)\n",
    "        if np.random.rand() < explore_start:\n",
    "            action += 0.2*np.random.rand()\n",
    "        action = np.clip(action, -1, 1)\n",
    "            \n",
    "        result = np.hstack((state, action))\n",
    "        outputs.append(result)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if len(memory.buffer) >= batch_size:\n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            ### ポイント！！！\n",
    "            # actionはスカラーなのでベクトルにする\n",
    "            # actionsはベクトルでなく、statesと同じ行列\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            ### ポイント終わり\n",
    "            rewards = np.array([[each[2]] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            dones = np.array([[each[4]] for each in batch])\n",
    "\n",
    "            # Train network\n",
    "            #non_final_mask = torch.tensor(tuple(map(lambda s: s==False, dones)), dtype=torch.uint8)\n",
    "            # 終端状態のQ値はその後の報酬が存在しないためゼロとする\n",
    "            #target_maxQs = torch.zeros(batch_size)\n",
    "            #target_maxQs[non_final_mask] = q_network(Variable(torch.FloatTensor(next_states)[non_final_mask])).max(1)[0].detach()\n",
    "\n",
    "            #tutorial way\n",
    "            next_actions = policy_network(Variable(torch.FloatTensor(next_states))).detach()\n",
    "            next_Qs = q_network(torch.cat([torch.FloatTensor(next_states), next_actions], -1)).detach().numpy()\n",
    "            targets = (torch.FloatTensor(rewards) + gamma * torch.FloatTensor(next_Qs*(1-dones)))\n",
    "\n",
    "            current_q_values = q_network(torch.cat([torch.FloatTensor(states), torch.FloatTensor(actions)], -1))\n",
    "\n",
    "            critic_loss = torch.nn.SmoothL1Loss()(current_q_values, targets)\n",
    "            # backpropagation of loss to NN\n",
    "            # 勾配を初期化\n",
    "            opt_q.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            opt_q.step()\n",
    "\n",
    "            #print(loss)\n",
    "\n",
    "            actor_loss = -q_network(torch.cat([torch.FloatTensor(states), policy_network(Variable(torch.FloatTensor(states)))], -1)).mean()\n",
    "            opt_policy.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            opt_policy.step()\n",
    "\n",
    "            #print(loss)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    # the episode ends so no next state\n",
    "    print('Episode: {}'.format(ep),\n",
    "          'Total reward: {}'.format(total_reward),\n",
    "          'Training loss: {:.4f}'.format(actor_loss.data.numpy()))\n",
    "    rewards_list.append((ep, total_reward))\n",
    "df = pd.DataFrame(outputs)\n",
    "df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network(torch.cat([torch.FloatTensor(states), policy_network(Variable(torch.FloatTensor(states)))], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
