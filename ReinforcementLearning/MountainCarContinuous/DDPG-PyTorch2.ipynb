{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym, time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, \n",
    "                 action_size=1, hidden_size=10, batch_size=20,\n",
    "                 name='QNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, \n",
    "                 action_size=1, hidden_size=10, batch_size=20,\n",
    "                 name='PolicyNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 300          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "action_size = 1\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.2            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 200000            # memory capacity\n",
    "batch_size = 512                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate,batch_size=batch_size)\n",
    "policy_netowork = PolicyNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.22442716e-01 -3.38304673e-05]\n",
      "[0.17662558 0.01380657]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "values = []\n",
    "for i in range(100000):\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    values.append(observation)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "values_array = np.array(values)\n",
    "means = values_array.mean(axis=0)\n",
    "stds = values_array.std(axis=0)\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(observation, means, stds):\n",
    "    return (np.array(observation) - means)/stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = make_state(observation, means, stds)\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length - 1):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "    next_state = make_state(next_observation, means, stds)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = make_state(observation, means, stds)\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Now train with experiences\n",
    "#saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "step = 0\n",
    "opt_q = optim.Adam(q_network.parameters(), learning_rate)\n",
    "opt_policy = optim.Adam(policy_network.parameters(), learning_rate)\n",
    "\n",
    "outputs = np.empty([1,6])\n",
    "\n",
    "count_stop = 0\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    # Start new episode\n",
    "    env.reset()\n",
    "    # Take one random step to get the pole and cart moving\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    state = make_state(observation, means, stds)\n",
    "            \n",
    "    for t in range(max_steps):\n",
    "        step += 1\n",
    "\n",
    "        action = policy_network(Variable(torch.FloatTensor(state))).data.numpy()\n",
    "            \n",
    "        result = np.hstack((state, action))\n",
    "        outputs = np.vstack((outputs, result))\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        next_state = make_state(next_observation, means, stds)\n",
    "\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Sample mini-batch from memory\n",
    "        batch = memory.sample(batch_size)\n",
    "        states_actions = np.array([each[0] for each in batch])\n",
    "        ### ポイント！！！\n",
    "        # actionはスカラーなのでベクトルにする\n",
    "        # actionsはベクトルでなく、statesと同じ行列\n",
    "        actions = np.array([[each[1]] for each in batch])\n",
    "        ### ポイント終わり\n",
    "        rewards = np.array([each[2] for each in batch])\n",
    "        next_states = np.array([each[3] for each in batch])\n",
    "        dones = np.array([each[4] for each in batch])\n",
    "\n",
    "        # Train network\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s==False, dones)), dtype=torch.uint8)\n",
    "        # 終端状態のQ値はその後の報酬が存在しないためゼロとする\n",
    "        target_maxQs = torch.zeros(batch_size)\n",
    "        target_maxQs[non_final_mask] = mainQN(Variable(torch.FloatTensor(next_states)[non_final_mask])).max(1)[0].detach()\n",
    "\n",
    "        #tutorial way\n",
    "        next_actions = policy_network(Variable(torch.FloatTensor(next_states))).detach()\n",
    "        next_Qs = q_network(torch.cat([torch.FloatTensor(next_states), next_actions], -1))\n",
    "        targets = (torch.FloatTensor(rewards) + gamma * q_network(Variable(torch.FloatTensor(torch.cat())))).unsqueeze(1)\n",
    "\n",
    "        current_q_values = q_network(Variable(torch.FloatTensor(states_actions)))\n",
    "        loss = torch.nn.SmoothL1Loss()(current_q_values, targets)\n",
    "        # backpropagation of loss to NN\n",
    "        # 勾配を初期化\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss.data.numpy()),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "            rewards_list.append((ep, total_reward))\n",
    "            break\n",
    "df = pd.DataFrame(outputs)\n",
    "df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.FloatTensor([True, False, False, True])\n",
    "b=torch.FloatTensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(a,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
