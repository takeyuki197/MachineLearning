{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"core.py\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "\n",
    "\"\"\"ddpg.py\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym, time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"MountainCarContinuous-v0\")\n",
    "test_env = gym.envs.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class continuous_policy(nn.Module):\n",
    "    def __init__(self, act_dim, obs_dim, hidden_layer=(400,300)):\n",
    "        super().__init__()\n",
    "        layer = [nn.Linear(obs_dim, hidden_layer[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_layer)):\n",
    "            layer.append(nn.Linear(hidden_layer[i-1], hidden_layer[i]))\n",
    "            layer.append(nn.ReLU())\n",
    "        layer.append(nn.Linear(hidden_layer[-1], act_dim))\n",
    "        layer.append(nn.Tanh())\n",
    "        self.policy = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.policy(obs)\n",
    "\n",
    "class q_function(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_layer=(400,300)):\n",
    "        super().__init__()\n",
    "        layer = [nn.Linear(obs_dim, hidden_layer[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_layer)):\n",
    "            layer.append(nn.Linear(hidden_layer[i-1], hidden_layer[i]))\n",
    "            layer.append(nn.ReLU())\n",
    "        layer.append(nn.Linear(hidden_layer[-1], 1))\n",
    "        self.policy = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.policy(obs)\n",
    "\n",
    "class actor_critic(nn.Module):\n",
    "    def __init__(self, act_dim, obs_dim, hidden_layer=(400,300), act_limit=2):\n",
    "        super().__init__()\n",
    "        self.policy = continuous_policy(act_dim, obs_dim, hidden_layer)\n",
    "\n",
    "        self.q = q_function(obs_dim+act_dim, hidden_layer)\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.policy_targ = continuous_policy(act_dim, obs_dim, hidden_layer)\n",
    "        self.q_targ = q_function(obs_dim+act_dim, hidden_layer)\n",
    "\n",
    "        self.copy_param()\n",
    "\n",
    "    def copy_param(self):\n",
    "        self.policy_targ.load_state_dict(self.policy.state_dict())\n",
    "        self.q_targ.load_state_dict(self.q.state_dict())\n",
    "\n",
    "    def get_action(self, obs, noise_scale):\n",
    "        pi = self.act_limit * self.policy(obs)\n",
    "        pi += noise_scale * torch.randn_like(pi)\n",
    "        pi.clamp_(max=self.act_limit, min=-self.act_limit)\n",
    "        return pi.squeeze()\n",
    "\n",
    "    def update_target(self, rho):\n",
    "        # compute rho * targ_p + (1 - rho) * main_p\n",
    "        for poly_p, poly_targ_p in zip(self.policy.parameters(), self.policy_targ.parameters()):\n",
    "            poly_targ_p.data = rho * poly_targ_p.data + (1-rho) * poly_p.data\n",
    "\n",
    "        for q_p, q_targ_p in zip(self.q.parameters(), self.q_targ.parameters()):\n",
    "            q_targ_p.data = rho * q_targ_p.data + (1-rho) * q_p.data\n",
    "\n",
    "    def compute_target(self, obs, gamma, rewards, done):\n",
    "        # compute r + gamma * (1 - d) * Q(s', mu_targ(s'))\n",
    "        pi = self.act_limit * self.policy_targ(obs)\n",
    "        return (rewards + gamma * (1-done) * self.q_targ(torch.cat([obs, pi], -1)).squeeze()).detach()\n",
    "\n",
    "    def q_function(self, obs, detach=True, action=None):\n",
    "        # compute Q(s, a) or Q(s, mu(s))\n",
    "        if action is None:\n",
    "            pi = self.act_limit * self.policy(obs)\n",
    "        else:\n",
    "            pi = action\n",
    "        if detach:\n",
    "            pi = pi.detach()\n",
    "        return self.q(torch.cat([obs, pi], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size, self.max_size = 0, size\n",
    "        self.obs1_buf = []\n",
    "        self.obs2_buf = []\n",
    "        self.acts_buf = []\n",
    "        self.rews_buf = []\n",
    "        self.done_buf = []\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf.append(obs)\n",
    "        self.obs2_buf.append(next_obs)\n",
    "        self.acts_buf.append(act)\n",
    "        self.rews_buf.append(rew)\n",
    "        self.done_buf.append(int(done))\n",
    "        while len(self.obs1_buf) > self.max_size:\n",
    "            self.obs1_buf.pop(0)\n",
    "            self.obs2_buf.pop(0)\n",
    "            self.acts_buf.pop(0)\n",
    "            self.rews_buf.pop(0)\n",
    "            self.done_buf.pop(0)\n",
    "\n",
    "        self.size = len(self.obs1_buf)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(low=0, high=self.size, size=(batch_size,))\n",
    "        obs1 = torch.FloatTensor([self.obs1_buf[i] for i in idxs])\n",
    "        obs2 = torch.FloatTensor([self.obs2_buf[i] for i in idxs])\n",
    "        acts = torch.FloatTensor([self.acts_buf[i] for i in idxs])\n",
    "        rews = torch.FloatTensor([self.rews_buf[i] for i in idxs])\n",
    "        done = torch.FloatTensor([self.done_buf[i] for i in idxs])\n",
    "        return [obs1, obs2, acts, rews, done]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9e4c75b67942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mact_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mactor_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mq_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_size' is not defined"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(replay_size)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "act_limit = int(env.action_space.high[0])\n",
    "\n",
    "actor_critic = actor_critic(act_dim, obs_dim, hidden_size, act_limit)\n",
    "\n",
    "q_optimizer = optim.Adam(actor_critic.q.parameters(), q_lr)\n",
    "policy_optimizer = optim.Adam(actor_critic.policy.parameters(), pi_lr)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "obs, ret, done, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "for t in range(total_steps):\n",
    "    if t > 50000:\n",
    "        env.render()\n",
    "    if t > start_steps:\n",
    "        obs_tens = torch.from_numpy(obs).float().reshape(1,-1)\n",
    "        act = actor_critic.get_action(obs_tens, act_noise).detach().numpy().reshape(-1)\n",
    "    else:\n",
    "        act = env.action_space.sample()\n",
    "\n",
    "    obs2, ret, done, _ = env.step(act)\n",
    "\n",
    "    ep_ret += ret\n",
    "    ep_len += 1\n",
    "\n",
    "    done = False if ep_len==max_ep_len else done\n",
    "\n",
    "    replay_buffer.store(obs, act, ret, obs2, done)\n",
    "\n",
    "    obs = obs2\n",
    "\n",
    "    if done or (ep_len == max_ep_len):\n",
    "        for _ in range(ep_len):\n",
    "            obs1_tens, obs2_tens, acts_tens, rews_tens, done_tens = replay_buffer.sample_batch(batch_size)\n",
    "            # compute Q(s, a)\n",
    "            q = actor_critic.q_function(obs1_tens, action=acts_tens)\n",
    "            # compute r + gamma * (1 - d) * Q(s', mu_targ(s'))\n",
    "            q_targ = actor_critic.compute_target(obs2_tens, gamma, rews_tens, done_tens)\n",
    "            # compute (Q(s, a) - y(r, s', d))^2\n",
    "            q_loss = (q.squeeze()-q_targ).pow(2).mean()\n",
    "\n",
    "            q_optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            logger.store(LossQ=q_loss.item(), QVals=q.detach().numpy())\n",
    "\n",
    "            # compute Q(s, mu(s))\n",
    "            policy_loss = -actor_critic.q_function(obs1_tens, detach=False).mean()\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            logger.store(LossPi=policy_loss.item())\n",
    "\n",
    "            # compute rho * targ_p + (1 - rho) * main_p\n",
    "            actor_critic.update_target(polyak)\n",
    "\n",
    "        obs, ret, done, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "    if t > 0 and t % steps_per_epoch == 0:\n",
    "        epoch = t // steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
