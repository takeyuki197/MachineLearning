{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import envs.TradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, gamma=0.99, learning_rate=0.01, state_size=9, \n",
    "                 action_size=3, hidden_size=10, batch_size=20,\n",
    "                 name='QNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.bn1 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn5 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 100000          # max number of episodes to learn from\n",
    "max_steps = 365                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "action_size = 3\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.01            # exploration probability at start\n",
    "explore_stop = 0.0            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 32               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.00001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 200000            # memory capacity\n",
    "batch_size = 512                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory\n",
    "\n",
    "transaction_cost_ratio = 0.02\n",
    "target_update = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, gamma=gamma, learning_rate=learning_rate,batch_size=batch_size)\n",
    "targetQN = QNetwork(name='main', hidden_size=hidden_size, gamma=gamma, learning_rate=learning_rate,batch_size=batch_size)\n",
    "env = envs.TradingEnv.FxEnv(scenario_length=max_steps, transaction_cost_ratio=transaction_cost_ratio)\n",
    "env.seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def make_state(observation, means, stds):\n",
    "    return np.array([observation[0], observation[1], observation[2],\n",
    "                    (observation[3]-means[3])/stds[3],\n",
    "                    (observation[4]-means[4])/stds[4],\n",
    "                    (observation[5]-means[5])/stds[5],\n",
    "                    (observation[6]-means[6])/stds[6],\n",
    "                    (observation[7]-means[7])/stds[7]])\n",
    "\"\"\"\n",
    "def make_state(observation, means, stds):\n",
    "    return np.array(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env.reset()\n",
    "values = []\n",
    "for i in range(100000):\n",
    "    observation, reward, done = env.step(env.action_space.sample())\n",
    "    values.append(observation)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "values_array = np.array(values)\n",
    "means = values_array.mean(axis=0)\n",
    "stds = values_array.std(axis=0)\n",
    "print(means)\n",
    "print(stds)\n",
    "\"\"\"\n",
    "means = [0.332350000, 0.334510000, 0.333140000, 0.000368700000, 0.000294200000, 0.546393900, 0.562338400, 0.493165397]\n",
    "stds = [0.47105571, 0.47181888, 0.47133612, 0.67677779, 0.97742333, 0.97325276, 0.95412702, 0.28551883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmemory = Memory(max_size=memory_size)\\nenv.reset()\\nobservation, reward, done, info = env.step(env.action_space.sample())\\nstate = make_state(observation, means, stds)\\nfor i in range(20000):\\n    memory.add((state))\\n    observation, reward, done, info = env.step(env.action_space.sample())\\n    state = make_state(observation, means, stds)\\n    if done:\\n        env.reset()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "memory = Memory(max_size=memory_size)\n",
    "env.reset()\n",
    "observation, reward, done, info = env.step(env.action_space.sample())\n",
    "state = make_state(observation, means, stds)\n",
    "for i in range(20000):\n",
    "    memory.add((state))\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    state = make_state(observation, means, stds)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f524ae82b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "[val[0:3] for val in memory.sample(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nopt = optim.Adam(mainQN.parameters(), 0.00001)\\ni = 0\\nwhile(True):\\n    batch = memory.sample(512)\\n    batch_state = np.array(batch)\\n    batch_q = torch.FloatTensor(np.array([val[0:3] for val in batch]))\\n    \\n    current_q_values = mainQN(Variable(torch.FloatTensor(batch_state)))\\n    loss = torch.nn.MSELoss()(current_q_values, batch_q)\\n    # backpropagation of loss to NN\\n    # 勾配を初期化\\n    opt.zero_grad()\\n    loss.backward()\\n    opt.step()\\n    \\n    if i % 100 == 0:\\n        print(loss.data.numpy())\\n    \\n    if loss.data.numpy() < 0.0001:\\n        break\\n        \\n    i += 1\\ntorch.save(mainQN.state_dict(), 'KeepPolicy.pth')\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "opt = optim.Adam(mainQN.parameters(), 0.00001)\n",
    "i = 0\n",
    "while(True):\n",
    "    batch = memory.sample(512)\n",
    "    batch_state = np.array(batch)\n",
    "    batch_q = torch.FloatTensor(np.array([val[0:3] for val in batch]))\n",
    "    \n",
    "    current_q_values = mainQN(Variable(torch.FloatTensor(batch_state)))\n",
    "    loss = torch.nn.MSELoss()(current_q_values, batch_q)\n",
    "    # backpropagation of loss to NN\n",
    "    # 勾配を初期化\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss.data.numpy())\n",
    "    \n",
    "    if loss.data.numpy() < 0.0001:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "torch.save(mainQN.state_dict(), 'KeepPolicy.pth')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mainQN.load_state_dict(torch.load('KeepPolicy.pth'))\n",
    "targetQN.load_state_dict(mainQN.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(memory, opt, batch_size):\n",
    "    # Sample mini-batch from memory\n",
    "    batch = memory.sample(batch_size)\n",
    "    #memory.buffer.clear()\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    ### ポイント！！！\n",
    "    # actionはスカラーなのでベクトルにする\n",
    "    # actionsはベクトルでなく、statesと同じ行列\n",
    "    actions = np.array([[each[1]] for each in batch])\n",
    "    ### ポイント終わり\n",
    "    rewards = np.array([each[2] for each in batch])\n",
    "    next_states = np.array([each[3] for each in batch])\n",
    "    # doneなら0, not doneなら1\n",
    "    # 終端状態のQ値はその後の報酬が存在しないためゼロとする\n",
    "    dones = np.array([1-float(each[4]) for each in batch])\n",
    "\n",
    "    # Train network\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s==False, dones)), dtype=torch.uint8)\n",
    "    \n",
    "    max_actions = mainQN(Variable(torch.FloatTensor(next_states))).max(1)[1].view(-1,1).detach()\n",
    "    target_maxQs = targetQN(Variable(torch.FloatTensor(next_states))).gather(1, max_actions).squeeze().detach()\n",
    "\n",
    "    #tutorial way\n",
    "    targets = (torch.FloatTensor(rewards) + torch.FloatTensor(dones) * gamma * target_maxQs).unsqueeze(1)\n",
    "\n",
    "    for i in range(1):\n",
    "        current_q_values = mainQN(Variable(torch.FloatTensor(states))).gather(1, torch.LongTensor(actions))\n",
    "        loss = torch.nn.SmoothL1Loss()(current_q_values, targets)\n",
    "        # backpropagation of loss to NN\n",
    "        # 勾配を初期化\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = make_state(observation, means, stds)\n",
    "last_action = 0\n",
    "current_position_reward = 0\n",
    "memory = Memory(max_size=memory_size)\n",
    "t = 0\n",
    "keep_list = []\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "while(len(memory.buffer)<batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    if 0.1 > np.random.rand():\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = last_action\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "    next_state = make_state(next_observation, means, stds)\n",
    "    \n",
    "    current_position_reward += reward\n",
    "    \n",
    "    if last_action != action:\n",
    "        if last_action != 0:\n",
    "            clipped_reward = current_position_reward/(t-entry_t)\n",
    "            memory.add((entry_state, last_action, clipped_reward, next_state, done))\n",
    "            memory.add((state, action, clipped_reward, next_state, done))\n",
    "            if current_position_reward > 0:\n",
    "                for val in keep_list:\n",
    "                    memory.add((val[0], val[1], clipped_reward, val[2], done))\n",
    "            keep_list = []\n",
    "            \n",
    "\n",
    "        entry_state = state\n",
    "        entry_t = t\n",
    "        current_position_reward = 0.0\n",
    "    else:\n",
    "        keep_list.append([state, action, next_state])\n",
    "\n",
    "    last_action = action\n",
    "    state = next_state\n",
    "    t += 1\n",
    "\n",
    "    if done:\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        last_action = 0\n",
    "        current_position_reward = 0.0\n",
    "        t = 0\n",
    "        entry_t = 0\n",
    "        observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = make_state(observation, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-2.5284\n",
      "entry_point=119.2600 exit_point=118.8200 max_point=119.1000 min_point=118.7700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.7780\n",
      "entry_point=121.2300 exit_point=120.5800 max_point=125.5000 min_point=118.9000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=14.4408\n",
      "entry_point=123.2000 exit_point=107.3900 max_point=123.5600 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.8250\n",
      "entry_point=106.0900 exit_point=101.6300 max_point=106.1200 min_point=102.2500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.8844\n",
      "entry_point=102.9600 exit_point=106.5100 max_point=107.2800 min_point=100.8300\n",
      "Episode: 0 Total reward: -4.0498 Training loss: 0.0173 Explore P: 0.0096 Reward mean: -4.0498 Reward std: 0.0000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-2.8004\n",
      "entry_point=108.8100 exit_point=109.4900 max_point=114.5000 min_point=107.1700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.0892\n",
      "entry_point=109.4900 exit_point=111.7400 max_point=112.9300 min_point=102.1700\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-3.8300\n",
      "entry_point=110.0000 exit_point=113.5500 max_point=114.3300 min_point=109.3000\n",
      "Episode: 1 Total reward: -18.2356 Training loss: 0.0328 Explore P: 0.0093 Reward mean: -11.1427 Reward std: 7.0929\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-17.8600\n",
      "entry_point=102.8800 exit_point=117.8900 max_point=121.1200 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-8.2476\n",
      "entry_point=118.0000 exit_point=111.4300 max_point=118.7700 min_point=111.8800\n",
      "Episode: 2 Total reward: -27.8952 Training loss: 0.0217 Explore P: 0.0090 Reward mean: -16.7269 Reward std: 9.7931\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-14.5150\n",
      "entry_point=105.9500 exit_point=117.3200 max_point=121.1200 min_point=104.7500\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-1.2700\n",
      "entry_point=117.9800 exit_point=119.7200 max_point=119.0600 min_point=109.6200\n",
      "Episode: 3 Total reward: -20.3736 Training loss: 0.0162 Explore P: 0.0086 Reward mean: -17.6385 Reward std: 8.6268\n",
      "entry_state_action=0, last_action=2, action=1, position reward=0.7830\n",
      "entry_point=83.9500 exit_point=80.2800 max_point=85.4700 min_point=79.3400\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-1.7820\n",
      "entry_point=80.8500 exit_point=78.1200 max_point=81.3100 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=1, position reward=0.1354\n",
      "entry_point=79.1000 exit_point=77.0100 max_point=78.1200 min_point=76.6900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.1968\n",
      "entry_point=76.7300 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-1.1794\n",
      "entry_point=78.3400 exit_point=78.0200 max_point=78.3000 min_point=76.9200\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.7658\n",
      "entry_point=77.9700 exit_point=76.3800 max_point=78.1300 min_point=76.6600\n",
      "entry_state_action=0, last_action=1, action=1, position reward=7.5000\n",
      "entry_point=76.1900 exit_point=83.6500 max_point=84.0000 min_point=76.1500\n",
      "Episode: 4 Total reward: -0.6184 Training loss: 0.0273 Explore P: 0.0083 Reward mean: -14.2345 Reward std: 10.2902\n",
      "entry_state_action=0, last_action=2, action=1, position reward=20.3190\n",
      "entry_point=122.5200 exit_point=100.8000 max_point=125.1300 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=2, position reward=1.0594\n",
      "entry_point=101.0500 exit_point=105.1100 max_point=105.2100 min_point=100.3100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-9.4156\n",
      "entry_point=104.5300 exit_point=111.9100 max_point=113.7800 min_point=105.1100\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-1.5700\n",
      "entry_point=113.8100 exit_point=115.1700 max_point=115.5000 min_point=114.2700\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-0.4700\n",
      "entry_point=117.7500 exit_point=117.6700 max_point=118.1400 min_point=117.0300\n",
      "Episode: 5 Total reward: 2.5712 Training loss: 0.0272 Explore P: 0.0080 Reward mean: -11.4336 Reward std: 11.2901\n",
      "entry_state_action=0, last_action=2, action=0, position reward=3.4898\n",
      "entry_point=109.1500 exit_point=104.5400 max_point=111.4000 min_point=102.1700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-16.1370\n",
      "entry_point=104.3100 exit_point=119.0000 max_point=118.8500 min_point=104.1500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-5.2024\n",
      "entry_point=118.8500 exit_point=116.9400 max_point=121.1200 min_point=115.7300\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-0.2092\n",
      "entry_point=117.3200 exit_point=117.7000 max_point=119.1100 min_point=117.9800\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-2.3540\n",
      "entry_point=118.9600 exit_point=117.5500 max_point=117.7000 min_point=117.7000\n",
      "Episode: 6 Total reward: -27.3584 Training loss: 0.0229 Explore P: 0.0077 Reward mean: -13.7085 Reward std: 11.8453\n",
      "entry_state_action=0, last_action=1, action=2, position reward=0.7656\n",
      "entry_point=116.4800 exit_point=121.4200 max_point=120.8900 min_point=116.8400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=0.8662\n",
      "entry_point=120.7200 exit_point=117.8000 max_point=121.7900 min_point=115.6400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.6410\n",
      "entry_point=117.8000 exit_point=108.0400 max_point=123.9500 min_point=105.9300\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-2.2912\n",
      "entry_point=99.7200 exit_point=99.8600 max_point=99.5600 min_point=99.5600\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-1.9870\n",
      "entry_point=99.5600 exit_point=101.9200 max_point=100.7200 min_point=98.8300\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-4.3706\n",
      "entry_point=101.1500 exit_point=102.8800 max_point=105.3200 min_point=101.8800\n",
      "entry_state_action=2, last_action=1, action=1, position reward=1.9300\n",
      "entry_point=103.5300 exit_point=104.8100 max_point=104.1700 min_point=102.8800\n",
      "Episode: 7 Total reward: -6.1190 Training loss: 0.0202 Explore P: 0.0075 Reward mean: -12.7598 Reward std: 11.3610\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.0188\n",
      "entry_point=113.9600 exit_point=117.5900 max_point=119.1100 min_point=114.5900\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.4684\n",
      "entry_point=117.7200 exit_point=117.0400 max_point=117.3700 min_point=109.6200\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-4.0674\n",
      "entry_point=114.6800 exit_point=116.5200 max_point=116.6000 min_point=114.4000\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-1.0764\n",
      "entry_point=116.3700 exit_point=115.8600 max_point=121.7900 min_point=114.9100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-7.5900\n",
      "entry_point=116.8200 exit_point=123.4500 max_point=123.7200 min_point=115.8600\n",
      "Episode: 8 Total reward: -26.3182 Training loss: 0.0216 Explore P: 0.0072 Reward mean: -14.2663 Reward std: 11.5276\n",
      "entry_state_action=0, last_action=2, action=1, position reward=14.1278\n",
      "entry_point=108.6400 exit_point=93.6500 max_point=108.9000 min_point=87.4700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-4.2200\n",
      "entry_point=93.1100 exit_point=89.4300 max_point=97.2100 min_point=86.3100\n",
      "Episode: 9 Total reward: 7.8150 Training loss: 0.0189 Explore P: 0.0069 Reward mean: -12.0582 Reward std: 12.7859\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.0426\n",
      "entry_point=103.0900 exit_point=106.5200 max_point=110.2800 min_point=97.0400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=7.0802\n",
      "entry_point=103.6300 exit_point=97.5700 max_point=106.5200 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.7800\n",
      "entry_point=95.3400 exit_point=93.7800 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10 Total reward: 0.2490 Training loss: 0.0229 Explore P: 0.0067 Reward mean: -10.9394 Reward std: 12.6939\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.4156\n",
      "entry_point=112.6100 exit_point=109.4600 max_point=114.3800 min_point=108.7200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-0.4446\n",
      "entry_point=109.0800 exit_point=108.4900 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=1.1400\n",
      "entry_point=109.3200 exit_point=110.3500 max_point=112.0100 min_point=108.9100\n",
      "Episode: 11 Total reward: -5.1892 Training loss: 0.0228 Explore P: 0.0065 Reward mean: -10.4602 Reward std: 12.2570\n",
      "entry_state_action=0, last_action=2, action=1, position reward=1.9960\n",
      "entry_point=124.2900 exit_point=120.3700 max_point=124.4500 min_point=118.9000\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-9.3066\n",
      "entry_point=120.2000 exit_point=114.6600 max_point=123.5600 min_point=112.4100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=0.4996\n",
      "entry_point=114.6600 exit_point=110.4200 max_point=114.3400 min_point=111.0200\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.3166\n",
      "entry_point=111.0200 exit_point=111.0300 max_point=111.3300 min_point=107.9900\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-3.9100\n",
      "entry_point=111.0300 exit_point=115.1600 max_point=118.1400 min_point=99.8100\n",
      "Episode: 12 Total reward: -19.0772 Training loss: 0.0225 Explore P: 0.0062 Reward mean: -11.1230 Reward std: 11.9979\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.2720\n",
      "entry_point=83.4200 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.5302\n",
      "entry_point=79.1000 exit_point=76.9100 max_point=78.1200 min_point=76.9900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.0968\n",
      "entry_point=77.0100 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.8242\n",
      "entry_point=78.3400 exit_point=77.8500 max_point=78.3000 min_point=77.7100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=2.4500\n",
      "entry_point=77.7100 exit_point=80.3000 max_point=84.0000 min_point=76.1500\n",
      "Episode: 13 Total reward: -5.5916 Training loss: 0.0313 Explore P: 0.0060 Reward mean: -10.7279 Reward std: 11.6489\n",
      "entry_state_action=0, last_action=2, action=0, position reward=13.4922\n",
      "entry_point=123.2000 exit_point=107.4600 max_point=123.5600 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=1, position reward=0.4304\n",
      "entry_point=107.4600 exit_point=106.0300 max_point=110.9400 min_point=100.8300\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-4.3232\n",
      "entry_point=105.9800 exit_point=101.5600 max_point=107.2800 min_point=99.8100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-9.7300\n",
      "entry_point=102.6600 exit_point=111.2900 max_point=118.1400 min_point=100.3100\n",
      "Episode: 14 Total reward: -5.5938 Training loss: 0.0188 Explore P: 0.0058 Reward mean: -10.3857 Reward std: 11.3266\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.8226\n",
      "entry_point=108.7200 exit_point=110.9300 max_point=112.9900 min_point=104.9400\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.0864\n",
      "entry_point=110.7800 exit_point=111.3000 max_point=114.4200 min_point=107.7300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.0104\n",
      "entry_point=111.3200 exit_point=111.8200 max_point=111.9400 min_point=109.9300\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.7410\n",
      "entry_point=111.8700 exit_point=107.9900 max_point=110.7000 min_point=107.3400\n",
      "Episode: 15 Total reward: -22.4578 Training loss: 0.0249 Explore P: 0.0056 Reward mean: -11.1402 Reward std: 11.3495\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-4.1628\n",
      "entry_point=112.8800 exit_point=114.2400 max_point=115.2300 min_point=108.3200\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-5.5090\n",
      "entry_point=114.1400 exit_point=111.2900 max_point=114.3800 min_point=104.9400\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-0.6000\n",
      "entry_point=111.3100 exit_point=111.3200 max_point=111.8700 min_point=110.7200\n",
      "Episode: 16 Total reward: -13.4456 Training loss: 0.0196 Explore P: 0.0054 Reward mean: -11.2758 Reward std: 11.0240\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-5.2078\n",
      "entry_point=91.6700 exit_point=87.8900 max_point=91.3400 min_point=87.8900\n",
      "entry_state_action=0, last_action=2, action=1, position reward=3.2538\n",
      "entry_point=87.4800 exit_point=82.9300 max_point=88.9900 min_point=80.6100\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-4.4320\n",
      "entry_point=82.8100 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=2, position reward=0.3600\n",
      "entry_point=79.1000 exit_point=77.7600 max_point=78.3400 min_point=75.7600\n",
      "Episode: 17 Total reward: -10.0590 Training loss: 0.0270 Explore P: 0.0052 Reward mean: -11.2082 Reward std: 10.7170\n",
      "entry_state_action=0, last_action=2, action=1, position reward=4.5240\n",
      "entry_point=116.2500 exit_point=108.1500 max_point=116.1500 min_point=108.1500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=0.9338\n",
      "entry_point=108.8000 exit_point=110.6100 max_point=114.5000 min_point=103.9500\n",
      "entry_state_action=0, last_action=2, action=2, position reward=4.7400\n",
      "entry_point=109.4600 exit_point=104.7200 max_point=111.4000 min_point=102.1700\n",
      "Episode: 18 Total reward: 5.7836 Training loss: 0.0166 Explore P: 0.0050 Reward mean: -10.3139 Reward std: 11.0998\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-6.7900\n",
      "entry_point=104.7900 exit_point=111.6200 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.4500\n",
      "entry_point=111.0000 exit_point=112.0700 max_point=111.6200 min_point=111.6200\n",
      "Episode: 19 Total reward: -9.4558 Training loss: 0.0183 Explore P: 0.0048 Reward mean: -10.2710 Reward std: 10.8204\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-0.5004\n",
      "entry_point=77.0100 exit_point=78.0700 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-7.7388\n",
      "entry_point=78.0200 exit_point=82.7400 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-4.3890\n",
      "entry_point=83.4400 exit_point=79.8600 max_point=83.1900 min_point=77.6100\n",
      "entry_state_action=0, last_action=1, action=2, position reward=8.0388\n",
      "entry_point=79.5300 exit_point=91.0300 max_point=90.5600 min_point=80.2400\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-0.7300\n",
      "entry_point=90.5600 exit_point=91.7600 max_point=91.1400 min_point=90.5100\n",
      "Episode: 20 Total reward: -7.8402 Training loss: 0.0203 Explore P: 0.0046 Reward mean: -10.1552 Reward std: 10.5723\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-14.2820\n",
      "entry_point=92.1400 exit_point=78.1200 max_point=92.7800 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=2, position reward=1.2000\n",
      "entry_point=79.1000 exit_point=76.9200 max_point=78.3400 min_point=75.7600\n",
      "Episode: 21 Total reward: -14.2848 Training loss: 0.0236 Explore P: 0.0045 Reward mean: -10.3429 Reward std: 10.3650\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.0414\n",
      "entry_point=104.7400 exit_point=102.6300 max_point=103.9000 min_point=101.2400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-6.7854\n",
      "entry_point=102.5700 exit_point=108.5300 max_point=107.2700 min_point=102.1100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-14.2600\n",
      "entry_point=108.9500 exit_point=122.5200 max_point=125.5000 min_point=105.9200\n",
      "Episode: 22 Total reward: -28.9106 Training loss: 0.0233 Explore P: 0.0043 Reward mean: -11.1502 Reward std: 10.8213\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-0.2682\n",
      "entry_point=96.6600 exit_point=96.1500 max_point=99.3000 min_point=87.4700\n",
      "entry_state_action=1, last_action=2, action=0, position reward=2.7996\n",
      "entry_point=96.4100 exit_point=92.3500 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=2.8200\n",
      "entry_point=93.0000 exit_point=90.0000 max_point=93.2000 min_point=90.0000\n",
      "Episode: 23 Total reward: 0.0882 Training loss: 0.0235 Explore P: 0.0042 Reward mean: -10.6820 Reward std: 10.8288\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.8094\n",
      "entry_point=116.7100 exit_point=119.0600 max_point=118.9900 min_point=109.6200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=2, last_action=1, action=2, position reward=-3.8236\n",
      "entry_point=117.9700 exit_point=116.7800 max_point=121.7900 min_point=114.9100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-0.8300\n",
      "entry_point=117.1800 exit_point=117.6100 max_point=123.9500 min_point=116.7600\n",
      "Episode: 24 Total reward: -9.5172 Training loss: 0.0287 Explore P: 0.0040 Reward mean: -10.6354 Reward std: 10.6125\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-5.5406\n",
      "entry_point=113.0100 exit_point=114.3300 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.8648\n",
      "entry_point=115.0300 exit_point=113.9500 max_point=115.1600 min_point=111.7700\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-3.9102\n",
      "entry_point=112.4000 exit_point=109.4800 max_point=114.2400 min_point=108.3200\n",
      "entry_state_action=0, last_action=2, action=2, position reward=0.5700\n",
      "entry_point=109.1400 exit_point=109.1700 max_point=109.7400 min_point=109.7400\n",
      "Episode: 25 Total reward: -19.1966 Training loss: 0.0271 Explore P: 0.0039 Reward mean: -10.9646 Reward std: 10.5359\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-0.6348\n",
      "entry_point=116.0100 exit_point=113.1600 max_point=115.9600 min_point=112.7700\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.0676\n",
      "entry_point=113.7400 exit_point=113.7300 max_point=115.2300 min_point=108.3200\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.5900\n",
      "entry_point=109.8700 exit_point=110.0300 max_point=110.6700 min_point=109.4400\n",
      "Episode: 26 Total reward: -6.0100 Training loss: 0.0224 Explore P: 0.0037 Reward mean: -10.7811 Reward std: 10.3812\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-22.7346\n",
      "entry_point=101.8300 exit_point=118.9000 max_point=125.5000 min_point=101.2600\n",
      "Episode: 27 Total reward: -24.3712 Training loss: 0.0220 Explore P: 0.0036 Reward mean: -11.2665 Reward std: 10.5014\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.8206\n",
      "entry_point=104.7900 exit_point=100.8300 max_point=105.8100 min_point=100.9700\n",
      "entry_state_action=0, last_action=1, action=1, position reward=8.6500\n",
      "entry_point=102.7000 exit_point=113.2900 max_point=118.1400 min_point=99.8100\n",
      "Episode: 28 Total reward: 8.2408 Training loss: 0.0475 Explore P: 0.0035 Reward mean: -10.5938 Reward std: 10.9154\n",
      "entry_state_action=0, last_action=1, action=2, position reward=9.0554\n",
      "entry_point=101.8400 exit_point=114.6900 max_point=113.7900 min_point=101.2600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-11.7504\n",
      "entry_point=113.7300 exit_point=124.0800 max_point=125.5000 min_point=114.4000\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-7.7480\n",
      "entry_point=124.0200 exit_point=119.0900 max_point=125.1300 min_point=118.9000\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-1.5800\n",
      "entry_point=118.9000 exit_point=120.6700 max_point=121.1800 min_point=119.0900\n",
      "Episode: 29 Total reward: -14.4598 Training loss: 0.0187 Explore P: 0.0033 Reward mean: -10.7227 Reward std: 10.7544\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-12.0400\n",
      "entry_point=92.8800 exit_point=108.9800 max_point=108.8600 min_point=94.6300\n",
      "Episode: 30 Total reward: -17.9576 Training loss: 0.0258 Explore P: 0.0032 Reward mean: -10.9561 Reward std: 10.6564\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-2.0082\n",
      "entry_point=94.3300 exit_point=96.1500 max_point=99.3000 min_point=96.4100\n",
      "entry_state_action=1, last_action=2, action=1, position reward=5.8298\n",
      "entry_point=96.4100 exit_point=88.4700 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.6300\n",
      "entry_point=88.5100 exit_point=85.8400 max_point=88.9900 min_point=85.1900\n",
      "Episode: 31 Total reward: 1.7250 Training loss: 0.0209 Explore P: 0.0031 Reward mean: -10.5598 Reward std: 10.7182\n",
      "entry_state_action=0, last_action=1, action=0, position reward=20.6580\n",
      "entry_point=98.2100 exit_point=120.9800 max_point=121.6000 min_point=98.2300\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.6196\n",
      "entry_point=120.9800 exit_point=120.2300 max_point=121.5400 min_point=116.4400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.8700\n",
      "entry_point=120.4800 exit_point=119.3600 max_point=120.2300 min_point=118.9400\n",
      "Episode: 32 Total reward: 13.5446 Training loss: 0.0168 Explore P: 0.0030 Reward mean: -9.8294 Reward std: 11.3345\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.2652\n",
      "entry_point=77.6000 exit_point=79.5900 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=1.2208\n",
      "entry_point=79.2600 exit_point=82.6400 max_point=82.4600 min_point=77.6100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.6092\n",
      "entry_point=92.6700 exit_point=95.5500 max_point=96.4700 min_point=93.1800\n",
      "entry_state_action=2, last_action=1, action=1, position reward=1.6400\n",
      "entry_point=94.9600 exit_point=97.1900 max_point=99.8000 min_point=92.8800\n",
      "Episode: 33 Total reward: -7.0590 Training loss: 0.0336 Explore P: 0.0029 Reward mean: -9.7479 Reward std: 11.1764\n",
      "entry_state_action=0, last_action=2, action=1, position reward=9.0832\n",
      "entry_point=97.5200 exit_point=84.5500 max_point=98.6100 min_point=83.0500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-3.1400\n",
      "entry_point=84.8400 exit_point=81.4100 max_point=84.5500 min_point=80.7300\n",
      "Episode: 34 Total reward: 5.6028 Training loss: 0.0257 Explore P: 0.0028 Reward mean: -9.3093 Reward std: 11.3085\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-17.5200\n",
      "entry_point=84.2600 exit_point=101.9500 max_point=105.4000 min_point=84.4300\n",
      "Episode: 35 Total reward: -19.3752 Training loss: 0.0340 Explore P: 0.0027 Reward mean: -9.5889 Reward std: 11.2724\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.4202\n",
      "entry_point=99.5600 exit_point=97.5700 max_point=110.2800 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.0222\n",
      "entry_point=95.3400 exit_point=98.1700 max_point=99.3000 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.7192\n",
      "entry_point=94.1600 exit_point=94.5700 max_point=98.6100 min_point=92.9100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.6100\n",
      "entry_point=95.4600 exit_point=92.9600 max_point=94.7500 min_point=92.1400\n",
      "Episode: 36 Total reward: -16.6124 Training loss: 0.0253 Explore P: 0.0026 Reward mean: -9.7787 Reward std: 11.1772\n",
      "entry_state_action=0, last_action=2, action=1, position reward=18.8494\n",
      "entry_point=118.7000 exit_point=97.0400 max_point=123.9500 min_point=97.5300\n",
      "entry_state_action=2, last_action=1, action=2, position reward=1.5060\n",
      "entry_point=97.5300 exit_point=99.8500 max_point=100.7200 min_point=97.0400\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-6.4006\n",
      "entry_point=100.2000 exit_point=102.8800 max_point=105.3200 min_point=99.8500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=4.8700\n",
      "entry_point=103.5300 exit_point=107.7500 max_point=108.3000 min_point=102.8800\n",
      "Episode: 37 Total reward: 16.3308 Training loss: 0.0165 Explore P: 0.0025 Reward mean: -9.0916 Reward std: 11.7945\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.5880\n",
      "entry_point=121.6000 exit_point=118.9000 max_point=121.2000 min_point=119.4000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.1080\n",
      "entry_point=116.4000 exit_point=119.0000 max_point=125.4500 min_point=116.1500\n",
      "entry_state_action=0, last_action=2, action=1, position reward=9.9970\n",
      "entry_point=119.9000 exit_point=107.4000 max_point=120.8500 min_point=107.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.1500\n",
      "entry_point=107.6500 exit_point=107.2500 max_point=107.4000 min_point=107.4000\n",
      "Episode: 38 Total reward: -1.5070 Training loss: 0.0274 Explore P: 0.0024 Reward mean: -8.8971 Reward std: 11.7039\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.7132\n",
      "entry_point=89.7500 exit_point=81.3800 max_point=94.4200 min_point=79.3400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=4.2000\n",
      "entry_point=81.0600 exit_point=85.1800 max_point=84.3300 min_point=80.9800\n",
      "Episode: 39 Total reward: 9.5270 Training loss: 0.0224 Explore P: 0.0023 Reward mean: -8.4365 Reward std: 11.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-15.7560\n",
      "entry_point=81.1700 exit_point=94.6300 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-1.0456\n",
      "entry_point=98.0900 exit_point=98.7600 max_point=100.3300 min_point=96.2400\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.0756\n",
      "entry_point=97.3000 exit_point=97.1600 max_point=97.2600 min_point=96.7800\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.4500\n",
      "entry_point=96.7800 exit_point=97.6100 max_point=97.1600 min_point=97.1600\n",
      "Episode: 40 Total reward: -23.4384 Training loss: 0.0243 Explore P: 0.0022 Reward mean: -8.8024 Reward std: 11.9886\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.6970\n",
      "entry_point=121.9500 exit_point=117.4000 max_point=125.1000 min_point=115.6000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.0210\n",
      "entry_point=116.2500 exit_point=108.4000 max_point=116.1500 min_point=108.9000\n",
      "entry_state_action=0, last_action=1, action=0, position reward=6.3002\n",
      "entry_point=105.3500 exit_point=114.3100 max_point=114.5000 min_point=103.9500\n",
      "Episode: 41 Total reward: 6.5572 Training loss: 0.0160 Explore P: 0.0022 Reward mean: -8.4367 Reward std: 12.0742\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-20.1846\n",
      "entry_point=78.1600 exit_point=96.8900 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.6772\n",
      "entry_point=96.7300 exit_point=98.5400 max_point=98.9800 min_point=96.2400\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.2538\n",
      "entry_point=98.3600 exit_point=99.5500 max_point=100.3300 min_point=98.1900\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.7456\n",
      "entry_point=98.1900 exit_point=98.7600 max_point=99.5500 min_point=98.7800\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-0.9300\n",
      "entry_point=98.3400 exit_point=99.8900 max_point=100.2900 min_point=96.7800\n",
      "Episode: 42 Total reward: -29.1012 Training loss: 0.0136 Explore P: 0.0021 Reward mean: -8.9173 Reward std: 12.3328\n",
      "entry_state_action=0, last_action=2, action=1, position reward=10.7550\n",
      "entry_point=100.0600 exit_point=90.3700 max_point=101.9500 min_point=86.3100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=2.7200\n",
      "entry_point=89.7500 exit_point=93.0900 max_point=94.4200 min_point=88.5400\n",
      "Episode: 43 Total reward: 9.8538 Training loss: 0.0172 Explore P: 0.0020 Reward mean: -8.4907 Reward std: 12.5086\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-12.1520\n",
      "entry_point=90.4900 exit_point=78.1200 max_point=94.4200 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=2, position reward=1.2000\n",
      "entry_point=79.1000 exit_point=76.9200 max_point=78.1200 min_point=76.6900\n",
      "Episode: 44 Total reward: -12.6018 Training loss: 0.0291 Explore P: 0.0019 Reward mean: -8.5820 Reward std: 12.3837\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.1322\n",
      "entry_point=107.9500 exit_point=106.2000 max_point=110.2800 min_point=97.0400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.6902\n",
      "entry_point=106.0700 exit_point=97.5700 max_point=106.9100 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.9816\n",
      "entry_point=95.3400 exit_point=98.5800 max_point=100.7800 min_point=87.4700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-3.0400\n",
      "entry_point=98.0800 exit_point=95.5400 max_point=99.3300 min_point=92.9100\n",
      "Episode: 45 Total reward: -5.8364 Training loss: 0.0261 Explore P: 0.0019 Reward mean: -8.5224 Reward std: 12.2549\n",
      "entry_state_action=0, last_action=1, action=0, position reward=9.1600\n",
      "entry_point=106.2300 exit_point=117.8900 max_point=121.1200 min_point=104.7500\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.8100\n",
      "entry_point=116.3400 exit_point=116.8000 max_point=117.4300 min_point=114.4000\n",
      "Episode: 46 Total reward: 5.4186 Training loss: 0.0228 Explore P: 0.0018 Reward mean: -8.2257 Reward std: 12.2896\n",
      "entry_state_action=0, last_action=2, action=0, position reward=11.7208\n",
      "entry_point=120.2400 exit_point=107.3900 max_point=125.5000 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=0.4700\n",
      "entry_point=106.0900 exit_point=105.6500 max_point=106.1200 min_point=106.1200\n",
      "Episode: 47 Total reward: 7.5642 Training loss: 0.0202 Explore P: 0.0017 Reward mean: -7.8968 Reward std: 12.3683\n",
      "entry_state_action=0, last_action=2, action=2, position reward=3.1300\n",
      "entry_point=113.4500 exit_point=110.1500 max_point=118.1400 min_point=99.8100\n",
      "Episode: 48 Total reward: 1.0310 Training loss: 0.0301 Explore P: 0.0017 Reward mean: -7.7146 Reward std: 12.3063\n",
      "entry_state_action=0, last_action=2, action=2, position reward=5.3700\n",
      "entry_point=123.3900 exit_point=117.6300 max_point=125.1300 min_point=99.8100\n",
      "Episode: 49 Total reward: 3.2922 Training loss: 0.0152 Explore P: 0.0016 Reward mean: -7.4945 Reward std: 12.2797\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-9.9866\n",
      "entry_point=112.8000 exit_point=119.3300 max_point=119.7600 min_point=111.6900\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.3876\n",
      "entry_point=119.3300 exit_point=117.7800 max_point=121.7900 min_point=114.9100\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.5222\n",
      "entry_point=118.8800 exit_point=118.7400 max_point=123.9500 min_point=115.6400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.4248\n",
      "entry_point=117.9900 exit_point=113.5700 max_point=117.4400 min_point=112.7600\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-2.2600\n",
      "entry_point=117.2500 exit_point=114.4800 max_point=116.7400 min_point=113.8800\n",
      "Episode: 50 Total reward: -23.6090 Training loss: 0.0179 Explore P: 0.0016 Reward mean: -7.8104 Reward std: 12.3623\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-20.9300\n",
      "entry_point=101.9800 exit_point=122.8600 max_point=125.5000 min_point=101.2600\n",
      "Episode: 51 Total reward: -22.9196 Training loss: 0.0155 Explore P: 0.0015 Reward mean: -8.1010 Reward std: 12.4174\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.6930\n",
      "entry_point=101.6400 exit_point=100.0600 max_point=102.8200 min_point=99.1300\n",
      "entry_state_action=0, last_action=2, action=1, position reward=11.1268\n",
      "entry_point=100.0600 exit_point=88.5400 max_point=101.9500 min_point=86.3100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=5.8800\n",
      "entry_point=88.6600 exit_point=94.4200 max_point=93.8300 min_point=88.5400\n",
      "Episode: 52 Total reward: 9.3098 Training loss: 0.0181 Explore P: 0.0014 Reward mean: -7.7725 Reward std: 12.5258\n",
      "entry_state_action=0, last_action=1, action=0, position reward=6.5206\n",
      "entry_point=108.7200 exit_point=117.4400 max_point=121.1200 min_point=106.7400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-0.2300\n",
      "entry_point=117.7600 exit_point=117.8000 max_point=118.0300 min_point=117.4300\n",
      "Episode: 53 Total reward: 1.4210 Training loss: 0.0193 Explore P: 0.0014 Reward mean: -7.6022 Reward std: 12.4710\n",
      "entry_state_action=0, last_action=2, action=0, position reward=9.6432\n",
      "entry_point=91.5600 exit_point=81.3800 max_point=94.4200 min_point=79.3400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.3200\n",
      "entry_point=81.6600 exit_point=82.8000 max_point=85.4700 min_point=82.4800\n",
      "Episode: 54 Total reward: 8.3088 Training loss: 0.0277 Explore P: 0.0013 Reward mean: -7.3129 Reward std: 12.5386\n",
      "entry_state_action=0, last_action=2, action=1, position reward=14.3594\n",
      "entry_point=120.1400 exit_point=102.8800 max_point=123.9500 min_point=97.0400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=3.1700\n",
      "entry_point=103.5300 exit_point=106.0500 max_point=108.3000 min_point=102.8800\n",
      "Episode: 55 Total reward: 14.6566 Training loss: 0.0158 Explore P: 0.0013 Reward mean: -6.9206 Reward std: 12.7623\n",
      "entry_state_action=0, last_action=1, action=0, position reward=20.6280\n",
      "entry_point=98.3100 exit_point=120.9800 max_point=121.6000 min_point=97.2600\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.4680\n",
      "entry_point=120.9800 exit_point=119.2000 max_point=121.5400 min_point=116.4400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.1300\n",
      "entry_point=119.4000 exit_point=119.0700 max_point=120.2700 min_point=119.2000\n",
      "Episode: 56 Total reward: 15.3342 Training loss: 0.0239 Explore P: 0.0012 Reward mean: -6.5302 Reward std: 12.9828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=19.0492\n",
      "entry_point=117.7600 exit_point=99.7200 max_point=123.9500 min_point=97.0400\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-0.3766\n",
      "entry_point=99.5600 exit_point=99.4000 max_point=100.7200 min_point=98.8300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=0.5012\n",
      "entry_point=98.8300 exit_point=101.4600 max_point=102.6400 min_point=99.4000\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-0.0500\n",
      "entry_point=101.1500 exit_point=101.9300 max_point=101.8800 min_point=101.8800\n",
      "Episode: 57 Total reward: 11.4544 Training loss: 0.0189 Explore P: 0.0012 Reward mean: -6.2201 Reward std: 13.0816\n",
      "entry_state_action=0, last_action=1, action=1, position reward=3.6500\n",
      "entry_point=114.0100 exit_point=117.8800 max_point=121.7900 min_point=109.6200\n",
      "Episode: 58 Total reward: 1.5898 Training loss: 0.0200 Explore P: 0.0012 Reward mean: -6.0877 Reward std: 13.0094\n",
      "entry_state_action=0, last_action=2, action=1, position reward=12.5694\n",
      "entry_point=118.7000 exit_point=102.8800 max_point=123.9500 min_point=97.0400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=5.1200\n",
      "entry_point=103.5300 exit_point=108.0000 max_point=108.3000 min_point=102.8800\n",
      "Episode: 59 Total reward: 15.1954 Training loss: 0.0140 Explore P: 0.0011 Reward mean: -5.7330 Reward std: 13.1851\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-6.2220\n",
      "entry_point=84.0900 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.0464\n",
      "entry_point=77.1300 exit_point=76.9600 max_point=77.4500 min_point=76.6900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.1468\n",
      "entry_point=76.8200 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-4.5202\n",
      "entry_point=78.3400 exit_point=81.8300 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.2100\n",
      "entry_point=81.5100 exit_point=79.6200 max_point=81.8300 min_point=78.1600\n",
      "Episode: 60 Total reward: -17.7598 Training loss: 0.0217 Explore P: 0.0011 Reward mean: -5.9302 Reward std: 13.1655\n",
      "entry_state_action=0, last_action=1, action=1, position reward=6.2400\n",
      "entry_point=108.5900 exit_point=115.6800 max_point=121.1200 min_point=108.5100\n",
      "Episode: 61 Total reward: 4.9182 Training loss: 0.0174 Explore P: 0.0010 Reward mean: -5.7552 Reward std: 13.1302\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-5.5890\n",
      "entry_point=113.9900 exit_point=111.2900 max_point=115.2300 min_point=104.9400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-1.0100\n",
      "entry_point=111.7300 exit_point=110.8000 max_point=111.8100 min_point=110.5500\n",
      "Episode: 62 Total reward: -10.7034 Training loss: 0.0342 Explore P: 0.0010 Reward mean: -5.8337 Reward std: 13.0402\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.9220\n",
      "entry_point=82.2000 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.5302\n",
      "entry_point=79.1000 exit_point=76.9100 max_point=78.1200 min_point=76.9900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.0968\n",
      "entry_point=77.0100 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=0.1968\n",
      "entry_point=78.3400 exit_point=76.7500 max_point=78.3000 min_point=76.6600\n",
      "entry_state_action=2, last_action=1, action=1, position reward=3.5100\n",
      "entry_point=76.6600 exit_point=80.2600 max_point=84.0000 min_point=76.1500\n",
      "Episode: 63 Total reward: -2.2662 Training loss: 0.0209 Explore P: 0.0010 Reward mean: -5.7780 Reward std: 12.9455\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-0.8452\n",
      "entry_point=80.2900 exit_point=80.3400 max_point=81.3100 min_point=75.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-1.1100\n",
      "entry_point=80.6800 exit_point=82.3600 max_point=84.0000 min_point=77.6100\n",
      "Episode: 64 Total reward: -5.5546 Training loss: 0.0271 Explore P: 0.0009 Reward mean: -5.7746 Reward std: 12.8456\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-7.9656\n",
      "entry_point=112.5000 exit_point=118.4800 max_point=121.1200 min_point=109.3000\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-2.1540\n",
      "entry_point=118.2800 exit_point=118.8200 max_point=119.4300 min_point=118.4500\n",
      "Episode: 65 Total reward: -12.3496 Training loss: 0.0158 Explore P: 0.0009 Reward mean: -5.8742 Reward std: 12.7732\n",
      "entry_state_action=0, last_action=2, action=2, position reward=16.2800\n",
      "entry_point=121.0600 exit_point=104.7900 max_point=125.5000 min_point=99.8100\n",
      "Episode: 66 Total reward: 13.8488 Training loss: 0.0206 Explore P: 0.0009 Reward mean: -5.5798 Reward std: 12.9011\n",
      "entry_state_action=0, last_action=2, action=2, position reward=7.1100\n",
      "entry_point=122.5200 exit_point=116.5300 max_point=125.1300 min_point=99.8100\n",
      "Episode: 67 Total reward: 3.5396 Training loss: 0.0146 Explore P: 0.0008 Reward mean: -5.4457 Reward std: 12.8528\n",
      "entry_state_action=0, last_action=1, action=2, position reward=22.8754\n",
      "entry_point=78.0800 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-3.2100\n",
      "entry_point=102.2300 exit_point=104.3900 max_point=105.4000 min_point=94.6300\n",
      "Episode: 68 Total reward: 18.3838 Training loss: 0.0216 Explore P: 0.0008 Reward mean: -5.1004 Reward std: 13.0733\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-15.6800\n",
      "entry_point=101.8100 exit_point=117.5400 max_point=125.5000 min_point=101.8600\n",
      "Episode: 69 Total reward: -17.7662 Training loss: 0.0223 Explore P: 0.0008 Reward mean: -5.2813 Reward std: 13.0663\n",
      "entry_state_action=0, last_action=1, action=2, position reward=20.8054\n",
      "entry_point=80.1300 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-3.7882\n",
      "entry_point=102.2300 exit_point=102.6300 max_point=102.9100 min_point=94.6300\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.6600\n",
      "entry_point=102.6300 exit_point=103.1100 max_point=102.4500 min_point=101.9800\n",
      "Episode: 70 Total reward: 14.1420 Training loss: 0.0197 Explore P: 0.0007 Reward mean: -5.0077 Reward std: 13.1743\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-3.3906\n",
      "entry_point=80.0600 exit_point=78.4600 max_point=81.3100 min_point=75.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-2.2300\n",
      "entry_point=78.4600 exit_point=81.1800 max_point=84.0000 min_point=77.6100\n",
      "Episode: 71 Total reward: -8.9910 Training loss: 0.0173 Explore P: 0.0007 Reward mean: -5.0631 Reward std: 13.0908\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.1978\n",
      "entry_point=107.1300 exit_point=106.2000 max_point=110.2800 min_point=97.0400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=16.0994\n",
      "entry_point=106.1300 exit_point=87.4700 max_point=106.9100 min_point=89.0300\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-5.4076\n",
      "entry_point=89.8700 exit_point=96.2400 max_point=100.7800 min_point=88.9000\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.4700\n",
      "entry_point=94.8800 exit_point=95.7700 max_point=98.6100 min_point=94.1600\n",
      "Episode: 72 Total reward: 3.3814 Training loss: 0.0172 Explore P: 0.0007 Reward mean: -4.9474 Reward std: 13.0379\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.4968\n",
      "entry_point=82.2800 exit_point=78.3000 max_point=85.4700 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.7928\n",
      "entry_point=78.3400 exit_point=77.7400 max_point=78.3000 min_point=76.9200\n",
      "entry_state_action=2, last_action=1, action=0, position reward=2.9434\n",
      "entry_point=77.6400 exit_point=82.3900 max_point=82.3300 min_point=76.1500\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-3.0300\n",
      "entry_point=81.5100 exit_point=78.1400 max_point=81.4500 min_point=78.1600\n",
      "Episode: 73 Total reward: -9.9620 Training loss: 0.0184 Explore P: 0.0007 Reward mean: -5.0151 Reward std: 12.9624\n",
      "entry_state_action=0, last_action=1, action=1, position reward=8.5100\n",
      "entry_point=108.8000 exit_point=117.0600 max_point=121.1200 min_point=102.1700\n",
      "Episode: 74 Total reward: 6.0840 Training loss: 0.0185 Explore P: 0.0006 Reward mean: -4.8671 Reward std: 12.9385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=2, position reward=19.4000\n",
      "entry_point=128.4500 exit_point=108.4000 max_point=127.8000 min_point=108.9000\n",
      "Episode: 75 Total reward: 17.4810 Training loss: 0.0213 Explore P: 0.0006 Reward mean: -4.5731 Reward std: 13.1029\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.0970\n",
      "entry_point=124.2500 exit_point=120.0000 max_point=125.1000 min_point=116.9000\n",
      "entry_state_action=0, last_action=2, action=2, position reward=5.6300\n",
      "entry_point=117.0000 exit_point=111.7700 max_point=120.5500 min_point=103.9500\n",
      "Episode: 76 Total reward: 2.1020 Training loss: 0.0162 Explore P: 0.0006 Reward mean: -4.4864 Reward std: 13.0395\n",
      "entry_state_action=0, last_action=1, action=0, position reward=1.8852\n",
      "entry_point=77.1500 exit_point=81.1800 max_point=84.0000 min_point=75.7600\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-0.8574\n",
      "entry_point=82.4500 exit_point=83.7100 max_point=83.3700 min_point=82.5600\n",
      "entry_state_action=0, last_action=1, action=2, position reward=3.8488\n",
      "entry_point=83.9200 exit_point=91.0300 max_point=90.5600 min_point=84.2600\n",
      "entry_state_action=1, last_action=2, action=2, position reward=-4.0500\n",
      "entry_point=90.5600 exit_point=95.0800 max_point=96.4700 min_point=90.5100\n",
      "Episode: 77 Total reward: -3.8238 Training loss: 0.0151 Explore P: 0.0006 Reward mean: -4.4779 Reward std: 12.9558\n",
      "entry_state_action=0, last_action=1, action=0, position reward=8.2096\n",
      "entry_point=108.3500 exit_point=119.6800 max_point=119.7900 min_point=102.1700\n",
      "Episode: 78 Total reward: 6.1226 Training loss: 0.0282 Explore P: 0.0006 Reward mean: -4.3437 Reward std: 12.9280\n",
      "entry_state_action=0, last_action=2, action=0, position reward=9.9040\n",
      "entry_point=122.1000 exit_point=110.0000 max_point=125.4500 min_point=108.1500\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-3.1000\n",
      "entry_point=108.6000 exit_point=111.3000 max_point=112.1100 min_point=105.3500\n",
      "Episode: 79 Total reward: 2.7900 Training loss: 0.0236 Explore P: 0.0005 Reward mean: -4.2546 Reward std: 12.8714\n",
      "entry_state_action=0, last_action=2, action=1, position reward=15.6688\n",
      "entry_point=116.8400 exit_point=99.8600 max_point=123.9500 min_point=97.0400\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-1.3140\n",
      "entry_point=99.5600 exit_point=99.8500 max_point=100.7200 min_point=98.8300\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-4.0982\n",
      "entry_point=100.2000 exit_point=103.9300 max_point=104.8800 min_point=99.8500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.5400\n",
      "entry_point=102.9100 exit_point=103.3900 max_point=105.3200 min_point=102.8800\n",
      "Episode: 80 Total reward: 7.2998 Training loss: 0.0139 Explore P: 0.0005 Reward mean: -4.1119 Reward std: 12.8552\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-10.1620\n",
      "entry_point=89.3200 exit_point=78.1200 max_point=94.4200 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=2, position reward=1.3100\n",
      "entry_point=79.1000 exit_point=76.8100 max_point=78.1200 min_point=76.9100\n",
      "Episode: 81 Total reward: -11.2984 Training loss: 0.0196 Explore P: 0.0005 Reward mean: -4.1995 Reward std: 12.8009\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-0.3700\n",
      "entry_point=106.7500 exit_point=106.6500 max_point=114.4200 min_point=104.9400\n",
      "Episode: 82 Total reward: -2.0350 Training loss: 0.0117 Explore P: 0.0005 Reward mean: -4.1735 Reward std: 12.7257\n",
      "entry_state_action=0, last_action=2, action=2, position reward=0.6700\n",
      "entry_point=107.8600 exit_point=106.1500 max_point=114.4200 min_point=104.9400\n",
      "Episode: 83 Total reward: -0.4472 Training loss: 0.0241 Explore P: 0.0005 Reward mean: -4.1291 Reward std: 12.6562\n",
      "entry_state_action=0, last_action=2, action=1, position reward=9.1978\n",
      "entry_point=92.6900 exit_point=82.0400 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.2700\n",
      "entry_point=82.1100 exit_point=82.3100 max_point=83.7300 min_point=81.4700\n",
      "Episode: 84 Total reward: 7.2840 Training loss: 0.0170 Explore P: 0.0004 Reward mean: -3.9948 Reward std: 12.6416\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.9698\n",
      "entry_point=103.5300 exit_point=95.3400 max_point=110.2800 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=5.6200\n",
      "entry_point=95.3400 exit_point=89.9400 max_point=100.7800 min_point=87.4700\n",
      "Episode: 85 Total reward: 8.0424 Training loss: 0.0173 Explore P: 0.0004 Reward mean: -3.8549 Reward std: 12.6339\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.3670\n",
      "entry_point=93.8400 exit_point=93.0000 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-0.4430\n",
      "entry_point=93.0000 exit_point=91.8700 max_point=93.2000 min_point=91.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-3.3600\n",
      "entry_point=91.6500 exit_point=88.5100 max_point=92.7800 min_point=89.3200\n",
      "Episode: 86 Total reward: -7.7168 Training loss: 0.0231 Explore P: 0.0004 Reward mean: -3.8993 Reward std: 12.5678\n",
      "entry_state_action=0, last_action=2, action=0, position reward=9.2286\n",
      "entry_point=121.8000 exit_point=111.3500 max_point=122.5200 min_point=99.8100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.6760\n",
      "entry_point=111.1300 exit_point=111.3400 max_point=111.8200 min_point=111.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.1400\n",
      "entry_point=111.8000 exit_point=110.2000 max_point=111.6500 min_point=110.4900\n",
      "Episode: 87 Total reward: -0.6560 Training loss: 0.0294 Explore P: 0.0004 Reward mean: -3.8624 Reward std: 12.5010\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.0130\n",
      "entry_point=95.8500 exit_point=93.0000 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=0.2400\n",
      "entry_point=93.0000 exit_point=92.5800 max_point=93.2000 min_point=92.2500\n",
      "Episode: 88 Total reward: -1.7040 Training loss: 0.0247 Explore P: 0.0004 Reward mean: -3.8382 Reward std: 12.4326\n",
      "entry_state_action=0, last_action=2, action=1, position reward=15.6694\n",
      "entry_point=117.7800 exit_point=97.0400 max_point=123.9500 min_point=97.5300\n",
      "entry_state_action=2, last_action=1, action=2, position reward=0.8456\n",
      "entry_point=97.5300 exit_point=99.5600 max_point=99.7200 min_point=97.0400\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-9.9622\n",
      "entry_point=99.7200 exit_point=104.8200 max_point=108.3000 min_point=98.8300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=2.4312\n",
      "entry_point=106.1100 exit_point=109.9400 max_point=110.1800 min_point=104.8200\n",
      "Episode: 89 Total reward: 8.7684 Training loss: 0.0196 Explore P: 0.0004 Reward mean: -3.6981 Reward std: 12.4338\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.0778\n",
      "entry_point=106.4600 exit_point=107.4600 max_point=107.3900 min_point=107.3900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-6.6350\n",
      "entry_point=107.3900 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-13.9796\n",
      "entry_point=102.2500 exit_point=112.6100 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.3700\n",
      "entry_point=112.9800 exit_point=112.2400 max_point=112.9000 min_point=112.0800\n",
      "Episode: 90 Total reward: -26.1216 Training loss: 0.0178 Explore P: 0.0004 Reward mean: -3.9445 Reward std: 12.5843\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-11.2300\n",
      "entry_point=77.9200 exit_point=89.2200 max_point=88.2000 min_point=75.7600\n",
      "Episode: 91 Total reward: -12.8584 Training loss: 0.0233 Explore P: 0.0003 Reward mean: -4.0414 Reward std: 12.5498\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-8.4868\n",
      "entry_point=90.6000 exit_point=84.4000 max_point=94.4200 min_point=84.3100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=4.1700\n",
      "entry_point=84.3400 exit_point=80.2300 max_point=85.8500 min_point=79.3400\n",
      "Episode: 92 Total reward: -5.6488 Training loss: 0.0221 Explore P: 0.0003 Reward mean: -4.0587 Reward std: 12.4832\n",
      "entry_state_action=0, last_action=1, action=0, position reward=16.0692\n",
      "entry_point=81.1800 exit_point=99.1600 max_point=103.5000 min_point=81.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=1, position reward=-5.9298\n",
      "entry_point=99.1600 exit_point=102.0600 max_point=105.4000 min_point=96.2400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.2300\n",
      "entry_point=101.9900 exit_point=101.8300 max_point=102.6600 min_point=101.4400\n",
      "Episode: 93 Total reward: 7.7326 Training loss: 0.0222 Explore P: 0.0003 Reward mean: -3.9332 Reward std: 12.4754\n",
      "entry_state_action=0, last_action=1, action=0, position reward=7.7206\n",
      "entry_point=109.8100 exit_point=119.8800 max_point=121.1200 min_point=102.1700\n",
      "Episode: 94 Total reward: 5.5644 Training loss: 0.0181 Explore P: 0.0003 Reward mean: -3.8333 Reward std: 12.4474\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.7852\n",
      "entry_point=111.3500 exit_point=112.4500 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.3500\n",
      "entry_point=112.7600 exit_point=112.8000 max_point=113.2800 min_point=111.9500\n",
      "Episode: 95 Total reward: -5.8522 Training loss: 0.0189 Explore P: 0.0003 Reward mean: -3.8543 Reward std: 12.3841\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.6454\n",
      "entry_point=112.3000 exit_point=108.4900 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=0, last_action=1, action=0, position reward=0.0306\n",
      "entry_point=109.3200 exit_point=111.3200 max_point=112.0100 min_point=108.9100\n",
      "Episode: 96 Total reward: -2.0964 Training loss: 0.0155 Explore P: 0.0003 Reward mean: -3.8362 Reward std: 12.3214\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.2200\n",
      "entry_point=117.7400 exit_point=116.5800 max_point=123.9500 min_point=109.6200\n",
      "Episode: 97 Total reward: -1.1948 Training loss: 0.0215 Explore P: 0.0003 Reward mean: -3.8092 Reward std: 12.2612\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-8.1926\n",
      "entry_point=106.7100 exit_point=106.5200 max_point=110.2800 min_point=103.6300\n",
      "entry_state_action=1, last_action=2, action=0, position reward=2.8370\n",
      "entry_point=103.6300 exit_point=100.0600 max_point=106.5200 min_point=99.1300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.9498\n",
      "entry_point=101.6800 exit_point=95.3400 max_point=101.9500 min_point=93.3100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.0200\n",
      "entry_point=92.6800 exit_point=91.6000 max_point=100.7800 min_point=86.3100\n",
      "Episode: 98 Total reward: -7.5372 Training loss: 0.0197 Explore P: 0.0003 Reward mean: -3.8469 Reward std: 12.2048\n",
      "entry_state_action=0, last_action=2, action=0, position reward=3.5358\n",
      "entry_point=115.4300 exit_point=109.0800 max_point=117.7300 min_point=107.3700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=14.8218\n",
      "entry_point=109.7700 exit_point=89.4900 max_point=110.2800 min_point=87.4700\n",
      "entry_state_action=0, last_action=1, action=1, position reward=7.8200\n",
      "entry_point=89.4200 exit_point=98.7600 max_point=99.3000 min_point=90.3000\n",
      "Episode: 99 Total reward: 23.6152 Training loss: 0.0241 Explore P: 0.0003 Reward mean: -3.5722 Reward std: 12.4473\n",
      "entry_state_action=0, last_action=1, action=0, position reward=8.8094\n",
      "entry_point=97.5700 exit_point=108.8600 max_point=108.5300 min_point=97.5500\n",
      "entry_state_action=0, last_action=1, action=0, position reward=5.4380\n",
      "entry_point=113.7900 exit_point=120.9800 max_point=121.6000 min_point=113.7300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.5136\n",
      "entry_point=120.9800 exit_point=118.8900 max_point=119.5200 min_point=117.6800\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.9396\n",
      "entry_point=119.4000 exit_point=120.2300 max_point=120.4800 min_point=119.0700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.1800\n",
      "entry_point=120.4800 exit_point=119.0500 max_point=120.2300 min_point=118.9700\n",
      "Episode: 100 Total reward: 1.1594 Training loss: 0.0239 Explore P: 0.0003 Reward mean: -3.5202 Reward std: 12.4561\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-1.5300\n",
      "entry_point=80.7500 exit_point=82.5600 max_point=84.0000 min_point=75.7600\n",
      "Episode: 101 Total reward: -3.4250 Training loss: 0.0206 Explore P: 0.0002 Reward mean: -3.3720 Reward std: 12.3680\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.7752\n",
      "entry_point=111.4300 exit_point=113.5700 max_point=123.9500 min_point=109.6200\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-3.6676\n",
      "entry_point=117.2500 exit_point=113.8800 max_point=116.7400 min_point=115.3800\n",
      "Episode: 102 Total reward: -12.3364 Training loss: 0.0207 Explore P: 0.0002 Reward mean: -3.2165 Reward std: 12.1545\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-9.4290\n",
      "entry_point=90.6500 exit_point=82.8700 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=1.9432\n",
      "entry_point=82.9500 exit_point=81.3800 max_point=82.9400 min_point=79.3400\n",
      "entry_state_action=0, last_action=2, action=1, position reward=2.9048\n",
      "entry_point=81.3800 exit_point=76.9200 max_point=85.4700 min_point=76.6900\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.1900\n",
      "entry_point=76.7600 exit_point=76.7300 max_point=76.9200 min_point=76.9200\n",
      "Episode: 103 Total reward: -7.7216 Training loss: 0.0148 Explore P: 0.0002 Reward mean: -3.0899 Reward std: 12.0406\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.7632\n",
      "entry_point=88.7400 exit_point=81.3800 max_point=94.4200 min_point=79.3400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-2.4200\n",
      "entry_point=81.6600 exit_point=80.0600 max_point=85.4700 min_point=80.1900\n",
      "Episode: 104 Total reward: 2.8052 Training loss: 0.0200 Explore P: 0.0002 Reward mean: -3.0557 Reward std: 12.0524\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.6740\n",
      "entry_point=123.9500 exit_point=116.5000 max_point=125.4500 min_point=116.3000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.7500\n",
      "entry_point=116.7000 exit_point=112.2500 max_point=120.5500 min_point=112.5000\n",
      "entry_state_action=0, last_action=1, action=0, position reward=3.6934\n",
      "entry_point=105.3500 exit_point=111.3000 max_point=112.1100 min_point=105.4100\n",
      "Episode: 105 Total reward: 1.9164 Training loss: 0.0206 Explore P: 0.0002 Reward mean: -3.0623 Reward std: 12.0495\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.5670\n",
      "entry_point=110.7700 exit_point=110.0600 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.0254\n",
      "entry_point=111.1300 exit_point=108.4900 max_point=110.9100 min_point=107.7300\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.2200\n",
      "entry_point=109.3200 exit_point=109.4300 max_point=109.2100 min_point=109.2100\n",
      "Episode: 106 Total reward: -8.0760 Training loss: 0.0257 Explore P: 0.0002 Reward mean: -2.8694 Reward std: 11.8111\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-2.8240\n",
      "entry_point=112.9300 exit_point=112.7100 max_point=114.3800 min_point=104.9400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.0454\n",
      "entry_point=110.6300 exit_point=108.4900 max_point=114.4200 min_point=107.7300\n",
      "entry_state_action=0, last_action=1, action=0, position reward=0.0306\n",
      "entry_point=109.3200 exit_point=111.3200 max_point=112.0100 min_point=108.9100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.0900\n",
      "entry_point=111.5800 exit_point=111.4000 max_point=111.8200 min_point=109.9300\n",
      "Episode: 107 Total reward: -11.3772 Training loss: 0.0135 Explore P: 0.0002 Reward mean: -2.9220 Reward std: 11.8371\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-10.0238\n",
      "entry_point=109.5400 exit_point=115.7300 max_point=121.1200 min_point=102.1700\n",
      "Episode: 108 Total reward: -12.6946 Training loss: 0.0156 Explore P: 0.0002 Reward mean: -2.7858 Reward std: 11.6439\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.8482\n",
      "entry_point=106.3000 exit_point=106.7400 max_point=114.5000 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=4.1000\n",
      "entry_point=106.9100 exit_point=110.8400 max_point=110.6300 min_point=106.7400\n",
      "Episode: 109 Total reward: -0.9442 Training loss: 0.0147 Explore P: 0.0002 Reward mean: -2.8734 Reward std: 11.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-21.3994\n",
      "entry_point=104.3900 exit_point=123.4700 max_point=125.5000 min_point=101.2400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.1500\n",
      "entry_point=123.0000 exit_point=122.7200 max_point=122.5700 min_point=122.5700\n",
      "Episode: 110 Total reward: -25.8672 Training loss: 0.0246 Explore P: 0.0002 Reward mean: -3.1345 Reward std: 11.8154\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.1674\n",
      "entry_point=110.8000 exit_point=110.1900 max_point=114.3800 min_point=104.9400\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.4394\n",
      "entry_point=109.8700 exit_point=111.4600 max_point=111.7900 min_point=110.1900\n",
      "Episode: 111 Total reward: -6.3186 Training loss: 0.0195 Explore P: 0.0002 Reward mean: -3.1458 Reward std: 11.8179\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-5.8620\n",
      "entry_point=82.0400 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.0464\n",
      "entry_point=77.1300 exit_point=76.9600 max_point=77.4500 min_point=76.6900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.1468\n",
      "entry_point=76.8200 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-3.8152\n",
      "entry_point=78.3400 exit_point=81.0200 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.8900\n",
      "entry_point=80.7600 exit_point=80.1300 max_point=81.6600 min_point=78.1600\n",
      "Episode: 112 Total reward: -13.6438 Training loss: 0.0157 Explore P: 0.0002 Reward mean: -3.0915 Reward std: 11.7569\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-13.6430\n",
      "entry_point=109.7900 exit_point=121.7900 max_point=121.6500 min_point=109.3000\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-3.1806\n",
      "entry_point=121.6500 exit_point=121.5700 max_point=121.7900 min_point=121.0300\n",
      "Episode: 113 Total reward: -19.5294 Training loss: 0.0230 Explore P: 0.0002 Reward mean: -3.2309 Reward std: 11.8678\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-1.9154\n",
      "entry_point=116.4800 exit_point=117.1800 max_point=121.7900 min_point=115.6400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.7648\n",
      "entry_point=117.1800 exit_point=113.5700 max_point=123.9500 min_point=112.7600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.1804\n",
      "entry_point=116.5500 exit_point=111.2800 max_point=117.7300 min_point=109.9800\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.8560\n",
      "entry_point=111.5100 exit_point=108.9100 max_point=114.4800 min_point=107.3700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.3228\n",
      "entry_point=101.1500 exit_point=103.0300 max_point=104.0000 min_point=101.8800\n",
      "Episode: 114 Total reward: -10.3064 Training loss: 0.0119 Explore P: 0.0002 Reward mean: -3.2780 Reward std: 11.8864\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-13.7300\n",
      "entry_point=108.3000 exit_point=120.9000 max_point=121.1200 min_point=102.1700\n",
      "Episode: 115 Total reward: -14.7660 Training loss: 0.0161 Explore P: 0.0001 Reward mean: -3.2011 Reward std: 11.7865\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.3872\n",
      "entry_point=101.9800 exit_point=102.2800 max_point=105.4000 min_point=102.2000\n",
      "entry_state_action=2, last_action=1, action=0, position reward=4.0794\n",
      "entry_point=102.8600 exit_point=108.8600 max_point=108.5300 min_point=101.2400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-12.8936\n",
      "entry_point=106.3500 exit_point=118.8900 max_point=121.6000 min_point=106.8600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-5.0200\n",
      "entry_point=119.9700 exit_point=124.8200 max_point=124.3000 min_point=118.9400\n",
      "Episode: 116 Total reward: -24.5374 Training loss: 0.0205 Explore P: 0.0001 Reward mean: -3.3120 Reward std: 11.9337\n",
      "entry_state_action=0, last_action=1, action=0, position reward=4.1026\n",
      "entry_point=77.8500 exit_point=83.7100 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.7616\n",
      "entry_point=92.6700 exit_point=94.6200 max_point=96.4700 min_point=93.1800\n",
      "entry_state_action=2, last_action=1, action=1, position reward=4.5200\n",
      "entry_point=94.0800 exit_point=99.1400 max_point=99.8000 min_point=92.8800\n",
      "Episode: 117 Total reward: 2.2106 Training loss: 0.0157 Explore P: 0.0001 Reward mean: -3.1893 Reward std: 11.9268\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-24.1894\n",
      "entry_point=102.2800 exit_point=123.4700 max_point=125.5000 min_point=101.2400\n",
      "Episode: 118 Total reward: -25.1950 Training loss: 0.0318 Explore P: 0.0001 Reward mean: -3.4991 Reward std: 12.0909\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.4284\n",
      "entry_point=113.9900 exit_point=118.8800 max_point=121.7900 min_point=109.6200\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-2.3172\n",
      "entry_point=116.8200 exit_point=117.4200 max_point=115.8600 min_point=115.8600\n",
      "Episode: 119 Total reward: -12.7718 Training loss: 0.0159 Explore P: 0.0001 Reward mean: -3.5322 Reward std: 12.1117\n",
      "entry_state_action=0, last_action=2, action=0, position reward=11.0880\n",
      "entry_point=131.5500 exit_point=117.9000 max_point=132.2000 min_point=115.6000\n",
      "Episode: 120 Total reward: 8.4570 Training loss: 0.0185 Explore P: 0.0001 Reward mean: -3.3693 Reward std: 12.1622\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-19.7318\n",
      "entry_point=102.2000 exit_point=120.0700 max_point=125.5000 min_point=101.2600\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-2.4648\n",
      "entry_point=122.7100 exit_point=123.2700 max_point=123.2400 min_point=123.2400\n",
      "Episode: 121 Total reward: -25.7048 Training loss: 0.0184 Explore P: 0.0001 Reward mean: -3.4835 Reward std: 12.3168\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-5.0772\n",
      "entry_point=110.6000 exit_point=108.7000 max_point=112.1100 min_point=103.9500\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-7.0920\n",
      "entry_point=108.6600 exit_point=109.3900 max_point=114.5000 min_point=107.9400\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-0.5904\n",
      "entry_point=111.1000 exit_point=111.6900 max_point=111.0200 min_point=107.1700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=2.1522\n",
      "entry_point=110.7000 exit_point=106.8300 max_point=111.7000 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.8800\n",
      "entry_point=106.8900 exit_point=105.9500 max_point=106.9300 min_point=106.8300\n",
      "Episode: 122 Total reward: -18.7066 Training loss: 0.0170 Explore P: 0.0001 Reward mean: -3.3814 Reward std: 12.1468\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.8128\n",
      "entry_point=120.2700 exit_point=114.7900 max_point=125.5000 min_point=115.3600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.3808\n",
      "entry_point=114.3400 exit_point=107.3900 max_point=113.9700 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.6100\n",
      "entry_point=102.2800 exit_point=101.0500 max_point=102.8500 min_point=101.5600\n",
      "Episode: 123 Total reward: 2.8458 Training loss: 0.0144 Explore P: 0.0001 Reward mean: -3.3539 Reward std: 12.1577\n",
      "entry_state_action=0, last_action=1, action=0, position reward=12.8192\n",
      "entry_point=84.4300 exit_point=99.1600 max_point=103.5000 min_point=84.7300\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-5.2900\n",
      "entry_point=96.7300 exit_point=102.1800 max_point=105.4000 min_point=96.2400\n",
      "Episode: 124 Total reward: 4.0460 Training loss: 0.0156 Explore P: 0.0001 Reward mean: -3.2182 Reward std: 12.1639\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.4202\n",
      "entry_point=104.8800 exit_point=97.5700 max_point=110.2800 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.9002\n",
      "entry_point=96.6600 exit_point=95.3400 max_point=95.0100 min_point=95.0100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.1590\n",
      "entry_point=95.3400 exit_point=92.7200 max_point=95.5600 min_point=92.5500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.2698\n",
      "entry_point=89.8700 exit_point=89.3000 max_point=93.8400 min_point=89.4900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-6.7782\n",
      "entry_point=90.0700 exit_point=95.4900 max_point=100.7800 min_point=88.9000\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.7192\n",
      "entry_point=94.1600 exit_point=94.5700 max_point=98.6100 min_point=92.9100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-3.4900\n",
      "entry_point=95.4600 exit_point=91.0800 max_point=94.7500 min_point=88.4800\n",
      "Episode: 125 Total reward: -22.0478 Training loss: 0.0116 Explore P: 0.0001 Reward mean: -3.2467 Reward std: 12.2046\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.2000\n",
      "entry_point=110.2600 exit_point=108.3100 max_point=114.4200 min_point=104.9400\n",
      "Episode: 126 Total reward: -0.2552 Training loss: 0.0169 Explore P: 0.0001 Reward mean: -3.1892 Reward std: 12.2050\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-12.2808\n",
      "entry_point=91.1400 exit_point=83.3800 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=2.4532\n",
      "entry_point=82.0400 exit_point=81.3800 max_point=83.7300 min_point=79.3400\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-1.6496\n",
      "entry_point=81.6600 exit_point=83.1500 max_point=82.4800 min_point=82.4800\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-3.0000\n",
      "entry_point=85.0000 exit_point=81.3100 max_point=84.3100 min_point=80.0600\n",
      "Episode: 127 Total reward: -19.3032 Training loss: 0.0152 Explore P: 0.0001 Reward mean: -3.1385 Reward std: 12.1272\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.0132\n",
      "entry_point=88.8100 exit_point=81.3800 max_point=88.9400 min_point=79.3400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.7048\n",
      "entry_point=81.3800 exit_point=79.1500 max_point=85.4700 min_point=78.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=2.4000\n",
      "entry_point=79.1500 exit_point=76.6600 max_point=79.2700 min_point=75.7600\n",
      "Episode: 128 Total reward: 6.4312 Training loss: 0.0212 Explore P: 0.0001 Reward mean: -3.1566 Reward std: 12.1115\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-3.4274\n",
      "entry_point=109.2500 exit_point=110.1900 max_point=114.3800 min_point=104.9400\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.4394\n",
      "entry_point=109.8700 exit_point=111.4600 max_point=111.7900 min_point=110.1900\n",
      "entry_state_action=0, last_action=1, action=1, position reward=1.7100\n",
      "entry_point=111.7900 exit_point=114.0200 max_point=113.8600 min_point=112.3100\n",
      "Episode: 129 Total reward: -6.1276 Training loss: 0.0141 Explore P: 0.0001 Reward mean: -3.0733 Reward std: 12.0621\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-2.4816\n",
      "entry_point=108.6900 exit_point=109.2000 max_point=109.7400 min_point=107.9900\n",
      "entry_state_action=0, last_action=2, action=1, position reward=0.2522\n",
      "entry_point=109.2000 exit_point=107.4600 max_point=111.7300 min_point=106.4600\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-6.6350\n",
      "entry_point=107.3900 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-16.1832\n",
      "entry_point=102.2500 exit_point=112.7900 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.3248\n",
      "entry_point=114.1600 exit_point=113.9500 max_point=115.1600 min_point=111.7700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-2.9602\n",
      "entry_point=108.8700 exit_point=109.4800 max_point=114.2400 min_point=108.3200\n",
      "Episode: 130 Total reward: -36.0778 Training loss: 0.0181 Explore P: 0.0001 Reward mean: -3.2545 Reward std: 12.4152\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.8334\n",
      "entry_point=118.2000 exit_point=107.2100 max_point=120.5500 min_point=105.3500\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-10.6100\n",
      "entry_point=112.7600 exit_point=102.8800 max_point=114.5000 min_point=103.2700\n",
      "Episode: 131 Total reward: -5.2658 Training loss: 0.0153 Explore P: 0.0001 Reward mean: -3.3244 Reward std: 12.4067\n",
      "entry_state_action=0, last_action=2, action=0, position reward=14.8922\n",
      "entry_point=124.2900 exit_point=107.4600 max_point=124.4500 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.1978\n",
      "entry_point=108.3700 exit_point=107.3900 max_point=110.9400 min_point=106.6100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.7566\n",
      "entry_point=115.2200 exit_point=114.1600 max_point=115.0300 min_point=114.3300\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.4700\n",
      "entry_point=114.3300 exit_point=114.6300 max_point=115.1600 min_point=112.7700\n",
      "Episode: 132 Total reward: 6.3258 Training loss: 0.0156 Explore P: 0.0001 Reward mean: -3.3966 Reward std: 12.3291\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.2362\n",
      "entry_point=118.7700 exit_point=109.3800 max_point=125.5000 min_point=108.0800\n",
      "Episode: 133 Total reward: 5.5308 Training loss: 0.0142 Explore P: 0.0001 Reward mean: -3.2707 Reward std: 12.3553\n",
      "entry_state_action=0, last_action=2, action=0, position reward=12.1458\n",
      "entry_point=123.1500 exit_point=109.0800 max_point=123.9500 min_point=107.3700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.1946\n",
      "entry_point=109.7700 exit_point=103.0900 max_point=108.2700 min_point=104.7700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=3.1194\n",
      "entry_point=101.6600 exit_point=97.0400 max_point=103.0900 min_point=97.5300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=6.3108\n",
      "entry_point=97.5300 exit_point=106.0700 max_point=110.2800 min_point=97.0400\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.8630\n",
      "entry_point=106.2800 exit_point=100.0600 max_point=105.0700 min_point=99.1300\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-2.0012\n",
      "entry_point=101.6500 exit_point=101.6800 max_point=100.0600 min_point=100.0600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.0200\n",
      "entry_point=97.9400 exit_point=97.5700 max_point=99.7500 min_point=97.1300\n",
      "Episode: 134 Total reward: 5.1804 Training loss: 0.0137 Explore P: 0.0001 Reward mean: -3.2749 Reward std: 12.3523\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.6164\n",
      "entry_point=98.0800 exit_point=88.5700 max_point=99.3300 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=2.0192\n",
      "entry_point=88.2600 exit_point=84.3100 max_point=94.4200 min_point=84.3300\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.6324\n",
      "entry_point=84.5400 exit_point=85.8500 max_point=85.6200 min_point=83.0500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=1.5002\n",
      "entry_point=85.6200 exit_point=82.2800 max_point=85.8500 min_point=82.4900\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.4800\n",
      "entry_point=82.4900 exit_point=81.8000 max_point=82.2800 min_point=82.2800\n",
      "Episode: 135 Total reward: 5.5766 Training loss: 0.0182 Explore P: 0.0001 Reward mean: -3.0254 Reward std: 12.2763\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.9682\n",
      "entry_point=106.1500 exit_point=106.7400 max_point=114.5000 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=1.7700\n",
      "entry_point=106.9100 exit_point=108.5100 max_point=109.4700 min_point=106.7400\n",
      "Episode: 136 Total reward: -3.4212 Training loss: 0.0165 Explore P: 0.0001 Reward mean: -2.8935 Reward std: 12.2003\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.5864\n",
      "entry_point=96.9900 exit_point=88.5700 max_point=99.3300 min_point=86.3100\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.2324\n",
      "entry_point=88.4800 exit_point=85.8500 max_point=94.4200 min_point=83.0500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=1.5002\n",
      "entry_point=85.6200 exit_point=82.2800 max_point=85.8500 min_point=82.4900\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.6900\n",
      "entry_point=82.4900 exit_point=81.5900 max_point=82.2800 min_point=81.8000\n",
      "Episode: 137 Total reward: -1.6752 Training loss: 0.0189 Explore P: 0.0001 Reward mean: -3.0735 Reward std: 12.0471\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-8.2338\n",
      "entry_point=87.7000 exit_point=80.0600 max_point=87.7000 min_point=79.3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=1, last_action=2, action=0, position reward=-0.2752\n",
      "entry_point=80.1900 exit_point=79.1500 max_point=81.3100 min_point=78.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=2.1700\n",
      "entry_point=79.1500 exit_point=76.8900 max_point=79.2700 min_point=75.7600\n",
      "Episode: 138 Total reward: -10.3360 Training loss: 0.0110 Explore P: 0.0001 Reward mean: -3.1618 Reward std: 12.0677\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-2.3212\n",
      "entry_point=118.8500 exit_point=119.7200 max_point=121.1200 min_point=109.6200\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.7784\n",
      "entry_point=115.6800 exit_point=118.8800 max_point=121.7900 min_point=114.9100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=1.0500\n",
      "entry_point=118.4100 exit_point=119.5900 max_point=119.7800 min_point=118.5400\n",
      "Episode: 139 Total reward: -13.1584 Training loss: 0.0187 Explore P: 0.0001 Reward mean: -3.3887 Reward std: 12.0402\n",
      "entry_state_action=0, last_action=1, action=0, position reward=22.9128\n",
      "entry_point=78.2600 exit_point=102.2800 max_point=105.4000 min_point=77.6100\n",
      "Episode: 140 Total reward: 20.9776 Training loss: 0.0161 Explore P: 0.0001 Reward mean: -2.9445 Reward std: 12.1114\n",
      "entry_state_action=0, last_action=1, action=2, position reward=21.7454\n",
      "entry_point=79.9700 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=3.9740\n",
      "entry_point=102.2300 exit_point=94.6300 max_point=101.1800 min_point=95.3000\n",
      "entry_state_action=0, last_action=2, action=1, position reward=1.2562\n",
      "entry_point=99.0700 exit_point=99.5500 max_point=100.8600 min_point=96.2400\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.8300\n",
      "entry_point=98.1900 exit_point=98.7200 max_point=99.5500 min_point=96.7800\n",
      "Episode: 141 Total reward: 21.1048 Training loss: 0.0252 Explore P: 0.0001 Reward mean: -2.7990 Reward std: 12.3104\n",
      "entry_state_action=0, last_action=1, action=2, position reward=22.5154\n",
      "entry_point=79.1000 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=4.6574\n",
      "entry_point=102.2300 exit_point=94.6800 max_point=101.1800 min_point=94.6300\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-4.7300\n",
      "entry_point=99.0700 exit_point=104.7800 max_point=105.4000 min_point=96.2400\n",
      "Episode: 142 Total reward: 17.5194 Training loss: 0.0224 Explore P: 0.0001 Reward mean: -2.3328 Reward std: 12.1877\n",
      "entry_state_action=0, last_action=1, action=0, position reward=14.1738\n",
      "entry_point=102.6100 exit_point=119.0000 max_point=121.6000 min_point=101.2600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.6446\n",
      "entry_point=119.0000 exit_point=118.9000 max_point=125.5000 min_point=116.4400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-0.4300\n",
      "entry_point=119.7700 exit_point=119.5400 max_point=120.4800 min_point=118.9900\n",
      "Episode: 143 Total reward: 1.6016 Training loss: 0.0172 Explore P: 0.0001 Reward mean: -2.4154 Reward std: 12.1327\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-9.7452\n",
      "entry_point=91.1400 exit_point=83.3200 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-1.5200\n",
      "entry_point=82.3900 exit_point=80.7500 max_point=85.4700 min_point=79.3400\n",
      "Episode: 144 Total reward: -14.6558 Training loss: 0.0091 Explore P: 0.0001 Reward mean: -2.4359 Reward std: 12.1516\n",
      "entry_state_action=0, last_action=1, action=0, position reward=17.9812\n",
      "entry_point=76.2400 exit_point=95.3000 max_point=103.5000 min_point=76.5300\n",
      "Episode: 145 Total reward: 16.7464 Training loss: 0.0172 Explore P: 0.0000 Reward mean: -2.2101 Reward std: 12.2953\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.3210\n",
      "entry_point=99.9900 exit_point=93.0600 max_point=102.8200 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.8738\n",
      "entry_point=93.0600 exit_point=86.7600 max_point=97.2100 min_point=86.3100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=5.0900\n",
      "entry_point=90.1600 exit_point=94.2700 max_point=94.4200 min_point=88.2600\n",
      "Episode: 146 Total reward: 8.6506 Training loss: 0.0175 Explore P: 0.0000 Reward mean: -2.1777 Reward std: 12.3196\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-16.1438\n",
      "entry_point=103.7300 exit_point=115.7300 max_point=121.1200 min_point=102.1700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.5424\n",
      "entry_point=115.9300 exit_point=111.4300 max_point=119.1100 min_point=111.8800\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-0.5424\n",
      "entry_point=110.4400 exit_point=110.9400 max_point=110.4900 min_point=109.6200\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-0.8174\n",
      "entry_point=109.6200 exit_point=112.7000 max_point=112.8000 min_point=110.8800\n",
      "Episode: 147 Total reward: -21.9732 Training loss: 0.0113 Explore P: 0.0000 Reward mean: -2.4731 Reward std: 12.4360\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-0.2920\n",
      "entry_point=121.0000 exit_point=121.9000 max_point=122.4000 min_point=117.2500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=10.3630\n",
      "entry_point=122.1000 exit_point=108.8000 max_point=125.4500 min_point=108.1500\n",
      "Episode: 148 Total reward: 6.8010 Training loss: 0.0129 Explore P: 0.0000 Reward mean: -2.4154 Reward std: 12.4655\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.3502\n",
      "entry_point=103.4300 exit_point=97.5700 max_point=110.2800 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.7494\n",
      "entry_point=95.3400 exit_point=87.4700 max_point=95.5600 min_point=89.0300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.2698\n",
      "entry_point=89.8700 exit_point=89.3000 max_point=93.8400 min_point=89.4900\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-9.3384\n",
      "entry_point=90.0700 exit_point=98.5000 max_point=99.3000 min_point=88.9000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.8710\n",
      "entry_point=97.8700 exit_point=93.0600 max_point=100.7800 min_point=92.9500\n",
      "Episode: 149 Total reward: -9.6792 Training loss: 0.0157 Explore P: 0.0000 Reward mean: -2.5451 Reward std: 12.4729\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.3018\n",
      "entry_point=99.9900 exit_point=95.4900 max_point=102.8200 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=4.3268\n",
      "entry_point=96.0300 exit_point=88.5400 max_point=98.6100 min_point=86.3100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=5.7300\n",
      "entry_point=88.6600 exit_point=94.2700 max_point=94.4200 min_point=88.5400\n",
      "Episode: 150 Total reward: 9.4482 Training loss: 0.0131 Explore P: 0.0000 Reward mean: -2.2146 Reward std: 12.3477\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-13.2412\n",
      "entry_point=90.9900 exit_point=79.2700 max_point=90.5100 min_point=78.7600\n",
      "entry_state_action=1, last_action=2, action=2, position reward=1.5700\n",
      "entry_point=79.0600 exit_point=77.7000 max_point=79.2700 min_point=75.7600\n",
      "Episode: 151 Total reward: -13.9710 Training loss: 0.0170 Explore P: 0.0000 Reward mean: -2.1251 Reward std: 12.2292\n",
      "entry_state_action=0, last_action=1, action=2, position reward=21.1054\n",
      "entry_point=79.4700 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=4.6574\n",
      "entry_point=102.2300 exit_point=94.6800 max_point=101.1800 min_point=94.6300\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-1.9300\n",
      "entry_point=99.0700 exit_point=101.9800 max_point=102.9100 min_point=96.2400\n",
      "Episode: 152 Total reward: 19.9420 Training loss: 0.0144 Explore P: 0.0000 Reward mean: -2.0188 Reward std: 12.3735\n",
      "entry_state_action=0, last_action=1, action=2, position reward=1.5220\n",
      "entry_point=118.6300 exit_point=119.7200 max_point=121.1900 min_point=118.1400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=10.1822\n",
      "entry_point=120.9000 exit_point=107.4600 max_point=119.7200 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.1970\n",
      "entry_point=109.1700 exit_point=101.2500 max_point=110.9400 min_point=100.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=2, position reward=-13.5200\n",
      "entry_point=100.4300 exit_point=114.1400 max_point=118.1400 min_point=100.1300\n",
      "Episode: 153 Total reward: -3.0634 Training loss: 0.0106 Explore P: 0.0000 Reward mean: -2.0636 Reward std: 12.3691\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-10.4638\n",
      "entry_point=109.1500 exit_point=115.7300 max_point=121.1200 min_point=102.1700\n",
      "Episode: 154 Total reward: -13.0768 Training loss: 0.0155 Explore P: 0.0000 Reward mean: -2.2775 Reward std: 12.3727\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-6.1284\n",
      "entry_point=115.9300 exit_point=118.8800 max_point=121.7900 min_point=109.6200\n",
      "entry_state_action=0, last_action=1, action=1, position reward=5.1400\n",
      "entry_point=118.4100 exit_point=123.6800 max_point=123.7200 min_point=118.5400\n",
      "Episode: 155 Total reward: -4.2752 Training loss: 0.0091 Explore P: 0.0000 Reward mean: -2.4668 Reward std: 12.2565\n",
      "entry_state_action=0, last_action=2, action=2, position reward=11.0400\n",
      "entry_point=119.7000 exit_point=108.8100 max_point=125.1000 min_point=103.9500\n",
      "Episode: 156 Total reward: 8.4960 Training loss: 0.0097 Explore P: 0.0000 Reward mean: -2.5352 Reward std: 12.1758\n",
      "entry_state_action=0, last_action=1, action=1, position reward=20.0800\n",
      "entry_point=76.8700 exit_point=96.9400 max_point=96.4700 min_point=75.7600\n",
      "Episode: 157 Total reward: 18.5326 Training loss: 0.0234 Explore P: 0.0000 Reward mean: -2.4644 Reward std: 12.2771\n",
      "entry_state_action=0, last_action=2, action=2, position reward=3.0900\n",
      "entry_point=112.4500 exit_point=108.4300 max_point=114.4200 min_point=104.9400\n",
      "Episode: 158 Total reward: 1.7710 Training loss: 0.0312 Explore P: 0.0000 Reward mean: -2.4626 Reward std: 12.2777\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-17.4588\n",
      "entry_point=101.9000 exit_point=117.3900 max_point=121.6000 min_point=101.2600\n",
      "Episode: 159 Total reward: -18.9068 Training loss: 0.0161 Explore P: 0.0000 Reward mean: -2.8036 Reward std: 12.2561\n",
      "entry_state_action=0, last_action=2, action=1, position reward=5.0168\n",
      "entry_point=83.4800 exit_point=77.7600 max_point=85.4700 min_point=75.7600\n",
      "entry_state_action=2, last_action=1, action=2, position reward=2.9526\n",
      "entry_point=77.6600 exit_point=82.3800 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=1.3430\n",
      "entry_point=82.3700 exit_point=80.2600 max_point=82.3800 min_point=79.8500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.5900\n",
      "entry_point=79.8500 exit_point=77.6700 max_point=80.4300 min_point=77.7800\n",
      "Episode: 160 Total reward: 4.4028 Training loss: 0.0139 Explore P: 0.0000 Reward mean: -2.5820 Reward std: 12.1838\n",
      "entry_state_action=0, last_action=1, action=0, position reward=7.8664\n",
      "entry_point=106.8700 exit_point=117.7400 max_point=121.1200 min_point=106.7400\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.4990\n",
      "entry_point=116.9300 exit_point=118.7200 max_point=119.7600 min_point=109.6200\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.6100\n",
      "entry_point=118.4500 exit_point=119.3300 max_point=118.7200 min_point=118.7200\n",
      "Episode: 161 Total reward: 0.9714 Training loss: 0.0161 Explore P: 0.0000 Reward mean: -2.6214 Reward std: 12.1658\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-16.2288\n",
      "entry_point=82.7500 exit_point=95.3000 max_point=103.5000 min_point=77.6100\n",
      "Episode: 162 Total reward: -17.2738 Training loss: 0.0100 Explore P: 0.0000 Reward mean: -2.6871 Reward std: 12.2269\n",
      "entry_state_action=0, last_action=2, action=1, position reward=9.1148\n",
      "entry_point=123.3000 exit_point=113.4900 max_point=123.9500 min_point=103.9500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.7798\n",
      "entry_point=112.7600 exit_point=114.3100 max_point=114.5000 min_point=112.7800\n",
      "Episode: 163 Total reward: 4.7690 Training loss: 0.0150 Explore P: 0.0000 Reward mean: -2.6168 Reward std: 12.2493\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.0618\n",
      "entry_point=119.8100 exit_point=113.3300 max_point=125.5000 min_point=112.4100\n",
      "Episode: 164 Total reward: 2.7556 Training loss: 0.0218 Explore P: 0.0000 Reward mean: -2.5337 Reward std: 12.2573\n",
      "entry_state_action=0, last_action=1, action=0, position reward=11.3822\n",
      "entry_point=105.5700 exit_point=118.6100 max_point=121.6000 min_point=105.1300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.7118\n",
      "entry_point=118.6100 exit_point=120.0700 max_point=125.5000 min_point=116.4400\n",
      "Episode: 165 Total reward: 3.5968 Training loss: 0.0123 Explore P: 0.0000 Reward mean: -2.3742 Reward std: 12.2322\n",
      "entry_state_action=0, last_action=2, action=0, position reward=10.9432\n",
      "entry_point=91.6500 exit_point=81.3800 max_point=92.7800 min_point=79.3400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.7048\n",
      "entry_point=81.3800 exit_point=79.1500 max_point=85.4700 min_point=78.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-1.0900\n",
      "entry_point=77.1700 exit_point=78.0800 max_point=78.3400 min_point=75.7600\n",
      "Episode: 166 Total reward: 5.8540 Training loss: 0.0114 Explore P: 0.0000 Reward mean: -2.4542 Reward std: 12.1518\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-17.4338\n",
      "entry_point=102.8800 exit_point=115.7300 max_point=121.1200 min_point=102.1700\n",
      "Episode: 167 Total reward: -19.2214 Training loss: 0.0121 Explore P: 0.0000 Reward mean: -2.6818 Reward std: 12.2502\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.1218\n",
      "entry_point=101.8600 exit_point=95.4900 max_point=101.9500 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.8438\n",
      "entry_point=96.0300 exit_point=86.7600 max_point=98.6100 min_point=86.3100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=4.4500\n",
      "entry_point=90.1600 exit_point=93.6300 max_point=94.4200 min_point=88.2600\n",
      "Episode: 168 Total reward: 9.7346 Training loss: 0.0079 Explore P: 0.0000 Reward mean: -2.7683 Reward std: 12.1311\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-20.7000\n",
      "entry_point=102.6800 exit_point=123.8500 max_point=125.5000 min_point=101.2400\n",
      "Episode: 169 Total reward: -23.2236 Training loss: 0.0155 Explore P: 0.0000 Reward mean: -2.8228 Reward std: 12.2104\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-4.5226\n",
      "entry_point=113.2900 exit_point=110.9100 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=1, last_action=2, action=2, position reward=2.5000\n",
      "entry_point=111.1300 exit_point=108.4100 max_point=112.2800 min_point=107.7300\n",
      "Episode: 170 Total reward: -3.9284 Training loss: 0.0139 Explore P: 0.0000 Reward mean: -3.0035 Reward std: 12.0912\n",
      "entry_state_action=0, last_action=1, action=0, position reward=12.5522\n",
      "entry_point=103.8200 exit_point=118.6100 max_point=121.6000 min_point=103.7400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.7118\n",
      "entry_point=118.6100 exit_point=120.0700 max_point=125.5000 min_point=116.4400\n",
      "Episode: 171 Total reward: 5.3818 Training loss: 0.0156 Explore P: 0.0000 Reward mean: -2.8598 Reward std: 12.1046\n",
      "entry_state_action=0, last_action=1, action=1, position reward=4.9400\n",
      "entry_point=106.4700 exit_point=112.1100 max_point=114.5000 min_point=102.1700\n",
      "Episode: 172 Total reward: 3.5106 Training loss: 0.0200 Explore P: 0.0000 Reward mean: -2.8585 Reward std: 12.1052\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-6.1284\n",
      "entry_point=115.9300 exit_point=118.8800 max_point=121.7900 min_point=109.6200\n",
      "Episode: 173 Total reward: -7.1770 Training loss: 0.0118 Explore P: 0.0000 Reward mean: -2.8307 Reward std: 12.0921\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.9682\n",
      "entry_point=106.1500 exit_point=106.7400 max_point=114.5000 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=1.9800\n",
      "entry_point=106.9100 exit_point=108.7200 max_point=109.4700 min_point=106.7400\n",
      "Episode: 174 Total reward: -3.2112 Training loss: 0.0176 Explore P: 0.0000 Reward mean: -2.9236 Reward std: 12.0589\n",
      "entry_state_action=0, last_action=1, action=2, position reward=2.8718\n",
      "entry_point=90.9400 exit_point=96.1500 max_point=99.3000 min_point=90.3000\n",
      "entry_state_action=1, last_action=2, action=1, position reward=6.5022\n",
      "entry_point=96.4100 exit_point=87.8900 max_point=100.7800 min_point=86.3100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=2, last_action=1, action=1, position reward=-1.2000\n",
      "entry_point=87.8900 exit_point=86.6900 max_point=88.9900 min_point=86.6500\n",
      "Episode: 175 Total reward: 7.2852 Training loss: 0.0169 Explore P: 0.0000 Reward mean: -3.0256 Reward std: 11.9283\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.0448\n",
      "entry_point=114.4000 exit_point=113.5700 max_point=123.9500 min_point=112.7600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.8560\n",
      "entry_point=111.5100 exit_point=108.9100 max_point=114.4800 min_point=107.3700\n",
      "Episode: 176 Total reward: -5.1094 Training loss: 0.0126 Explore P: 0.0000 Reward mean: -3.0977 Reward std: 11.9189\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.3556\n",
      "entry_point=115.6100 exit_point=115.6400 max_point=121.7900 min_point=113.9900\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.4494\n",
      "entry_point=115.6400 exit_point=118.5400 max_point=123.9500 min_point=115.8600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.0808\n",
      "entry_point=111.5100 exit_point=108.6800 max_point=111.0300 min_point=108.5400\n",
      "Episode: 177 Total reward: -16.2910 Training loss: 0.0164 Explore P: 0.0000 Reward mean: -3.2224 Reward std: 11.9908\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.7964\n",
      "entry_point=98.5000 exit_point=88.5700 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=0.5970\n",
      "entry_point=88.2600 exit_point=87.1500 max_point=94.4200 min_point=86.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.7500\n",
      "entry_point=86.6500 exit_point=84.4000 max_point=87.7000 min_point=84.3100\n",
      "Episode: 178 Total reward: 1.4282 Training loss: 0.0176 Explore P: 0.0000 Reward mean: -3.2693 Reward std: 11.9633\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-11.7100\n",
      "entry_point=90.9800 exit_point=77.9200 max_point=94.4200 min_point=78.2900\n",
      "Episode: 179 Total reward: -14.8796 Training loss: 0.0123 Explore P: 0.0000 Reward mean: -3.4460 Reward std: 12.0029\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-2.3282\n",
      "entry_point=107.1900 exit_point=106.7400 max_point=114.5000 min_point=102.1700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=5.6100\n",
      "entry_point=106.9100 exit_point=112.3500 max_point=112.3000 min_point=106.7400\n",
      "Episode: 180 Total reward: 1.4380 Training loss: 0.0152 Explore P: 0.0000 Reward mean: -3.5046 Reward std: 11.9645\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-9.3438\n",
      "entry_point=87.6000 exit_point=80.0600 max_point=88.9900 min_point=79.3400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-0.2752\n",
      "entry_point=80.1900 exit_point=79.1500 max_point=81.3100 min_point=78.7600\n",
      "entry_state_action=0, last_action=2, action=2, position reward=0.9300\n",
      "entry_point=79.1500 exit_point=78.1300 max_point=79.2700 min_point=75.7600\n",
      "Episode: 181 Total reward: -11.4740 Training loss: 0.0156 Explore P: 0.0000 Reward mean: -3.5064 Reward std: 11.9657\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.3784\n",
      "entry_point=91.1500 exit_point=90.2100 max_point=91.2400 min_point=88.9200\n",
      "entry_state_action=1, last_action=2, action=1, position reward=6.3878\n",
      "entry_point=88.9200 exit_point=82.0400 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.0700\n",
      "entry_point=82.1100 exit_point=82.1100 max_point=83.7300 min_point=81.4700\n",
      "Episode: 182 Total reward: -0.6636 Training loss: 0.0120 Explore P: 0.0000 Reward mean: -3.4927 Reward std: 11.9682\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.7804\n",
      "entry_point=82.8000 exit_point=79.8900 max_point=84.0000 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=2, position reward=1.5800\n",
      "entry_point=80.0200 exit_point=78.3100 max_point=80.4300 min_point=77.6100\n",
      "Episode: 183 Total reward: -4.3264 Training loss: 0.0126 Explore P: 0.0000 Reward mean: -3.5315 Reward std: 11.9645\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-1.2178\n",
      "entry_point=102.6400 exit_point=106.5000 max_point=110.2800 min_point=101.1500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=9.5898\n",
      "entry_point=104.3900 exit_point=95.3400 max_point=106.9600 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.1590\n",
      "entry_point=95.3400 exit_point=92.7200 max_point=95.5600 min_point=92.5500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.0760\n",
      "entry_point=89.8700 exit_point=89.2500 max_point=93.8400 min_point=89.3000\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-7.2092\n",
      "entry_point=90.0700 exit_point=94.5700 max_point=100.7800 min_point=88.9000\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-4.3600\n",
      "entry_point=95.4600 exit_point=90.2100 max_point=94.7500 min_point=88.9200\n",
      "Episode: 184 Total reward: -13.2426 Training loss: 0.0124 Explore P: 0.0000 Reward mean: -3.7367 Reward std: 11.9533\n",
      "entry_state_action=0, last_action=2, action=1, position reward=9.5722\n",
      "entry_point=119.2700 exit_point=107.4600 max_point=121.1900 min_point=106.4600\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-6.6350\n",
      "entry_point=107.3900 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-1.5430\n",
      "entry_point=102.2500 exit_point=101.2500 max_point=107.2800 min_point=100.8300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-12.6714\n",
      "entry_point=100.4300 exit_point=111.3500 max_point=118.1400 min_point=100.1300\n",
      "Episode: 185 Total reward: -15.6312 Training loss: 0.0122 Explore P: 0.0000 Reward mean: -3.9735 Reward std: 11.9521\n",
      "entry_state_action=0, last_action=1, action=0, position reward=12.4858\n",
      "entry_point=103.1100 exit_point=118.0900 max_point=118.2100 min_point=101.2400\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-4.6400\n",
      "entry_point=120.9800 exit_point=124.1600 max_point=124.8200 min_point=116.4400\n",
      "Episode: 186 Total reward: 5.0740 Training loss: 0.0102 Explore P: 0.0000 Reward mean: -3.8456 Reward std: 11.9797\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-7.2800\n",
      "entry_point=98.9200 exit_point=92.8200 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=6.9700\n",
      "entry_point=93.0000 exit_point=85.8500 max_point=93.2000 min_point=83.0500\n",
      "Episode: 187 Total reward: -2.6084 Training loss: 0.0120 Explore P: 0.0000 Reward mean: -3.8651 Reward std: 11.9761\n",
      "entry_state_action=0, last_action=2, action=1, position reward=7.4302\n",
      "entry_point=108.3300 exit_point=97.5700 max_point=108.5100 min_point=93.3100\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-8.7942\n",
      "entry_point=97.4900 exit_point=89.8400 max_point=100.7800 min_point=87.4700\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.8314\n",
      "entry_point=90.2100 exit_point=90.2600 max_point=93.4000 min_point=86.3100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-0.9400\n",
      "entry_point=89.5700 exit_point=89.3200 max_point=91.8500 min_point=89.0200\n",
      "Episode: 188 Total reward: -3.7620 Training loss: 0.0163 Explore P: 0.0000 Reward mean: -3.8857 Reward std: 11.9742\n",
      "entry_state_action=0, last_action=1, action=0, position reward=10.8612\n",
      "entry_point=83.5600 exit_point=95.3000 max_point=103.5000 min_point=77.6100\n",
      "Episode: 189 Total reward: 9.2800 Training loss: 0.0179 Explore P: 0.0000 Reward mean: -3.8805 Reward std: 11.9797\n",
      "entry_state_action=0, last_action=1, action=0, position reward=9.1842\n",
      "entry_point=105.0200 exit_point=114.3400 max_point=121.1200 min_point=102.1700\n",
      "Episode: 190 Total reward: 6.3538 Training loss: 0.0139 Explore P: 0.0000 Reward mean: -3.5558 Reward std: 11.8113\n",
      "entry_state_action=0, last_action=1, action=2, position reward=0.9018\n",
      "entry_point=93.1100 exit_point=96.1500 max_point=99.3000 min_point=88.9000\n",
      "entry_state_action=1, last_action=2, action=0, position reward=1.9530\n",
      "entry_point=96.4100 exit_point=93.0000 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-0.4430\n",
      "entry_point=93.0000 exit_point=91.8700 max_point=93.2000 min_point=91.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.5100\n",
      "entry_point=91.6500 exit_point=89.3600 max_point=92.7800 min_point=89.3200\n",
      "Episode: 191 Total reward: -2.9104 Training loss: 0.0141 Explore P: 0.0000 Reward mean: -3.4563 Reward std: 11.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=2, action=0, position reward=-14.7948\n",
      "entry_point=91.0300 exit_point=102.9500 max_point=105.4000 min_point=90.5100\n",
      "Episode: 192 Total reward: -16.0954 Training loss: 0.0109 Explore P: 0.0000 Reward mean: -3.5608 Reward std: 11.8396\n",
      "entry_state_action=0, last_action=2, action=0, position reward=11.2522\n",
      "entry_point=121.1400 exit_point=107.4600 max_point=123.5600 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.7670\n",
      "entry_point=108.3700 exit_point=101.2500 max_point=110.9400 min_point=100.8300\n",
      "Episode: 193 Total reward: 12.2090 Training loss: 0.0198 Explore P: 0.0000 Reward mean: -3.5160 Reward std: 11.8905\n",
      "entry_state_action=0, last_action=2, action=0, position reward=11.2522\n",
      "entry_point=121.1400 exit_point=107.4600 max_point=123.5600 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.7670\n",
      "entry_point=108.3700 exit_point=101.2500 max_point=110.9400 min_point=100.8300\n",
      "Episode: 194 Total reward: 12.2090 Training loss: 0.0158 Explore P: 0.0000 Reward mean: -3.4496 Reward std: 11.9594\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-11.9804\n",
      "entry_point=91.2300 exit_point=82.0800 max_point=92.7800 min_point=79.3400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=1.7448\n",
      "entry_point=81.5200 exit_point=79.1500 max_point=82.1300 min_point=78.7600\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-1.3068\n",
      "entry_point=79.1000 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=2, position reward=0.7000\n",
      "entry_point=78.3400 exit_point=77.6000 max_point=78.3000 min_point=77.7100\n",
      "Episode: 195 Total reward: -15.1490 Training loss: 0.0191 Explore P: 0.0000 Reward mean: -3.5425 Reward std: 12.0138\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.5768\n",
      "entry_point=82.3700 exit_point=78.3000 max_point=85.4700 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.8132\n",
      "entry_point=78.3400 exit_point=77.7600 max_point=78.3000 min_point=76.9200\n",
      "entry_state_action=2, last_action=1, action=2, position reward=2.9526\n",
      "entry_point=77.6600 exit_point=82.3800 max_point=84.0000 min_point=76.1500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=0.2648\n",
      "entry_point=82.3700 exit_point=81.0200 max_point=82.3800 min_point=80.7600\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.6400\n",
      "entry_point=80.7600 exit_point=79.3800 max_point=81.6600 min_point=78.0800\n",
      "Episode: 196 Total reward: -6.4400 Training loss: 0.0219 Explore P: 0.0000 Reward mean: -3.5860 Reward std: 12.0163\n",
      "entry_state_action=0, last_action=1, action=2, position reward=8.9442\n",
      "entry_point=102.4100 exit_point=113.7300 max_point=113.7900 min_point=101.2600\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-10.4346\n",
      "entry_point=113.7900 exit_point=118.9000 max_point=125.5000 min_point=113.7300\n",
      "Episode: 197 Total reward: -3.3186 Training loss: 0.0164 Explore P: 0.0000 Reward mean: -3.6072 Reward std: 12.0139\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.7752\n",
      "entry_point=111.4300 exit_point=113.5700 max_point=123.9500 min_point=109.6200\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-2.8600\n",
      "entry_point=117.2500 exit_point=113.8800 max_point=116.7400 min_point=115.3800\n",
      "Episode: 198 Total reward: -11.5288 Training loss: 0.0163 Explore P: 0.0000 Reward mean: -3.6471 Reward std: 12.0336\n",
      "entry_state_action=0, last_action=1, action=0, position reward=13.5462\n",
      "entry_point=105.4300 exit_point=121.1200 max_point=120.6900 min_point=104.1500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.9268\n",
      "entry_point=119.9700 exit_point=114.6000 max_point=119.8800 min_point=114.6600\n",
      "Episode: 199 Total reward: 11.3550 Training loss: 0.0111 Explore P: 0.0000 Reward mean: -3.7697 Reward std: 11.8157\n",
      "entry_state_action=0, last_action=2, action=1, position reward=13.8310\n",
      "entry_point=130.7000 exit_point=116.9500 max_point=132.2000 min_point=115.6000\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-6.7000\n",
      "entry_point=115.9500 exit_point=112.2500 max_point=117.3500 min_point=112.5000\n",
      "Episode: 200 Total reward: 4.1170 Training loss: 0.0142 Explore P: 0.0000 Reward mean: -3.7401 Reward std: 11.8316\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-11.7938\n",
      "entry_point=90.9900 exit_point=80.0600 max_point=90.5100 min_point=79.3400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-0.2752\n",
      "entry_point=80.1900 exit_point=79.1500 max_point=81.3100 min_point=78.7600\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-1.3068\n",
      "entry_point=79.1000 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=2, position reward=0.5600\n",
      "entry_point=78.3400 exit_point=77.7400 max_point=78.3000 min_point=76.9200\n",
      "Episode: 201 Total reward: -17.6776 Training loss: 0.0153 Explore P: 0.0000 Reward mean: -3.8827 Reward std: 11.9126\n",
      "entry_state_action=0, last_action=2, action=0, position reward=10.6622\n",
      "entry_point=119.8300 exit_point=107.4600 max_point=123.5600 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.7670\n",
      "entry_point=108.3700 exit_point=101.2500 max_point=110.9400 min_point=100.8300\n",
      "Episode: 202 Total reward: 10.9252 Training loss: 0.0138 Explore P: 0.0000 Reward mean: -3.6501 Reward std: 11.9722\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.1846\n",
      "entry_point=120.2400 exit_point=116.9900 max_point=125.5000 min_point=116.4400\n",
      "Episode: 203 Total reward: -2.2902 Training loss: 0.0149 Explore P: 0.0000 Reward mean: -3.5957 Reward std: 11.9659\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.8640\n",
      "entry_point=119.4000 exit_point=112.0500 max_point=125.4500 min_point=111.8000\n",
      "Episode: 204 Total reward: 2.9760 Training loss: 0.0138 Explore P: 0.0000 Reward mean: -3.5940 Reward std: 11.9668\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-8.2758\n",
      "entry_point=86.0100 exit_point=77.9200 max_point=86.1400 min_point=78.2900\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-0.5690\n",
      "entry_point=78.2900 exit_point=76.7000 max_point=79.1000 min_point=76.6900\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-2.0952\n",
      "entry_point=78.3400 exit_point=78.1000 max_point=78.3000 min_point=76.9200\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-1.3400\n",
      "entry_point=77.8900 exit_point=76.7900 max_point=78.1300 min_point=76.6600\n",
      "Episode: 205 Total reward: -17.5648 Training loss: 0.0084 Explore P: 0.0000 Reward mean: -3.7888 Reward std: 12.0339\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.6182\n",
      "entry_point=95.8500 exit_point=95.4900 max_point=100.7800 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.8976\n",
      "entry_point=96.0300 exit_point=96.2400 max_point=94.8800 min_point=94.8800\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.1338\n",
      "entry_point=94.8600 exit_point=86.7600 max_point=98.6100 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-7.7770\n",
      "entry_point=86.7600 exit_point=93.0000 max_point=94.4200 min_point=86.4200\n",
      "entry_state_action=0, last_action=2, action=2, position reward=1.1700\n",
      "entry_point=93.0000 exit_point=91.6500 max_point=93.2000 min_point=92.2500\n",
      "Episode: 206 Total reward: -11.3190 Training loss: 0.0123 Explore P: 0.0000 Reward mean: -3.8213 Reward std: 12.0498\n",
      "entry_state_action=0, last_action=1, action=2, position reward=1.9532\n",
      "entry_point=111.5700 exit_point=115.5900 max_point=115.8400 min_point=111.4600\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-8.1462\n",
      "entry_point=115.8400 exit_point=120.3400 max_point=121.7900 min_point=109.6200\n",
      "Episode: 207 Total reward: -8.1744 Training loss: 0.0226 Explore P: 0.0000 Reward mean: -3.7892 Reward std: 12.0339\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.0646\n",
      "entry_point=109.1700 exit_point=108.4900 max_point=114.4200 min_point=104.9400\n",
      "Episode: 208 Total reward: -2.8980 Training loss: 0.0106 Explore P: 0.0000 Reward mean: -3.6913 Reward std: 12.0008\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-1.1418\n",
      "entry_point=93.3100 exit_point=98.4100 max_point=98.5900 min_point=97.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=1, last_action=2, action=0, position reward=0.5818\n",
      "entry_point=98.5900 exit_point=95.4900 max_point=100.7800 min_point=87.4700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.0710\n",
      "entry_point=96.0300 exit_point=93.0600 max_point=98.6100 min_point=92.9500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.0738\n",
      "entry_point=92.9100 exit_point=86.7600 max_point=97.2100 min_point=86.3100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=3.7100\n",
      "entry_point=90.1600 exit_point=92.8900 max_point=94.4200 min_point=88.2600\n",
      "Episode: 209 Total reward: 5.4466 Training loss: 0.0199 Explore P: 0.0000 Reward mean: -3.6274 Reward std: 12.0323\n",
      "entry_state_action=0, last_action=2, action=0, position reward=3.3640\n",
      "entry_point=117.5500 exit_point=112.0500 max_point=121.2500 min_point=111.8000\n",
      "Episode: 210 Total reward: 1.1630 Training loss: 0.0098 Explore P: 0.0000 Reward mean: -3.3571 Reward std: 11.8316\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-11.3646\n",
      "entry_point=108.5100 exit_point=118.1100 max_point=121.1200 min_point=108.7200\n",
      "Episode: 211 Total reward: -13.7448 Training loss: 0.0138 Explore P: 0.0000 Reward mean: -3.4313 Reward std: 11.8732\n",
      "entry_state_action=0, last_action=2, action=0, position reward=8.8334\n",
      "entry_point=117.9000 exit_point=107.2100 max_point=120.5500 min_point=105.3500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.7580\n",
      "entry_point=108.6600 exit_point=104.3100 max_point=114.5000 min_point=104.1000\n",
      "Episode: 212 Total reward: 6.8802 Training loss: 0.0145 Explore P: 0.0000 Reward mean: -3.2261 Reward std: 11.8722\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-6.5032\n",
      "entry_point=94.8700 exit_point=89.1800 max_point=97.2100 min_point=86.3100\n",
      "entry_state_action=0, last_action=2, action=2, position reward=6.9200\n",
      "entry_point=88.7400 exit_point=82.7700 max_point=94.4200 min_point=80.6100\n",
      "Episode: 213 Total reward: -4.2154 Training loss: 0.0111 Explore P: 0.0000 Reward mean: -3.0730 Reward std: 11.7592\n",
      "entry_state_action=0, last_action=1, action=2, position reward=5.6354\n",
      "entry_point=104.7800 exit_point=114.6900 max_point=113.7900 min_point=101.2400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-7.2408\n",
      "entry_point=113.7300 exit_point=118.9600 max_point=121.6000 min_point=114.4000\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-0.0900\n",
      "entry_point=122.9700 exit_point=123.5600 max_point=123.8900 min_point=122.7000\n",
      "Episode: 214 Total reward: -6.6704 Training loss: 0.0172 Explore P: 0.0000 Reward mean: -3.0366 Reward std: 11.7424\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.5980\n",
      "entry_point=113.1000 exit_point=104.3100 max_point=113.0600 min_point=104.1000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-7.9458\n",
      "entry_point=103.0600 exit_point=110.3000 max_point=112.9300 min_point=102.1700\n",
      "Episode: 215 Total reward: -6.3310 Training loss: 0.0068 Explore P: 0.0000 Reward mean: -2.9522 Reward std: 11.6880\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.2968\n",
      "entry_point=82.7700 exit_point=78.3000 max_point=85.4700 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-1.2020\n",
      "entry_point=78.3400 exit_point=77.2500 max_point=78.3000 min_point=77.6000\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-2.7130\n",
      "entry_point=77.6000 exit_point=76.2400 max_point=78.2100 min_point=76.1500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-5.8752\n",
      "entry_point=76.1500 exit_point=81.0200 max_point=84.0000 min_point=76.2400\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-3.0120\n",
      "entry_point=80.7600 exit_point=79.5100 max_point=81.6600 min_point=79.2000\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.4800\n",
      "entry_point=79.5100 exit_point=79.4100 max_point=80.4300 min_point=78.1600\n",
      "Episode: 216 Total reward: -22.1046 Training loss: 0.0154 Explore P: 0.0000 Reward mean: -2.9279 Reward std: 11.6455\n",
      "entry_state_action=0, last_action=1, action=2, position reward=3.2702\n",
      "entry_point=108.7000 exit_point=112.7800 max_point=113.4900 min_point=107.9400\n",
      "entry_state_action=1, last_action=2, action=0, position reward=6.5980\n",
      "entry_point=113.4900 exit_point=104.3100 max_point=114.5000 min_point=104.1000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-6.6402\n",
      "entry_point=103.0600 exit_point=108.7200 max_point=109.4700 min_point=102.1700\n",
      "Episode: 217 Total reward: -2.0272 Training loss: 0.0136 Explore P: 0.0000 Reward mean: -2.9703 Reward std: 11.6344\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.0778\n",
      "entry_point=105.9300 exit_point=106.2000 max_point=110.2800 min_point=97.0400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.4702\n",
      "entry_point=106.1300 exit_point=97.5700 max_point=106.9100 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.9130\n",
      "entry_point=97.5700 exit_point=97.1700 max_point=95.6500 min_point=95.6500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.9390\n",
      "entry_point=95.0100 exit_point=92.7200 max_point=95.5600 min_point=92.5500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.3306\n",
      "entry_point=91.1700 exit_point=87.4700 max_point=90.4800 min_point=89.0300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-11.4884\n",
      "entry_point=89.1700 exit_point=98.5000 max_point=99.3000 min_point=88.9000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-4.1364\n",
      "entry_point=97.8700 exit_point=100.6400 max_point=100.7800 min_point=97.6800\n",
      "Episode: 218 Total reward: -22.2140 Training loss: 0.0221 Explore P: 0.0000 Reward mean: -2.9405 Reward std: 11.5811\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-21.1560\n",
      "entry_point=79.6800 exit_point=96.4400 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.4642\n",
      "entry_point=96.8900 exit_point=99.7200 max_point=100.3300 min_point=96.2400\n",
      "Episode: 219 Total reward: -29.4416 Training loss: 0.0093 Explore P: 0.0000 Reward mean: -3.1072 Reward std: 11.8385\n",
      "entry_state_action=0, last_action=1, action=0, position reward=16.7912\n",
      "entry_point=77.6900 exit_point=95.3000 max_point=103.5000 min_point=77.5500\n",
      "Episode: 220 Total reward: 15.2674 Training loss: 0.0104 Explore P: 0.0000 Reward mean: -3.0391 Reward std: 11.9242\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-9.3572\n",
      "entry_point=108.4300 exit_point=115.7600 max_point=115.9800 min_point=102.1700\n",
      "Episode: 221 Total reward: -11.4058 Training loss: 0.0153 Explore P: 0.0000 Reward mean: -2.8961 Reward std: 11.7357\n",
      "entry_state_action=0, last_action=2, action=0, position reward=17.2094\n",
      "entry_point=107.3400 exit_point=87.4700 max_point=110.2800 min_point=89.0300\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-7.8182\n",
      "entry_point=89.8700 exit_point=95.4900 max_point=100.7800 min_point=88.9000\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-1.9102\n",
      "entry_point=90.2100 exit_point=90.0700 max_point=92.0600 min_point=88.4800\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.6900\n",
      "entry_point=90.0100 exit_point=90.7600 max_point=93.4000 min_point=86.3100\n",
      "Episode: 222 Total reward: 1.9726 Training loss: 0.0172 Explore P: 0.0000 Reward mean: -2.6893 Reward std: 11.6371\n",
      "entry_state_action=0, last_action=1, action=2, position reward=13.6272\n",
      "entry_point=102.2800 exit_point=117.6300 max_point=118.1400 min_point=100.3100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=7.1436\n",
      "entry_point=118.1400 exit_point=109.1900 max_point=117.9600 min_point=108.3200\n",
      "Episode: 223 Total reward: 19.1052 Training loss: 0.0201 Explore P: 0.0000 Reward mean: -2.5267 Reward std: 11.8254\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.3522\n",
      "entry_point=113.3000 exit_point=107.4600 max_point=113.9000 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-4.2672\n",
      "entry_point=109.1700 exit_point=110.2900 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.4700\n",
      "entry_point=110.3600 exit_point=108.8200 max_point=110.7900 min_point=109.1000\n",
      "Episode: 224 Total reward: -5.6244 Training loss: 0.0115 Explore P: 0.0000 Reward mean: -2.6234 Reward std: 11.8108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry_state_action=0, last_action=1, action=2, position reward=-10.1150\n",
      "entry_point=110.7300 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-11.6614\n",
      "entry_point=102.2500 exit_point=111.3500 max_point=118.1400 min_point=99.8100\n",
      "Episode: 225 Total reward: -23.7810 Training loss: 0.0150 Explore P: 0.0000 Reward mean: -2.6407 Reward std: 11.8405\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.3806\n",
      "entry_point=119.9200 exit_point=115.3600 max_point=125.5000 min_point=116.9700\n",
      "entry_state_action=0, last_action=2, action=0, position reward=5.3808\n",
      "entry_point=114.3400 exit_point=107.3900 max_point=113.9700 min_point=106.4600\n",
      "Episode: 226 Total reward: 1.6762 Training loss: 0.0148 Explore P: 0.0000 Reward mean: -2.6214 Reward std: 11.8459\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-4.6100\n",
      "entry_point=96.2400 exit_point=92.8200 max_point=98.6100 min_point=86.3100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=11.8200\n",
      "entry_point=93.0000 exit_point=81.0000 max_point=93.2000 min_point=80.6100\n",
      "Episode: 227 Total reward: 4.9752 Training loss: 0.0151 Explore P: 0.0000 Reward mean: -2.3786 Reward std: 11.7500\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-3.1988\n",
      "entry_point=117.6500 exit_point=117.8900 max_point=118.0200 min_point=116.3900\n",
      "entry_state_action=1, last_action=2, action=0, position reward=2.0486\n",
      "entry_point=117.4400 exit_point=114.2100 max_point=123.9500 min_point=112.7600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.2236\n",
      "entry_point=111.5100 exit_point=107.3700 max_point=111.0300 min_point=108.5400\n",
      "Episode: 228 Total reward: -4.8870 Training loss: 0.0104 Explore P: 0.0000 Reward mean: -2.4918 Reward std: 11.7190\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.8028\n",
      "entry_point=124.6400 exit_point=114.7900 max_point=125.1300 min_point=115.3600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.7408\n",
      "entry_point=112.4100 exit_point=107.3900 max_point=114.6600 min_point=106.4600\n",
      "Episode: 229 Total reward: 6.0526 Training loss: 0.0106 Explore P: 0.0000 Reward mean: -2.3700 Reward std: 11.7439\n",
      "entry_state_action=0, last_action=2, action=0, position reward=15.4308\n",
      "entry_point=124.0900 exit_point=107.3900 max_point=125.1300 min_point=106.4600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-0.6566\n",
      "entry_point=116.0100 exit_point=114.1600 max_point=115.9600 min_point=114.3300\n",
      "Episode: 230 Total reward: 10.0922 Training loss: 0.0168 Explore P: 0.0000 Reward mean: -1.9083 Reward std: 11.3091\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.0800\n",
      "entry_point=96.0200 exit_point=92.8200 max_point=97.2100 min_point=86.3100\n",
      "entry_state_action=1, last_action=2, action=2, position reward=8.7300\n",
      "entry_point=93.0000 exit_point=84.0900 max_point=93.2000 min_point=80.6100\n",
      "Episode: 231 Total reward: 2.1096 Training loss: 0.0076 Explore P: 0.0000 Reward mean: -1.8346 Reward std: 11.3110\n",
      "entry_state_action=0, last_action=1, action=2, position reward=9.0554\n",
      "entry_point=101.8400 exit_point=114.6900 max_point=113.7900 min_point=101.2600\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-4.0788\n",
      "entry_point=113.7300 exit_point=117.3900 max_point=121.6000 min_point=114.4000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-7.7194\n",
      "entry_point=117.8300 exit_point=123.4700 max_point=125.5000 min_point=117.3000\n",
      "Episode: 232 Total reward: -7.4162 Training loss: 0.0189 Explore P: 0.0000 Reward mean: -1.9720 Reward std: 11.2945\n",
      "entry_state_action=0, last_action=2, action=0, position reward=6.0618\n",
      "entry_point=119.8100 exit_point=113.3300 max_point=125.5000 min_point=112.4100\n",
      "Episode: 233 Total reward: 2.7556 Training loss: 0.0079 Explore P: 0.0000 Reward mean: -1.9997 Reward std: 11.2795\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-2.6120\n",
      "entry_point=81.3900 exit_point=78.1200 max_point=85.4700 min_point=77.1800\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-0.5302\n",
      "entry_point=79.1000 exit_point=76.9100 max_point=78.1200 min_point=76.9900\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-0.0968\n",
      "entry_point=77.0100 exit_point=78.3000 max_point=78.3400 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-0.9642\n",
      "entry_point=78.3400 exit_point=77.8500 max_point=78.3000 min_point=77.7100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-1.8500\n",
      "entry_point=83.5600 exit_point=81.8000 max_point=83.6500 min_point=82.1400\n",
      "Episode: 234 Total reward: -9.5422 Training loss: 0.0117 Explore P: 0.0000 Reward mean: -2.1470 Reward std: 11.2809\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.7402\n",
      "entry_point=112.8100 exit_point=108.8000 max_point=114.4200 min_point=104.9400\n",
      "Episode: 235 Total reward: -0.1060 Training loss: 0.0082 Explore P: 0.0000 Reward mean: -2.2038 Reward std: 11.2561\n",
      "entry_state_action=0, last_action=1, action=0, position reward=2.3248\n",
      "entry_point=109.9200 exit_point=115.6800 max_point=121.1200 min_point=109.3000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.6652\n",
      "entry_point=117.9800 exit_point=118.0300 max_point=119.7600 min_point=109.6200\n",
      "Episode: 236 Total reward: -3.6584 Training loss: 0.0127 Explore P: 0.0000 Reward mean: -2.2062 Reward std: 11.2564\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.7964\n",
      "entry_point=98.5000 exit_point=88.5700 max_point=100.7800 min_point=86.3100\n",
      "entry_state_action=0, last_action=1, action=2, position reward=2.4100\n",
      "entry_point=88.4800 exit_point=92.8200 max_point=94.4200 min_point=88.5400\n",
      "entry_state_action=1, last_action=2, action=1, position reward=4.9370\n",
      "entry_point=93.0000 exit_point=87.1500 max_point=93.2000 min_point=86.6500\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-2.7500\n",
      "entry_point=86.6500 exit_point=84.4000 max_point=87.7000 min_point=84.3100\n",
      "Episode: 237 Total reward: 8.8238 Training loss: 0.0112 Explore P: 0.0000 Reward mean: -2.1012 Reward std: 11.3097\n",
      "entry_state_action=0, last_action=2, action=2, position reward=-7.7800\n",
      "entry_point=96.0700 exit_point=103.7400 max_point=105.4000 min_point=92.8800\n",
      "Episode: 238 Total reward: -9.5914 Training loss: 0.0085 Explore P: 0.0000 Reward mean: -2.0937 Reward std: 11.3045\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-11.6980\n",
      "entry_point=102.9600 exit_point=111.7200 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.4146\n",
      "entry_point=111.2600 exit_point=113.7600 max_point=114.3800 min_point=108.3200\n",
      "Episode: 239 Total reward: -20.7170 Training loss: 0.0209 Explore P: 0.0000 Reward mean: -2.1693 Reward std: 11.4031\n",
      "entry_state_action=0, last_action=2, action=0, position reward=7.1958\n",
      "entry_point=118.9200 exit_point=109.0800 max_point=119.7900 min_point=107.3700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=8.0894\n",
      "entry_point=109.7700 exit_point=97.0400 max_point=108.2700 min_point=97.5300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=2.8416\n",
      "entry_point=97.5300 exit_point=102.3200 max_point=101.9200 min_point=97.0400\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-12.8356\n",
      "entry_point=104.8900 exit_point=93.3100 max_point=106.9600 min_point=94.2800\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-4.2530\n",
      "entry_point=94.2800 exit_point=97.1700 max_point=99.7500 min_point=93.3100\n",
      "entry_state_action=0, last_action=2, action=0, position reward=1.7718\n",
      "entry_point=96.6600 exit_point=89.4900 max_point=95.5600 min_point=87.4700\n",
      "Episode: 240 Total reward: -0.7948 Training loss: 0.0122 Explore P: 0.0000 Reward mean: -2.3870 Reward std: 11.1644\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.7726\n",
      "entry_point=112.6400 exit_point=110.9300 max_point=114.3800 min_point=104.9400\n",
      "Episode: 241 Total reward: -2.4554 Training loss: 0.0113 Explore P: 0.0000 Reward mean: -2.6226 Reward std: 10.9119\n",
      "entry_state_action=0, last_action=1, action=2, position reward=13.1412\n",
      "entry_point=82.4100 exit_point=95.3000 max_point=103.5000 min_point=82.4500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-8.1688\n",
      "entry_point=96.4400 exit_point=102.1800 max_point=105.4000 min_point=94.6300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 242 Total reward: 3.4242 Training loss: 0.0108 Explore P: 0.0000 Reward mean: -2.7636 Reward std: 10.7405\n",
      "entry_state_action=0, last_action=1, action=2, position reward=22.7754\n",
      "entry_point=78.3600 exit_point=101.1800 max_point=103.5000 min_point=77.6100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=0.4040\n",
      "entry_point=102.2300 exit_point=96.4400 max_point=101.1800 min_point=97.4400\n",
      "Episode: 243 Total reward: 21.7122 Training loss: 0.0116 Explore P: 0.0000 Reward mean: -2.5625 Reward std: 11.0054\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-8.6946\n",
      "entry_point=105.9200 exit_point=114.6900 max_point=113.7900 min_point=106.3500\n",
      "entry_state_action=2, last_action=1, action=0, position reward=1.8222\n",
      "entry_point=113.7300 exit_point=118.6100 max_point=121.6000 min_point=114.4000\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-3.7118\n",
      "entry_point=118.6100 exit_point=120.0700 max_point=125.5000 min_point=116.4400\n",
      "Episode: 244 Total reward: -14.6548 Training loss: 0.0088 Explore P: 0.0000 Reward mean: -2.5625 Reward std: 11.0053\n",
      "entry_state_action=0, last_action=2, action=0, position reward=4.0558\n",
      "entry_point=115.1300 exit_point=109.0800 max_point=117.7300 min_point=107.3700\n",
      "entry_state_action=0, last_action=2, action=1, position reward=8.0894\n",
      "entry_point=109.7700 exit_point=97.0400 max_point=108.2700 min_point=97.5300\n",
      "entry_state_action=2, last_action=1, action=0, position reward=2.8416\n",
      "entry_point=97.5300 exit_point=102.3200 max_point=101.9200 min_point=97.0400\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-5.0292\n",
      "entry_point=102.3200 exit_point=106.0700 max_point=110.2800 min_point=101.1500\n",
      "entry_state_action=0, last_action=2, action=1, position reward=1.6766\n",
      "entry_point=102.8200 exit_point=97.3200 max_point=101.9500 min_point=97.6700\n",
      "entry_state_action=2, last_action=1, action=1, position reward=0.2200\n",
      "entry_point=97.6700 exit_point=97.5400 max_point=99.7500 min_point=87.4700\n",
      "Episode: 245 Total reward: 5.5934 Training loss: 0.0116 Explore P: 0.0000 Reward mean: -2.6740 Reward std: 10.8647\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-9.7452\n",
      "entry_point=91.1400 exit_point=83.3200 max_point=94.4200 min_point=80.6100\n",
      "entry_state_action=0, last_action=1, action=1, position reward=0.4200\n",
      "entry_point=80.2300 exit_point=81.0100 max_point=81.0300 min_point=80.2000\n",
      "Episode: 246 Total reward: -12.1926 Training loss: 0.0094 Explore P: 0.0000 Reward mean: -2.8824 Reward std: 10.8454\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-13.7400\n",
      "entry_point=97.7400 exit_point=84.1300 max_point=100.7800 min_point=83.8500\n",
      "Episode: 247 Total reward: -15.5648 Training loss: 0.0149 Explore P: 0.0000 Reward mean: -2.8184 Reward std: 10.7509\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-14.2662\n",
      "entry_point=101.0500 exit_point=112.4000 max_point=118.1400 min_point=100.3100\n",
      "Episode: 248 Total reward: -16.0372 Training loss: 0.0113 Explore P: 0.0000 Reward mean: -3.0467 Reward std: 10.7867\n",
      "entry_state_action=0, last_action=1, action=1, position reward=9.6000\n",
      "entry_point=94.0700 exit_point=102.3100 max_point=105.4000 min_point=92.0900\n",
      "Episode: 249 Total reward: 6.3586 Training loss: 0.0094 Explore P: 0.0000 Reward mean: -2.8864 Reward std: 10.8061\n",
      "entry_state_action=0, last_action=1, action=0, position reward=-0.8252\n",
      "entry_point=80.3500 exit_point=80.3400 max_point=81.3100 min_point=75.7600\n",
      "entry_state_action=0, last_action=2, action=0, position reward=0.3490\n",
      "entry_point=80.9800 exit_point=78.9800 max_point=81.6600 min_point=78.1600\n",
      "entry_state_action=0, last_action=1, action=1, position reward=2.9600\n",
      "entry_point=78.1500 exit_point=81.4800 max_point=81.1800 min_point=78.3500\n",
      "Episode: 250 Total reward: -1.8258 Training loss: 0.0067 Explore P: 0.0000 Reward mean: -2.9991 Reward std: 10.7354\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.5870\n",
      "entry_point=112.4300 exit_point=110.0600 max_point=114.4200 min_point=104.9400\n",
      "entry_state_action=0, last_action=2, action=2, position reward=3.1800\n",
      "entry_point=111.1300 exit_point=107.7300 max_point=110.9100 min_point=110.9100\n",
      "Episode: 251 Total reward: -2.2182 Training loss: 0.0167 Explore P: 0.0000 Reward mean: -2.8816 Reward std: 10.6788\n",
      "entry_state_action=0, last_action=2, action=1, position reward=3.2222\n",
      "entry_point=112.6600 exit_point=107.4600 max_point=112.6900 min_point=106.4600\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-6.6350\n",
      "entry_point=107.3900 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-11.0072\n",
      "entry_point=102.2500 exit_point=110.2900 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=1, position reward=-1.8700\n",
      "entry_point=110.3600 exit_point=108.4200 max_point=110.7900 min_point=108.6400\n",
      "Episode: 252 Total reward: -18.5732 Training loss: 0.0111 Explore P: 0.0000 Reward mean: -3.2667 Reward std: 10.5424\n",
      "entry_state_action=0, last_action=2, action=0, position reward=2.6380\n",
      "entry_point=110.2500 exit_point=104.3100 max_point=114.5000 min_point=103.9500\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-6.6402\n",
      "entry_point=103.0600 exit_point=108.7200 max_point=109.4700 min_point=102.1700\n",
      "Episode: 253 Total reward: -7.8184 Training loss: 0.0114 Explore P: 0.0000 Reward mean: -3.3143 Reward std: 10.5520\n",
      "entry_state_action=0, last_action=1, action=2, position reward=-5.2968\n",
      "entry_point=82.7700 exit_point=78.3000 max_point=85.4700 min_point=75.7600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-1.7318\n",
      "entry_point=78.3400 exit_point=77.7100 max_point=78.3000 min_point=78.0800\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-2.3778\n",
      "entry_point=78.0900 exit_point=76.9100 max_point=78.2100 min_point=76.6600\n",
      "entry_state_action=1, last_action=2, action=1, position reward=-1.6584\n",
      "entry_point=76.8900 exit_point=76.8100 max_point=76.9200 min_point=76.9100\n",
      "entry_state_action=2, last_action=1, action=2, position reward=-2.2730\n",
      "entry_point=76.9200 exit_point=76.2400 max_point=77.8000 min_point=76.1500\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-7.5428\n",
      "entry_point=76.1500 exit_point=83.1900 max_point=84.0000 min_point=76.2400\n",
      "entry_state_action=0, last_action=1, action=1, position reward=-0.3200\n",
      "entry_point=81.1900 exit_point=79.5300 max_point=80.4300 min_point=78.1600\n",
      "Episode: 254 Total reward: -26.4798 Training loss: 0.0147 Explore P: 0.0000 Reward mean: -3.4483 Reward std: 10.7583\n",
      "entry_state_action=0, last_action=1, action=2, position reward=4.6212\n",
      "entry_point=90.5600 exit_point=95.3000 max_point=103.5000 min_point=90.5100\n",
      "entry_state_action=1, last_action=2, action=0, position reward=-8.1688\n",
      "entry_point=96.4400 exit_point=102.1800 max_point=105.4000 min_point=94.6300\n",
      "Episode: 255 Total reward: -4.8888 Training loss: 0.0113 Explore P: 0.0000 Reward mean: -3.4544 Reward std: 10.7589\n",
      "entry_state_action=0, last_action=2, action=0, position reward=-1.5312\n",
      "entry_point=111.0300 exit_point=110.9900 max_point=114.3800 min_point=104.9400\n",
      "Episode: 256 Total reward: -4.4718 Training loss: 0.0199 Explore P: 0.0000 Reward mean: -3.5841 Reward std: 10.6921\n",
      "entry_state_action=0, last_action=2, action=1, position reward=-7.9100\n",
      "entry_point=105.1300 exit_point=111.6200 max_point=118.1400 min_point=99.8100\n",
      "entry_state_action=2, last_action=1, action=0, position reward=-1.4190\n",
      "entry_point=111.0000 exit_point=111.5200 max_point=113.6500 min_point=111.6200\n",
      "Episode: 257 Total reward: -10.9916 Training loss: 0.0143 Explore P: 0.0000 Reward mean: -3.8794 Reward std: 10.4829\n",
      "entry_state_action=0, last_action=2, action=2, position reward=3.0200\n",
      "entry_point=112.0400 exit_point=110.7200 max_point=115.2300 min_point=104.9400\n",
      "Episode: 258 Total reward: -0.9208 Training loss: 0.0179 Explore P: 0.0000 Reward mean: -3.9063 Reward std: 10.4718\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ee0c17f90438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-be59e829b06e>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(memory, opt, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# 勾配を初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "opt = optim.Adam(mainQN.parameters(), learning_rate)\n",
    "\n",
    "outputs = np.empty([1,11])\n",
    "episode_reward_deque = deque(maxlen=100)\n",
    "\n",
    "count_stop = 0\n",
    "for ep in range(train_episodes):\n",
    "    total_reward = 0\n",
    "    current_position_reward = 0\n",
    "    last_action = 0\n",
    "    initial_action = 0\n",
    "    t = 0\n",
    "    # Start new episode\n",
    "    observation = env.reset()\n",
    "    state = make_state(observation, means, stds)\n",
    "            \n",
    "    for t in range(max_steps):\n",
    "        # Explore or Exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*(ep*max_steps+t))\n",
    "        if ep > train_episodes:\n",
    "            explore_p = 0.0\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            mainQN.eval()\n",
    "            Qs = mainQN(Variable(torch.FloatTensor([state]))).data.numpy()\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "        #result = np.hstack((state, mainQN(Variable(torch.FloatTensor([state]))).data.numpy()))\n",
    "        #outputs = np.vstack((outputs, result))\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        last_current = observation[3]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        next_state = make_state(observation, means, stds)\n",
    "\n",
    "        total_reward += reward\n",
    "        current_position_reward += reward\n",
    "        \n",
    "        if last_action != action or done:\n",
    "            if last_action != 0:\n",
    "                clipped_reward = current_position_reward/(t-entry_t)\n",
    "                memory.add((entry_state, last_action, clipped_reward, next_state, done))\n",
    "                memory.add((state, action, clipped_reward, next_state, done))\n",
    "                if current_position_reward > 0:\n",
    "                    for val in keep_list:\n",
    "                        memory.add((val[0], val[1], clipped_reward, val[2], done))\n",
    "                keep_list = []\n",
    "                print('entry_state_action={}, last_action={}, action={}, position reward={:.4f}'.format(np.argmax(entry_state[0:3]), last_action, action, current_position_reward))\n",
    "                print('entry_point={:.4f} exit_point={:.4f} max_point={:.4f} min_point={:.4f}'.format(entry_point, info, max_point, min_point))\n",
    "                \n",
    "            entry_state = state\n",
    "            initial_action = last_action\n",
    "            entry_t = t\n",
    "            current_position_reward = 0.0\n",
    "            max_point = info\n",
    "            min_point = info\n",
    "            entry_point = last_current\n",
    "        else:\n",
    "            keep_list.append([state, action, next_state])\n",
    "            max_point = info if info > max_point else max_point\n",
    "            min_point = info if info < min_point else min_point\n",
    "            \n",
    "        last_action = action\n",
    "        state = next_state\n",
    "\n",
    "        mainQN.train()\n",
    "        loss = optimize(memory, opt, batch_size)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            episode_reward_deque.append(total_reward)\n",
    "            # the episode ends so no next state\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {:.4f}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss.data.numpy()),\n",
    "                  'Explore P: {:.4f}'.format(explore_p),\n",
    "                  'Reward mean: {:.4f}'.format(np.array(list(episode_reward_deque)).mean()),\n",
    "                  'Reward std: {:.4f}'.format(np.array(list(episode_reward_deque)).std()))\n",
    "            rewards_list.append(total_reward)\n",
    "            \n",
    "            if ep % target_update == 0:\n",
    "                targetQN.load_state_dict(mainQN.state_dict())\n",
    "            \n",
    "            break\n",
    "#df = pd.DataFrame(outputs)\n",
    "#df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rews = np.array(rewards_list).T\n",
    "eps = range(len(rews))\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999.7061111111111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rewards_list[1990:2000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1991, 18.950000000000003),\n",
       " (1992, 20.279999999999987),\n",
       " (1993, 5.310000000000002),\n",
       " (1994, 3.039999999999992),\n",
       " (1995, -26.14),\n",
       " (1996, 0),\n",
       " (1997, 0),\n",
       " (1998, 18.269999999999996),\n",
       " (1999, 0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_list[1990:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
