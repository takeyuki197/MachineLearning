{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import envs.TradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, gamma=0.99, learning_rate=0.01, state_size=8, \n",
    "                 action_size=3, hidden_size=10, batch_size=20,\n",
    "                 name='QNetwork'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 10000          # max number of episodes to learn from\n",
    "max_steps = 365                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "action_size = 3\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 0.1            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 8               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.00001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 200000            # memory capacity\n",
    "batch_size = 258                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory\n",
    "\n",
    "transaction_cost_ratio = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, gamma=gamma, learning_rate=learning_rate,batch_size=batch_size)\n",
    "env = envs.TradingEnv.FxEnv(scenario_length=max_steps, transaction_cost_ratio=transaction_cost_ratio)\n",
    "env.seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(observation, means, stds):\n",
    "    return np.array([observation[0], observation[1], observation[2],\n",
    "                    (observation[3]-means[3])/stds[3],\n",
    "                    (observation[4]-means[4])/stds[4],\n",
    "                    (observation[5]-means[5])/stds[5],\n",
    "                    (observation[6]-means[6])/stds[6],\n",
    "                    (observation[7]-means[7])/stds[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env.reset()\n",
    "values = []\n",
    "for i in range(100000):\n",
    "    observation, reward, done = env.step(env.action_space.sample())\n",
    "    values.append(observation)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "values_array = np.array(values)\n",
    "means = values_array.mean(axis=0)\n",
    "stds = values_array.std(axis=0)\n",
    "print(means)\n",
    "print(stds)\n",
    "\"\"\"\n",
    "means = [0.332350000, 0.334510000, 0.333140000, 0.000368700000, 0.000294200000, 0.546393900, 0.562338400, 0.493165397]\n",
    "stds = [0.47105571, 0.47181888, 0.47133612, 0.67677779, 0.97742333, 0.97325276, 0.95412702, 0.28551883]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c0fd1220ad54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/Documents/MachineLearning/ReinforcementLearning/FXTrading/envs/TradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/Documents/MachineLearning/ReinforcementLearning/FXTrading/envs/TradingEnv.py\u001b[0m in \u001b[0;36mcalc_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maccelaration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdownfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mupfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mrest_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  10954\u001b[0m                                       skipna=skipna)\n\u001b[1;32m  10955\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m> 10956\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m  10957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10958\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   3628\u001b[0m                                           'numeric_only.'.format(name))\n\u001b[1;32m   3629\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m         \u001b[0;31m# TODO(EA) dispatch to Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mreduction\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         values, mask, dtype, dtype_max, fill_value = _get_values(\n\u001b[0;32m--> 731\u001b[0;31m             values, skipna, fill_value_typ=fill_value_typ, mask=mask)\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         if ((axis is not None and values.shape[axis] == 0) or\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(values, skipna, fill_value, fill_value_typ, isfinite, copy, mask)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_isfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_datetime_or_timedelta_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_new\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           \u001b[0mABCExtensionArray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                           ABCDatetimeArray, ABCTimedeltaArray)):\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_isna_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCGeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/missing.py\u001b[0m in \u001b[0;36m_isna_ndarraylike\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;31m# this is the NaT pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mneeds_i8_conversion\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     return (is_datetime_or_timedelta_dtype(arr_or_dtype) or\n\u001b[1;32m   1468\u001b[0m             \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             is_period_dtype(arr_or_dtype))\n\u001b[0m\u001b[1;32m   1470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_period_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr_or_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = Memory(max_size=memory_size)\n",
    "env.reset()\n",
    "observation, reward, done = env.step(env.action_space.sample())\n",
    "state = make_state(observation, means, stds)\n",
    "for i in range(20000):\n",
    "    memory.add((state))\n",
    "    observation, reward, done = env.step(env.action_space.sample())\n",
    "    state = make_state(observation, means, stds)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f524ae82b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": [
    "[val[0:3] for val in memory.sample(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45329916\n",
      "0.44351408\n",
      "0.4524858\n",
      "0.44287172\n",
      "0.43894967\n",
      "0.43980053\n",
      "0.44616437\n",
      "0.42724034\n",
      "0.42224783\n",
      "0.42525277\n",
      "0.42040077\n",
      "0.4102428\n",
      "0.4147234\n",
      "0.41300148\n",
      "0.41278923\n",
      "0.4100938\n",
      "0.39350542\n",
      "0.39896485\n",
      "0.38395402\n",
      "0.37952358\n",
      "0.3797274\n",
      "0.37142873\n",
      "0.37497583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-845205939175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# backpropagation of loss to NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# 勾配を初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0m__constants__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'reduction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(mainQN.parameters(), 0.00001)\n",
    "i = 0\n",
    "while(True):\n",
    "    batch = memory.sample(512)\n",
    "    batch_state = np.array(batch)\n",
    "    batch_q = torch.FloatTensor(np.array([val[0:3] for val in batch]))\n",
    "    \n",
    "    current_q_values = mainQN(Variable(torch.FloatTensor(batch_state)))\n",
    "    loss = torch.nn.MSELoss()(current_q_values, batch_q)\n",
    "    # backpropagation of loss to NN\n",
    "    # 勾配を初期化\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(loss.data.numpy())\n",
    "    \n",
    "    if loss.data.numpy() < 0.0001:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "torch.save(mainQN.state_dict(), 'KeepPolicy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainQN.load_state_dict(torch.load('KeepPolicy.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(memory, opt, batch_size):\n",
    "    # Sample mini-batch from memory\n",
    "    batch = memory.sample(batch_size)\n",
    "    #memory.buffer.clear()\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    ### ポイント！！！\n",
    "    # actionはスカラーなのでベクトルにする\n",
    "    # actionsはベクトルでなく、statesと同じ行列\n",
    "    actions = np.array([[each[1]] for each in batch])\n",
    "    ### ポイント終わり\n",
    "    rewards = np.array([each[2] for each in batch])\n",
    "    next_states = np.array([each[3] for each in batch])\n",
    "    dones = np.array([each[4] for each in batch])\n",
    "\n",
    "    # Train network\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s==False, dones)), dtype=torch.uint8)\n",
    "    # 終端状態のQ値はその後の報酬が存在しないためゼロとする\n",
    "    target_maxQs = torch.zeros(batch_size)\n",
    "    target_maxQs[non_final_mask] = mainQN(Variable(torch.FloatTensor(next_states)[non_final_mask])).max(1)[0].detach()\n",
    "\n",
    "    #tutorial way\n",
    "    targets = (torch.FloatTensor(rewards) + gamma * target_maxQs).unsqueeze(1)\n",
    "\n",
    "    for i in range(1):\n",
    "        current_q_values = mainQN(Variable(torch.FloatTensor(states))).gather(1, torch.LongTensor(actions))\n",
    "        loss = torch.nn.SmoothL1Loss()(current_q_values, targets)\n",
    "        # backpropagation of loss to NN\n",
    "        # 勾配を初期化\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = make_state(observation, means, stds)\n",
    "last_action = 0\n",
    "current_position_reward = 0\n",
    "memory = Memory(max_size=memory_size)\n",
    "t = 0\n",
    "keep_list = []\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "while(len(memory.buffer)<batch_size):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    if 0.1 > np.random.rand():\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = last_action\n",
    "    next_observation, reward, done, _ = env.step(action)\n",
    "    next_state = make_state(next_observation, means, stds)\n",
    "    \n",
    "    current_position_reward += reward\n",
    "    \n",
    "    if last_action != action:\n",
    "        if last_action != 0:\n",
    "            clipped_reward = current_position_reward/(t-entry_t)\n",
    "            memory.add((entry_state, last_action, clipped_reward, next_state, done))\n",
    "            if current_position_reward > 0:\n",
    "                for val in keep_list:\n",
    "                    memory.add((val[0], val[1], clipped_reward, val[2], done))\n",
    "            keep_list = []\n",
    "            \n",
    "\n",
    "        entry_state = state\n",
    "        entry_t = t\n",
    "        current_position_reward = 0.0\n",
    "    else:\n",
    "        keep_list.append([state, action, next_state])\n",
    "\n",
    "    last_action = action\n",
    "    state = next_state\n",
    "    t += 1\n",
    "\n",
    "    if done:\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        last_action = 0\n",
    "        current_position_reward = 0.0\n",
    "        t = 0\n",
    "        entry_t = 0\n",
    "        observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = make_state(observation, means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4052346  -0.09494403 -0.25772655]\n",
      "entry_state_action=0, initial_action=0, action=2, position reward=-15.0600\n",
      "entry_point=101.9800 exit_point=117.4700 max_point=121.6000 min_point=101.2600\n",
      "[ 0.51191914 -0.12909076 -0.210711  ]\n",
      "entry_state_action=0, initial_action=0, action=2, position reward=-4.4300\n",
      "entry_point=120.1600 exit_point=123.8900 max_point=125.5000 min_point=119.2700\n",
      "Episode: 10000 Total reward: -18.7400 Training loss: 0.0390 Explore P: 0.0100 Reward mean: -18.7400 Reward std: 0.0000\n",
      "Episode: 10001 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -9.3700 Reward std: 9.3700\n",
      "Episode: 10002 Total reward: -0.8700 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -6.5367 Reward std: 8.6364\n",
      "Episode: 10003 Total reward: 7.0700 Training loss: 0.0358 Explore P: 0.0000 Reward mean: -3.1350 Reward std: 9.5212\n",
      "Episode: 10004 Total reward: 0.0000 Training loss: 0.0358 Explore P: 0.0000 Reward mean: -2.5080 Reward std: 8.6079\n",
      "[-0.1379241   0.06213009 -0.45608482]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.6000\n",
      "entry_point=106.4600 exit_point=104.8100 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10005 Total reward: -1.6700 Training loss: 0.0386 Explore P: 0.0000 Reward mean: -2.3683 Reward std: 7.8641\n",
      "Episode: 10006 Total reward: -0.8800 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -2.1557 Reward std: 7.2993\n",
      "Episode: 10007 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.8862 Reward std: 6.8650\n",
      "Episode: 10008 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.6767 Reward std: 6.4995\n",
      "Episode: 10009 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.5090 Reward std: 6.1865\n",
      "Episode: 10010 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.3718 Reward std: 5.9145\n",
      "Episode: 10011 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.2575 Reward std: 5.6754\n",
      "Episode: 10012 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -1.1608 Reward std: 5.4630\n",
      "Episode: 10013 Total reward: 4.6100 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7486 Reward std: 5.4700\n",
      "Episode: 10014 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.6987 Reward std: 5.2879\n",
      "Episode: 10015 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.6550 Reward std: 5.1227\n",
      "[-0.05285256  0.1305305  -0.48015335]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-17.6200\n",
      "entry_point=109.3000 exit_point=91.8700 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10016 Total reward: -18.0100 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.6759 Reward std: 6.4323\n",
      "Episode: 10017 Total reward: 0.0000 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.5828 Reward std: 6.2628\n",
      "[-0.08237161  0.06642208 -0.44264427]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.1000\n",
      "entry_point=109.3000 exit_point=104.1600 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10018 Total reward: -4.4900 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -1.7358 Reward std: 6.1302\n",
      "Episode: 10019 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -1.6490 Reward std: 5.9870\n",
      "Episode: 10020 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -1.5705 Reward std: 5.8532\n",
      "Episode: 10021 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -1.4991 Reward std: 5.7280\n",
      "Episode: 10022 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -1.4339 Reward std: 5.6105\n",
      "[-0.08411571  0.0559752  -0.43617785]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.7600\n",
      "entry_point=109.3000 exit_point=104.7400 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10023 Total reward: -5.1500 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.5887 Reward std: 5.5423\n",
      "Episode: 10024 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.5252 Reward std: 5.4392\n",
      "Episode: 10025 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.4665 Reward std: 5.3417\n",
      "Episode: 10026 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.4122 Reward std: 5.2491\n",
      "Episode: 10027 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.3618 Reward std: 5.1612\n",
      "Episode: 10028 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.3148 Reward std: 5.0775\n",
      "Episode: 10029 Total reward: 0.0000 Training loss: 0.0337 Explore P: 0.0000 Reward mean: -1.2710 Reward std: 4.9977\n",
      "[-0.14472649  0.01998207 -0.4306242 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.6900\n",
      "entry_point=106.4600 exit_point=100.8300 max_point=110.9400 min_point=100.9700\n",
      "Episode: 10030 Total reward: -3.7600 Training loss: 0.0358 Explore P: 0.0000 Reward mean: -1.3513 Reward std: 4.9361\n",
      "Episode: 10031 Total reward: 0.0000 Training loss: 0.0358 Explore P: 0.0000 Reward mean: -1.3091 Reward std: 4.8641\n",
      "[ 0.01663681  0.2035851  -0.42494598]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.2900\n",
      "entry_point=94.2800 exit_point=89.4300 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10032 Total reward: -5.2600 Training loss: 0.0313 Explore P: 0.0000 Reward mean: -1.4288 Reward std: 4.8374\n",
      "[-0.03610925  0.13722667 -0.4797666 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.3900\n",
      "entry_point=109.3000 exit_point=98.5700 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10033 Total reward: -11.7800 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.7332 Reward std: 5.0765\n",
      "Episode: 10034 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.6837 Reward std: 5.0118\n",
      "Episode: 10035 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.6369 Reward std: 4.9495\n",
      "Episode: 10036 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.5927 Reward std: 4.8893\n",
      "Episode: 10037 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.5508 Reward std: 4.8313\n",
      "Episode: 10038 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.5110 Reward std: 4.7752\n",
      "Episode: 10039 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.4732 Reward std: 4.7211\n",
      "Episode: 10040 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.4373 Reward std: 4.6687\n",
      "Episode: 10041 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.4031 Reward std: 4.6180\n",
      "Episode: 10042 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.3705 Reward std: 4.5689\n",
      "Episode: 10043 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.3393 Reward std: 4.5213\n",
      "Episode: 10044 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -1.3096 Reward std: 4.4751\n",
      "Episode: 10045 Total reward: 6.9200 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -1.1307 Reward std: 4.5860\n",
      "Episode: 10046 Total reward: 0.0000 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -1.1066 Reward std: 4.5399\n",
      "Episode: 10047 Total reward: 0.0000 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -1.0835 Reward std: 4.4951\n",
      "[-0.07201624  0.12591699 -0.4800524 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-13.2600\n",
      "entry_point=109.3000 exit_point=97.5700 max_point=110.2800 min_point=93.3100\n",
      "Episode: 10048 Total reward: -13.6500 Training loss: 0.0379 Explore P: 0.0000 Reward mean: -1.3400 Reward std: 4.7907\n",
      "[ 0.00643843  0.14998695 -0.40168372]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.3900\n",
      "entry_point=94.2800 exit_point=90.8600 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10049 Total reward: -5.3600 Training loss: 0.0431 Explore P: 0.0000 Reward mean: -1.4204 Reward std: 4.7758\n",
      "Episode: 10050 Total reward: 0.0000 Training loss: 0.0431 Explore P: 0.0000 Reward mean: -1.3925 Reward std: 4.7329\n",
      "[-0.04469907  0.1324121  -0.4807582 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.2300\n",
      "entry_point=109.3000 exit_point=97.8700 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10051 Total reward: -11.6200 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.5892 Reward std: 4.8931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10052 Total reward: 0.0000 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.5592 Reward std: 4.8515\n",
      "Episode: 10053 Total reward: 0.0000 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.5304 Reward std: 4.8110\n",
      "Episode: 10054 Total reward: 0.0000 Training loss: 0.0367 Explore P: 0.0000 Reward mean: -1.5025 Reward std: 4.7714\n",
      "[-0.06073965  0.12857687 -0.48044467]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-19.4200\n",
      "entry_point=109.3000 exit_point=91.4100 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10055 Total reward: -19.8100 Training loss: 0.0375 Explore P: 0.0000 Reward mean: -1.8295 Reward std: 5.3139\n",
      "[-0.1427401   0.03929749 -0.4433241 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.4800\n",
      "entry_point=106.4600 exit_point=103.2800 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10056 Total reward: -2.5500 Training loss: 0.0359 Explore P: 0.0000 Reward mean: -1.8421 Reward std: 5.2680\n",
      "Episode: 10057 Total reward: 0.0000 Training loss: 0.0359 Explore P: 0.0000 Reward mean: -1.8103 Reward std: 5.2279\n",
      "Episode: 10058 Total reward: 0.0000 Training loss: 0.0359 Explore P: 0.0000 Reward mean: -1.7797 Reward std: 5.1886\n",
      "Episode: 10059 Total reward: 0.0000 Training loss: 0.0359 Explore P: 0.0000 Reward mean: -1.7500 Reward std: 5.1503\n",
      "[ 0.10368596  0.13738337 -0.43797526]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.6400\n",
      "entry_point=97.5300 exit_point=96.0200 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10060 Total reward: -1.1300 Training loss: 0.0395 Explore P: 0.0000 Reward mean: -1.7398 Reward std: 5.1085\n",
      "Episode: 10061 Total reward: 0.0000 Training loss: 0.0395 Explore P: 0.0000 Reward mean: -1.7118 Reward std: 5.0719\n",
      "Episode: 10062 Total reward: 8.5400 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.5490 Reward std: 5.1920\n",
      "Episode: 10063 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.5248 Reward std: 5.1549\n",
      "Episode: 10064 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.5014 Reward std: 5.1185\n",
      "Episode: 10065 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.4786 Reward std: 5.0829\n",
      "Episode: 10066 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.4566 Reward std: 5.0480\n",
      "Episode: 10067 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.4351 Reward std: 5.0138\n",
      "Episode: 10068 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.4143 Reward std: 4.9803\n",
      "Episode: 10069 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.3941 Reward std: 4.9475\n",
      "Episode: 10070 Total reward: 0.0000 Training loss: 0.0301 Explore P: 0.0000 Reward mean: -1.3745 Reward std: 4.9152\n",
      "Episode: 10071 Total reward: 3.3600 Training loss: 0.0349 Explore P: 0.0000 Reward mean: -1.3087 Reward std: 4.9123\n",
      "[-0.05713279  0.12936655 -0.48075783]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-19.1300\n",
      "entry_point=109.3000 exit_point=89.6000 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10072 Total reward: -19.5200 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -1.5582 Reward std: 5.3180\n",
      "Episode: 10073 Total reward: 0.0000 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -1.5372 Reward std: 5.2850\n",
      "Episode: 10074 Total reward: 0.0000 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -1.5167 Reward std: 5.2526\n",
      "[-0.07386737  0.12537572 -0.48043865]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.3400\n",
      "entry_point=109.3000 exit_point=97.4900 max_point=110.2800 min_point=93.3100\n",
      "Episode: 10075 Total reward: -11.7300 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.6511 Reward std: 5.3462\n",
      "Episode: 10076 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.6296 Reward std: 5.3146\n",
      "Episode: 10077 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.6087 Reward std: 5.2836\n",
      "Episode: 10078 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.5884 Reward std: 5.2532\n",
      "Episode: 10079 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.5685 Reward std: 5.2232\n",
      "Episode: 10080 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.5491 Reward std: 5.1938\n",
      "Episode: 10081 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.5302 Reward std: 5.1648\n",
      "Episode: 10082 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.5118 Reward std: 5.1363\n",
      "Episode: 10083 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.4938 Reward std: 5.1083\n",
      "Episode: 10084 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.4762 Reward std: 5.0807\n",
      "Episode: 10085 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.4591 Reward std: 5.0536\n",
      "Episode: 10086 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.4423 Reward std: 5.0268\n",
      "Episode: 10087 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -1.4259 Reward std: 5.0005\n",
      "[ 0.02451126  0.20821695 -0.42473197]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.7000\n",
      "entry_point=94.2800 exit_point=92.1100 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10088 Total reward: -1.6700 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.4287 Reward std: 4.9724\n",
      "Episode: 10089 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.4128 Reward std: 4.9470\n",
      "Episode: 10090 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.3973 Reward std: 4.9219\n",
      "Episode: 10091 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.3821 Reward std: 4.8973\n",
      "Episode: 10092 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.3672 Reward std: 4.8729\n",
      "Episode: 10093 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.3527 Reward std: 4.8490\n",
      "Episode: 10094 Total reward: 0.0000 Training loss: 0.0414 Explore P: 0.0000 Reward mean: -1.3384 Reward std: 4.8254\n",
      "Episode: 10095 Total reward: 4.8300 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -1.2742 Reward std: 4.8408\n",
      "Episode: 10096 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -1.2610 Reward std: 4.8176\n",
      "Episode: 10097 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -1.2482 Reward std: 4.7946\n",
      "Episode: 10098 Total reward: 7.5000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -1.1598 Reward std: 4.8499\n",
      "Episode: 10099 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -1.1482 Reward std: 4.8269\n",
      "Episode: 10100 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -0.9608 Reward std: 4.4925\n",
      "Episode: 10101 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -0.9608 Reward std: 4.4925\n",
      "Episode: 10102 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -0.9521 Reward std: 4.4935\n",
      "Episode: 10103 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -1.0228 Reward std: 4.4218\n",
      "Episode: 10104 Total reward: 0.0000 Training loss: 0.0390 Explore P: 0.0000 Reward mean: -1.0228 Reward std: 4.4218\n",
      "Episode: 10105 Total reward: -0.0900 Training loss: 0.0482 Explore P: 0.0000 Reward mean: -1.0070 Reward std: 4.4223\n",
      "Episode: 10106 Total reward: 4.5700 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -0.9525 Reward std: 4.4569\n",
      "Episode: 10107 Total reward: 0.0000 Training loss: 0.0420 Explore P: 0.0000 Reward mean: -0.9525 Reward std: 4.4569\n",
      "[-0.08198779  0.08638319 -0.4566931 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.8900\n",
      "entry_point=109.3000 exit_point=107.3400 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10108 Total reward: -1.2800 Training loss: 0.0485 Explore P: 0.0000 Reward mean: -0.9653 Reward std: 4.4560\n",
      "Episode: 10109 Total reward: 0.0000 Training loss: 0.0485 Explore P: 0.0000 Reward mean: -0.9653 Reward std: 4.4560\n",
      "Episode: 10110 Total reward: 0.0000 Training loss: 0.0485 Explore P: 0.0000 Reward mean: -0.9653 Reward std: 4.4560\n",
      "[-0.04605901  0.13200316 -0.48143765]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-9.9900\n",
      "entry_point=109.3000 exit_point=98.2600 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10111 Total reward: -10.3800 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -1.0691 Reward std: 4.5522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10112 Total reward: 0.0000 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -1.0691 Reward std: 4.5522\n",
      "Episode: 10113 Total reward: 0.0000 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -1.1152 Reward std: 4.5176\n",
      "Episode: 10114 Total reward: 0.0000 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -1.1152 Reward std: 4.5176\n",
      "Episode: 10115 Total reward: 0.0000 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -1.1152 Reward std: 4.5176\n",
      "Episode: 10116 Total reward: 0.0000 Training loss: 0.0361 Explore P: 0.0000 Reward mean: -0.9351 Reward std: 4.1875\n",
      "Episode: 10117 Total reward: 6.4100 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -0.8710 Reward std: 4.2499\n",
      "Episode: 10118 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -0.8261 Reward std: 4.2351\n",
      "Episode: 10119 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -0.8261 Reward std: 4.2351\n",
      "Episode: 10120 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -0.8261 Reward std: 4.2351\n",
      "Episode: 10121 Total reward: 0.0000 Training loss: 0.0376 Explore P: 0.0000 Reward mean: -0.8261 Reward std: 4.2351\n",
      "[-0.07729585  0.1186935  -0.47711897]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.2400\n",
      "entry_point=109.3000 exit_point=100.4600 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10122 Total reward: -11.6300 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.9424 Reward std: 4.3684\n",
      "Episode: 10123 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10124 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10125 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10126 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10127 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10128 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10129 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8909 Reward std: 4.3488\n",
      "Episode: 10130 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8533 Reward std: 4.3401\n",
      "Episode: 10131 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8533 Reward std: 4.3401\n",
      "Episode: 10132 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.8007 Reward std: 4.3182\n",
      "Episode: 10133 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10134 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10135 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10136 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10137 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10138 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10139 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10140 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10141 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10142 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10143 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10144 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6829 Reward std: 4.1754\n",
      "Episode: 10145 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.7521 Reward std: 4.1056\n",
      "Episode: 10146 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.7521 Reward std: 4.1056\n",
      "Episode: 10147 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.7521 Reward std: 4.1056\n",
      "Episode: 10148 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6156 Reward std: 3.8960\n",
      "Episode: 10149 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.5620 Reward std: 3.8672\n",
      "Episode: 10150 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.5620 Reward std: 3.8672\n",
      "Episode: 10151 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.4458 Reward std: 3.7043\n",
      "Episode: 10152 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.4458 Reward std: 3.7043\n",
      "Episode: 10153 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.4458 Reward std: 3.7043\n",
      "Episode: 10154 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.4458 Reward std: 3.7043\n",
      "[-0.07831313  0.11308157 -0.47369975]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-7.2700\n",
      "entry_point=109.3000 exit_point=101.6500 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10155 Total reward: -7.6600 Training loss: 0.0517 Explore P: 0.0000 Reward mean: -0.3243 Reward std: 3.2369\n",
      "[-0.09216079  0.02600157 -0.4194375 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.6800\n",
      "entry_point=109.3000 exit_point=107.9000 max_point=110.1300 min_point=105.9300\n",
      "Episode: 10156 Total reward: -1.0700 Training loss: 0.0373 Explore P: 0.0000 Reward mean: -0.3095 Reward std: 3.2301\n",
      "Episode: 10157 Total reward: 0.0000 Training loss: 0.0373 Explore P: 0.0000 Reward mean: -0.3095 Reward std: 3.2301\n",
      "Episode: 10158 Total reward: 0.0000 Training loss: 0.0373 Explore P: 0.0000 Reward mean: -0.3095 Reward std: 3.2301\n",
      "Episode: 10159 Total reward: 0.0000 Training loss: 0.0373 Explore P: 0.0000 Reward mean: -0.3095 Reward std: 3.2301\n",
      "[-0.08753265  0.05649194 -0.43854597]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.9800\n",
      "entry_point=109.3000 exit_point=102.9100 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10160 Total reward: -5.3700 Training loss: 0.0459 Explore P: 0.0000 Reward mean: -0.3519 Reward std: 3.2682\n",
      "Episode: 10161 Total reward: 0.0000 Training loss: 0.0459 Explore P: 0.0000 Reward mean: -0.3519 Reward std: 3.2682\n",
      "[-0.0389493   0.13487205 -0.481489  ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-12.8800\n",
      "entry_point=109.3000 exit_point=95.4900 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10162 Total reward: -13.2700 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.5700 Reward std: 3.3929\n",
      "Episode: 10163 Total reward: 0.0000 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.5700 Reward std: 3.3929\n",
      "Episode: 10164 Total reward: 0.0000 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.5700 Reward std: 3.3929\n",
      "Episode: 10165 Total reward: 0.0000 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.5700 Reward std: 3.3929\n",
      "Episode: 10166 Total reward: 0.0000 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.5700 Reward std: 3.3929\n",
      "[-0.14735326  0.02766767 -0.4376871 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-5.1800\n",
      "entry_point=106.4600 exit_point=101.6300 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10167 Total reward: -4.2500 Training loss: 0.0401 Explore P: 0.0000 Reward mean: -0.6125 Reward std: 3.4120\n",
      "Episode: 10168 Total reward: 0.0000 Training loss: 0.0401 Explore P: 0.0000 Reward mean: -0.6125 Reward std: 3.4120\n",
      "Episode: 10169 Total reward: 0.0000 Training loss: 0.0401 Explore P: 0.0000 Reward mean: -0.6125 Reward std: 3.4120\n",
      "Episode: 10170 Total reward: 0.0000 Training loss: 0.0401 Explore P: 0.0000 Reward mean: -0.6125 Reward std: 3.4120\n",
      "[-0.15036806  0.00946814 -0.42641893]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.5800\n",
      "entry_point=106.4600 exit_point=104.7900 max_point=110.9400 min_point=103.8300\n",
      "Episode: 10171 Total reward: -0.6500 Training loss: 0.0297 Explore P: 0.0000 Reward mean: -0.6526 Reward std: 3.3886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10172 Total reward: 0.0000 Training loss: 0.0297 Explore P: 0.0000 Reward mean: -0.4574 Reward std: 2.8087\n",
      "Episode: 10173 Total reward: 0.0000 Training loss: 0.0297 Explore P: 0.0000 Reward mean: -0.4574 Reward std: 2.8087\n",
      "Episode: 10174 Total reward: 3.7800 Training loss: 0.0430 Explore P: 0.0000 Reward mean: -0.4196 Reward std: 2.8399\n",
      "Episode: 10175 Total reward: 0.0000 Training loss: 0.0430 Explore P: 0.0000 Reward mean: -0.3023 Reward std: 2.6026\n",
      "[ 0.00759627  0.19804716 -0.42731595]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4000\n",
      "entry_point=94.2800 exit_point=90.3700 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10176 Total reward: -3.3700 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "Episode: 10177 Total reward: 0.0000 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "Episode: 10178 Total reward: 0.0000 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "Episode: 10179 Total reward: 0.0000 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "Episode: 10180 Total reward: 0.0000 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "Episode: 10181 Total reward: 0.0000 Training loss: 0.0444 Explore P: 0.0000 Reward mean: -0.3360 Reward std: 2.6203\n",
      "[ 0.10809359  0.13841355 -0.43911868]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.5000\n",
      "entry_point=97.5300 exit_point=94.9900 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10182 Total reward: -1.9900 Training loss: 0.0356 Explore P: 0.0000 Reward mean: -0.3559 Reward std: 2.6252\n",
      "Episode: 10183 Total reward: 0.0000 Training loss: 0.0356 Explore P: 0.0000 Reward mean: -0.3559 Reward std: 2.6252\n",
      "[-0.03694995  0.13602045 -0.48160556]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-13.9400\n",
      "entry_point=109.3000 exit_point=96.4700 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10184 Total reward: -14.3300 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4992 Reward std: 2.9703\n",
      "Episode: 10185 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4992 Reward std: 2.9703\n",
      "Episode: 10186 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4992 Reward std: 2.9703\n",
      "Episode: 10187 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4992 Reward std: 2.9703\n",
      "Episode: 10188 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4825 Reward std: 2.9683\n",
      "Episode: 10189 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4825 Reward std: 2.9683\n",
      "Episode: 10190 Total reward: 0.0000 Training loss: 0.0381 Explore P: 0.0000 Reward mean: -0.4825 Reward std: 2.9683\n",
      "[-0.09162779  0.03858891 -0.42803037]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-9.0500\n",
      "entry_point=109.3000 exit_point=100.7200 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10191 Total reward: -9.4400 Training loss: 0.0362 Explore P: 0.0000 Reward mean: -0.5769 Reward std: 3.0987\n",
      "Episode: 10192 Total reward: 0.0000 Training loss: 0.0362 Explore P: 0.0000 Reward mean: -0.5769 Reward std: 3.0987\n",
      "Episode: 10193 Total reward: 0.0000 Training loss: 0.0362 Explore P: 0.0000 Reward mean: -0.5769 Reward std: 3.0987\n",
      "Episode: 10194 Total reward: 0.0000 Training loss: 0.0362 Explore P: 0.0000 Reward mean: -0.5769 Reward std: 3.0987\n",
      "Episode: 10195 Total reward: -0.4500 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.6297 Reward std: 3.0508\n",
      "Episode: 10196 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.6297 Reward std: 3.0508\n",
      "Episode: 10197 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.6297 Reward std: 3.0508\n",
      "Episode: 10198 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.7047 Reward std: 2.9402\n",
      "Episode: 10199 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.7047 Reward std: 2.9402\n",
      "Episode: 10200 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.7047 Reward std: 2.9402\n",
      "Episode: 10201 Total reward: 0.0000 Training loss: 0.0415 Explore P: 0.0000 Reward mean: -0.7047 Reward std: 2.9402\n",
      "[-0.15016174  0.01852694 -0.4326728 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.1100\n",
      "entry_point=106.4600 exit_point=106.0300 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10202 Total reward: 0.8200 Training loss: 0.0387 Explore P: 0.0000 Reward mean: -0.6965 Reward std: 2.9433\n",
      "Episode: 10203 Total reward: 0.0000 Training loss: 0.0387 Explore P: 0.0000 Reward mean: -0.6965 Reward std: 2.9433\n",
      "Episode: 10204 Total reward: 0.0000 Training loss: 0.0387 Explore P: 0.0000 Reward mean: -0.6965 Reward std: 2.9433\n",
      "[-0.08156645  0.1062645  -0.47045428]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.4500\n",
      "entry_point=109.3000 exit_point=106.9600 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10205 Total reward: -3.8400 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7340 Reward std: 2.9592\n",
      "Episode: 10206 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7797 Reward std: 2.9118\n",
      "Episode: 10207 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7797 Reward std: 2.9118\n",
      "Episode: 10208 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7669 Reward std: 2.9124\n",
      "Episode: 10209 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7669 Reward std: 2.9124\n",
      "Episode: 10210 Total reward: 0.0000 Training loss: 0.0400 Explore P: 0.0000 Reward mean: -0.7669 Reward std: 2.9124\n",
      "[-0.09339434  0.03218403 -0.4243617 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-6.5900\n",
      "entry_point=109.3000 exit_point=102.8800 max_point=110.1300 min_point=103.0900\n",
      "Episode: 10211 Total reward: -6.9800 Training loss: 0.0396 Explore P: 0.0000 Reward mean: -0.7329 Reward std: 2.8183\n",
      "Episode: 10212 Total reward: 0.0000 Training loss: 0.0396 Explore P: 0.0000 Reward mean: -0.7329 Reward std: 2.8183\n",
      "Episode: 10213 Total reward: 0.0000 Training loss: 0.0396 Explore P: 0.0000 Reward mean: -0.7329 Reward std: 2.8183\n",
      "Episode: 10214 Total reward: 0.0000 Training loss: 0.0396 Explore P: 0.0000 Reward mean: -0.7329 Reward std: 2.8183\n",
      "Episode: 10215 Total reward: 3.3700 Training loss: 0.0386 Explore P: 0.0000 Reward mean: -0.6992 Reward std: 2.8469\n",
      "Episode: 10216 Total reward: 0.0000 Training loss: 0.0386 Explore P: 0.0000 Reward mean: -0.6992 Reward std: 2.8469\n",
      "Episode: 10217 Total reward: 0.0000 Training loss: 0.0386 Explore P: 0.0000 Reward mean: -0.7633 Reward std: 2.7568\n",
      "[-0.08547866  0.08518738 -0.4575547 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.8100\n",
      "entry_point=109.3000 exit_point=107.4200 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10218 Total reward: -1.2000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7753 Reward std: 2.7561\n",
      "Episode: 10219 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7753 Reward std: 2.7561\n",
      "[-0.08311467  0.10156441 -0.4678357 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.7500\n",
      "entry_point=109.3000 exit_point=107.7700 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10220 Total reward: -2.1400 Training loss: 0.0399 Explore P: 0.0000 Reward mean: -0.7967 Reward std: 2.7583\n",
      "Episode: 10221 Total reward: 0.0000 Training loss: 0.0399 Explore P: 0.0000 Reward mean: -0.7967 Reward std: 2.7583\n",
      "Episode: 10222 Total reward: 0.0000 Training loss: 0.0399 Explore P: 0.0000 Reward mean: -0.6804 Reward std: 2.5352\n",
      "Episode: 10223 Total reward: 0.0000 Training loss: 0.0399 Explore P: 0.0000 Reward mean: -0.6804 Reward std: 2.5352\n",
      "Episode: 10224 Total reward: -0.3200 Training loss: 0.0409 Explore P: 0.0000 Reward mean: -0.6836 Reward std: 2.5345\n",
      "[-0.04004029  0.13454062 -0.4823975 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-14.0500\n",
      "entry_point=109.3000 exit_point=94.5400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10225 Total reward: -14.4400 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10226 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10227 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10228 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10229 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10230 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10231 Total reward: 0.0000 Training loss: 0.0388 Explore P: 0.0000 Reward mean: -0.8280 Reward std: 2.8794\n",
      "Episode: 10232 Total reward: 5.1900 Training loss: 0.0487 Explore P: 0.0000 Reward mean: -0.7761 Reward std: 2.9400\n",
      "Episode: 10233 Total reward: 0.0000 Training loss: 0.0487 Explore P: 0.0000 Reward mean: -0.7761 Reward std: 2.9400\n",
      "Episode: 10234 Total reward: 0.0000 Training loss: 0.0487 Explore P: 0.0000 Reward mean: -0.7761 Reward std: 2.9400\n",
      "Episode: 10235 Total reward: 0.0000 Training loss: 0.0487 Explore P: 0.0000 Reward mean: -0.7761 Reward std: 2.9400\n",
      "Episode: 10236 Total reward: 0.0000 Training loss: 0.0487 Explore P: 0.0000 Reward mean: -0.7761 Reward std: 2.9400\n",
      "Episode: 10237 Total reward: 11.5000 Training loss: 0.0434 Explore P: 0.0000 Reward mean: -0.6611 Reward std: 3.1830\n",
      "Episode: 10238 Total reward: 0.0000 Training loss: 0.0434 Explore P: 0.0000 Reward mean: -0.6611 Reward std: 3.1830\n",
      "Episode: 10239 Total reward: 0.0000 Training loss: 0.0434 Explore P: 0.0000 Reward mean: -0.6611 Reward std: 3.1830\n",
      "[-0.15236023  0.01687318 -0.4324935 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.3600\n",
      "entry_point=106.4600 exit_point=105.9800 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10240 Total reward: -0.4300 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -0.6654 Reward std: 3.1823\n",
      "Episode: 10241 Total reward: 0.0000 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -0.6654 Reward std: 3.1823\n",
      "Episode: 10242 Total reward: 0.0000 Training loss: 0.0413 Explore P: 0.0000 Reward mean: -0.6654 Reward std: 3.1823\n",
      "[-0.09030488  0.06251749 -0.44398713]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.4900\n",
      "entry_point=109.3000 exit_point=105.3100 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10243 Total reward: -4.8800 Training loss: 0.0460 Explore P: 0.0000 Reward mean: -0.7142 Reward std: 3.2091\n",
      "Episode: 10244 Total reward: 0.0000 Training loss: 0.0460 Explore P: 0.0000 Reward mean: -0.7142 Reward std: 3.2091\n",
      "Episode: 10245 Total reward: 0.0000 Training loss: 0.0460 Explore P: 0.0000 Reward mean: -0.7142 Reward std: 3.2091\n",
      "Episode: 10246 Total reward: 7.0200 Training loss: 0.0442 Explore P: 0.0000 Reward mean: -0.6440 Reward std: 3.2994\n",
      "Episode: 10247 Total reward: 0.0000 Training loss: 0.0442 Explore P: 0.0000 Reward mean: -0.6440 Reward std: 3.2994\n",
      "Episode: 10248 Total reward: 0.0000 Training loss: 0.0442 Explore P: 0.0000 Reward mean: -0.6440 Reward std: 3.2994\n",
      "Episode: 10249 Total reward: 0.0000 Training loss: 0.0442 Explore P: 0.0000 Reward mean: -0.6440 Reward std: 3.2994\n",
      "[ 0.1038584   0.13753998 -0.4398507 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.7800\n",
      "entry_point=97.5300 exit_point=94.7400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10250 Total reward: -2.2700 Training loss: 0.0378 Explore P: 0.0000 Reward mean: -0.6667 Reward std: 3.3027\n",
      "Episode: 10251 Total reward: 0.0000 Training loss: 0.0378 Explore P: 0.0000 Reward mean: -0.6667 Reward std: 3.3027\n",
      "[ 0.00114463  0.15484223 -0.40859187]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.2300\n",
      "entry_point=94.2800 exit_point=90.8300 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10252 Total reward: -3.2000 Training loss: 0.0434 Explore P: 0.0000 Reward mean: -0.6987 Reward std: 3.3116\n",
      "[-0.1556894   0.00139162 -0.42329767]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.2900\n",
      "entry_point=106.4600 exit_point=106.7500 max_point=110.9400 min_point=106.4600\n",
      "Episode: 10253 Total reward: 0.6400 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6923 Reward std: 3.3136\n",
      "Episode: 10254 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6923 Reward std: 3.3136\n",
      "Episode: 10255 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6157 Reward std: 3.2393\n",
      "Episode: 10256 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6050 Reward std: 3.2396\n",
      "Episode: 10257 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6050 Reward std: 3.2396\n",
      "Episode: 10258 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6050 Reward std: 3.2396\n",
      "Episode: 10259 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.6050 Reward std: 3.2396\n",
      "Episode: 10260 Total reward: 0.0000 Training loss: 0.0408 Explore P: 0.0000 Reward mean: -0.5513 Reward std: 3.2045\n",
      "[ 0.00154448  0.16609067 -0.4141995 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.0500\n",
      "entry_point=94.2800 exit_point=89.1600 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10261 Total reward: -5.0200 Training loss: 0.0407 Explore P: 0.0000 Reward mean: -0.6015 Reward std: 3.2346\n",
      "Episode: 10262 Total reward: 0.0000 Training loss: 0.0407 Explore P: 0.0000 Reward mean: -0.4688 Reward std: 2.9738\n",
      "Episode: 10263 Total reward: 0.0000 Training loss: 0.0407 Explore P: 0.0000 Reward mean: -0.4688 Reward std: 2.9738\n",
      "Episode: 10264 Total reward: 0.0000 Training loss: 0.0407 Explore P: 0.0000 Reward mean: -0.4688 Reward std: 2.9738\n",
      "Episode: 10265 Total reward: 0.0000 Training loss: 0.0407 Explore P: 0.0000 Reward mean: -0.4688 Reward std: 2.9738\n",
      "[-0.06787454  0.1268293  -0.48270038]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-20.0100\n",
      "entry_point=109.3000 exit_point=89.9400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10266 Total reward: -20.4000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.6728 Reward std: 3.5739\n",
      "Episode: 10267 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.6303 Reward std: 3.5563\n",
      "Episode: 10268 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.6303 Reward std: 3.5563\n",
      "[-0.07287495  0.12563378 -0.48265004]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-18.0700\n",
      "entry_point=109.3000 exit_point=90.4300 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10269 Total reward: -18.4600 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8149 Reward std: 3.9734\n",
      "Episode: 10270 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8149 Reward std: 3.9734\n",
      "Episode: 10271 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8084 Reward std: 3.9742\n",
      "Episode: 10272 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8084 Reward std: 3.9742\n",
      "Episode: 10273 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8084 Reward std: 3.9742\n",
      "Episode: 10274 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8462 Reward std: 3.9483\n",
      "Episode: 10275 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8462 Reward std: 3.9483\n",
      "Episode: 10276 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "Episode: 10277 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "Episode: 10278 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "Episode: 10279 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "Episode: 10280 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "Episode: 10281 Total reward: 0.0000 Training loss: 0.0472 Explore P: 0.0000 Reward mean: -0.8125 Reward std: 3.9410\n",
      "[ 0.00291818  0.19579716 -0.42876366]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.5600\n",
      "entry_point=94.2800 exit_point=90.2600 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10282 Total reward: -4.5300 Training loss: 0.0538 Explore P: 0.0000 Reward mean: -0.8379 Reward std: 3.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10283 Total reward: 0.0000 Training loss: 0.0538 Explore P: 0.0000 Reward mean: -0.8379 Reward std: 3.9567\n",
      "Episode: 10284 Total reward: 0.0000 Training loss: 0.0538 Explore P: 0.0000 Reward mean: -0.6946 Reward std: 3.7177\n",
      "[-0.14884177  0.05212718 -0.45530844]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.2200\n",
      "entry_point=106.4600 exit_point=104.4900 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10285 Total reward: -2.2900 Training loss: 0.0447 Explore P: 0.0000 Reward mean: -0.7175 Reward std: 3.7204\n",
      "Episode: 10286 Total reward: 0.0000 Training loss: 0.0447 Explore P: 0.0000 Reward mean: -0.7175 Reward std: 3.7204\n",
      "Episode: 10287 Total reward: 0.0000 Training loss: 0.0447 Explore P: 0.0000 Reward mean: -0.7175 Reward std: 3.7204\n",
      "Episode: 10288 Total reward: 0.0000 Training loss: 0.0447 Explore P: 0.0000 Reward mean: -0.7175 Reward std: 3.7204\n",
      "[-0.04749061  0.13186634 -0.4834011 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.1500\n",
      "entry_point=109.3000 exit_point=96.5500 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10289 Total reward: -11.5400 Training loss: 0.0478 Explore P: 0.0000 Reward mean: -0.8329 Reward std: 3.8722\n",
      "Episode: 10290 Total reward: 0.0000 Training loss: 0.0478 Explore P: 0.0000 Reward mean: -0.8329 Reward std: 3.8722\n",
      "Episode: 10291 Total reward: 6.5800 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6727 Reward std: 3.8441\n",
      "Episode: 10292 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6727 Reward std: 3.8441\n",
      "Episode: 10293 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6727 Reward std: 3.8441\n",
      "Episode: 10294 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6727 Reward std: 3.8441\n",
      "Episode: 10295 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6682 Reward std: 3.8446\n",
      "Episode: 10296 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6682 Reward std: 3.8446\n",
      "Episode: 10297 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6682 Reward std: 3.8446\n",
      "Episode: 10298 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6682 Reward std: 3.8446\n",
      "Episode: 10299 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.6682 Reward std: 3.8446\n",
      "Episode: 10300 Total reward: 6.3100 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.6051 Reward std: 3.9064\n",
      "Episode: 10301 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.6051 Reward std: 3.9064\n",
      "Episode: 10302 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.6133 Reward std: 3.9042\n",
      "Episode: 10303 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.6133 Reward std: 3.9042\n",
      "Episode: 10304 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.6133 Reward std: 3.9042\n",
      "Episode: 10305 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10306 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10307 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10308 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10309 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10310 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5749 Reward std: 3.8912\n",
      "Episode: 10311 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5051 Reward std: 3.8379\n",
      "Episode: 10312 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5051 Reward std: 3.8379\n",
      "Episode: 10313 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5051 Reward std: 3.8379\n",
      "Episode: 10314 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5051 Reward std: 3.8379\n",
      "Episode: 10315 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5388 Reward std: 3.8184\n",
      "Episode: 10316 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5388 Reward std: 3.8184\n",
      "Episode: 10317 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5388 Reward std: 3.8184\n",
      "Episode: 10318 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5268 Reward std: 3.8182\n",
      "Episode: 10319 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5268 Reward std: 3.8182\n",
      "Episode: 10320 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5054 Reward std: 3.8151\n",
      "Episode: 10321 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5054 Reward std: 3.8151\n",
      "Episode: 10322 Total reward: 0.0000 Training loss: 0.0437 Explore P: 0.0000 Reward mean: -0.5054 Reward std: 3.8151\n",
      "[ 0.10446595  0.1378682  -0.4404531 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4700\n",
      "entry_point=97.5300 exit_point=95.4600 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10323 Total reward: -2.9600 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -0.5350 Reward std: 3.8226\n",
      "Episode: 10324 Total reward: 0.0000 Training loss: 0.0423 Explore P: 0.0000 Reward mean: -0.5318 Reward std: 3.8229\n",
      "[-0.04624641  0.1322648  -0.48368895]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-13.4200\n",
      "entry_point=109.3000 exit_point=95.9100 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10325 Total reward: -13.8100 Training loss: 0.0438 Explore P: 0.0000 Reward mean: -0.5255 Reward std: 3.8004\n",
      "[-0.08214553  0.12353963 -0.48295683]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-14.7000\n",
      "entry_point=109.3000 exit_point=95.8500 max_point=110.2800 min_point=93.3100\n",
      "Episode: 10326 Total reward: -15.0900 Training loss: 0.0446 Explore P: 0.0000 Reward mean: -0.6764 Reward std: 4.0668\n",
      "Episode: 10327 Total reward: 7.2900 Training loss: 0.0387 Explore P: 0.0000 Reward mean: -0.6035 Reward std: 4.1429\n",
      "Episode: 10328 Total reward: 0.0000 Training loss: 0.0387 Explore P: 0.0000 Reward mean: -0.6035 Reward std: 4.1429\n",
      "[-0.09128971  0.07868773 -0.4554662 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.8200\n",
      "entry_point=109.3000 exit_point=104.8200 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10329 Total reward: -4.2100 Training loss: 0.0473 Explore P: 0.0000 Reward mean: -0.6456 Reward std: 4.1579\n",
      "Episode: 10330 Total reward: 0.0000 Training loss: 0.0473 Explore P: 0.0000 Reward mean: -0.6456 Reward std: 4.1579\n",
      "[-0.08792896  0.10177174 -0.46989882]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4100\n",
      "entry_point=109.3000 exit_point=104.3900 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10331 Total reward: -2.8000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.6736 Reward std: 4.1629\n",
      "Episode: 10332 Total reward: 0.0000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7255 Reward std: 4.1216\n",
      "Episode: 10333 Total reward: 0.0000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7255 Reward std: 4.1216\n",
      "Episode: 10334 Total reward: 0.0000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7255 Reward std: 4.1216\n",
      "Episode: 10335 Total reward: 0.0000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7255 Reward std: 4.1216\n",
      "Episode: 10336 Total reward: 0.0000 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7255 Reward std: 4.1216\n",
      "Episode: 10337 Total reward: 7.7800 Training loss: 0.0461 Explore P: 0.0000 Reward mean: -0.7627 Reward std: 4.0268\n",
      "[ 0.10330262  0.13769644 -0.4408018 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.2900\n",
      "entry_point=97.5300 exit_point=94.5700 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10338 Total reward: -2.7800 Training loss: 0.0470 Explore P: 0.0000 Reward mean: -0.7905 Reward std: 4.0310\n",
      "Episode: 10339 Total reward: 0.0000 Training loss: 0.0470 Explore P: 0.0000 Reward mean: -0.7905 Reward std: 4.0310\n",
      "Episode: 10340 Total reward: 0.0000 Training loss: 0.0470 Explore P: 0.0000 Reward mean: -0.7862 Reward std: 4.0316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10341 Total reward: 0.0000 Training loss: 0.0470 Explore P: 0.0000 Reward mean: -0.7862 Reward std: 4.0316\n",
      "Episode: 10342 Total reward: 0.0000 Training loss: 0.0470 Explore P: 0.0000 Reward mean: -0.7862 Reward std: 4.0316\n",
      "Episode: 10343 Total reward: 2.3600 Training loss: 0.0402 Explore P: 0.0000 Reward mean: -0.7138 Reward std: 4.0225\n",
      "Episode: 10344 Total reward: 0.0000 Training loss: 0.0402 Explore P: 0.0000 Reward mean: -0.7138 Reward std: 4.0225\n",
      "Episode: 10345 Total reward: 0.0000 Training loss: 0.0402 Explore P: 0.0000 Reward mean: -0.7138 Reward std: 4.0225\n",
      "[-0.10107359  0.02257058 -0.4210138 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.7900\n",
      "entry_point=109.3000 exit_point=107.9500 max_point=110.1300 min_point=105.9300\n",
      "Episode: 10346 Total reward: -1.1800 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7958 Reward std: 3.9469\n",
      "[ 0.01042206  0.20096195 -0.4287403 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.8600\n",
      "entry_point=94.2800 exit_point=91.8400 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10347 Total reward: -1.8300 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.8141 Reward std: 3.9474\n",
      "Episode: 10348 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.8141 Reward std: 3.9474\n",
      "Episode: 10349 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.8141 Reward std: 3.9474\n",
      "Episode: 10350 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7914 Reward std: 3.9455\n",
      "Episode: 10351 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7914 Reward std: 3.9455\n",
      "Episode: 10352 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7594 Reward std: 3.9388\n",
      "Episode: 10353 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7658 Reward std: 3.9370\n",
      "Episode: 10354 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7658 Reward std: 3.9370\n",
      "Episode: 10355 Total reward: 0.0000 Training loss: 0.0394 Explore P: 0.0000 Reward mean: -0.7658 Reward std: 3.9370\n",
      "[ 0.002335    0.19604461 -0.4296655 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.5500\n",
      "entry_point=94.2800 exit_point=89.9500 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10356 Total reward: -4.5200 Training loss: 0.0424 Explore P: 0.0000 Reward mean: -0.8110 Reward std: 3.9539\n",
      "[-0.1583017   0.01430726 -0.43324342]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.3600\n",
      "entry_point=106.4600 exit_point=105.9800 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10357 Total reward: -0.4300 Training loss: 0.0418 Explore P: 0.0000 Reward mean: -0.8153 Reward std: 3.9532\n",
      "[-0.09059643  0.09567893 -0.46678367]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.3500\n",
      "entry_point=109.3000 exit_point=108.6400 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10358 Total reward: -0.7400 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.8227 Reward std: 3.9524\n",
      "Episode: 10359 Total reward: 0.0000 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.8227 Reward std: 3.9524\n",
      "Episode: 10360 Total reward: 0.0000 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.8227 Reward std: 3.9524\n",
      "Episode: 10361 Total reward: 0.0000 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.7725 Reward std: 3.9306\n",
      "Episode: 10362 Total reward: 0.0000 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.7725 Reward std: 3.9306\n",
      "Episode: 10363 Total reward: 0.0000 Training loss: 0.0422 Explore P: 0.0000 Reward mean: -0.7725 Reward std: 3.9306\n",
      "Episode: 10364 Total reward: 3.7600 Training loss: 0.0411 Explore P: 0.0000 Reward mean: -0.7349 Reward std: 3.9557\n",
      "Episode: 10365 Total reward: 0.0000 Training loss: 0.0411 Explore P: 0.0000 Reward mean: -0.7349 Reward std: 3.9557\n",
      "Episode: 10366 Total reward: 0.0000 Training loss: 0.0411 Explore P: 0.0000 Reward mean: -0.5309 Reward std: 3.4269\n",
      "Episode: 10367 Total reward: 0.0000 Training loss: 0.0411 Explore P: 0.0000 Reward mean: -0.5309 Reward std: 3.4269\n",
      "Episode: 10368 Total reward: 0.0000 Training loss: 0.0411 Explore P: 0.0000 Reward mean: -0.5309 Reward std: 3.4269\n",
      "Episode: 10369 Total reward: 4.6700 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.2996 Reward std: 2.9574\n",
      "Episode: 10370 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.2996 Reward std: 2.9574\n",
      "Episode: 10371 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.2996 Reward std: 2.9574\n",
      "Episode: 10372 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.2996 Reward std: 2.9574\n",
      "Episode: 10373 Total reward: 0.0000 Training loss: 0.0465 Explore P: 0.0000 Reward mean: -0.2996 Reward std: 2.9574\n",
      "[ 0.00160777  0.19595245 -0.429986  ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.2300\n",
      "entry_point=94.2800 exit_point=89.7600 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10374 Total reward: -4.2000 Training loss: 0.0469 Explore P: 0.0000 Reward mean: -0.3416 Reward std: 2.9826\n",
      "Episode: 10375 Total reward: 0.0000 Training loss: 0.0469 Explore P: 0.0000 Reward mean: -0.3416 Reward std: 2.9826\n",
      "Episode: 10376 Total reward: 0.0000 Training loss: 0.0469 Explore P: 0.0000 Reward mean: -0.3416 Reward std: 2.9826\n",
      "Episode: 10377 Total reward: 5.8600 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2830 Reward std: 3.0456\n",
      "Episode: 10378 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2830 Reward std: 3.0456\n",
      "Episode: 10379 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2830 Reward std: 3.0456\n",
      "Episode: 10380 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2830 Reward std: 3.0456\n",
      "Episode: 10381 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2830 Reward std: 3.0456\n",
      "Episode: 10382 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2377 Reward std: 3.0157\n",
      "Episode: 10383 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2377 Reward std: 3.0157\n",
      "Episode: 10384 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2377 Reward std: 3.0157\n",
      "Episode: 10385 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2148 Reward std: 3.0087\n",
      "Episode: 10386 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2148 Reward std: 3.0087\n",
      "Episode: 10387 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2148 Reward std: 3.0087\n",
      "Episode: 10388 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2148 Reward std: 3.0087\n",
      "Episode: 10389 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.0994 Reward std: 2.7851\n",
      "Episode: 10390 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.0994 Reward std: 2.7851\n",
      "Episode: 10391 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10392 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10393 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10394 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10395 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10396 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10397 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10398 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10399 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.1652 Reward std: 2.7030\n",
      "Episode: 10400 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n",
      "Episode: 10401 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10402 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n",
      "Episode: 10403 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n",
      "Episode: 10404 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n",
      "Episode: 10405 Total reward: 0.0000 Training loss: 0.0412 Explore P: 0.0000 Reward mean: -0.2283 Reward std: 2.6236\n",
      "[-0.10403353  0.01715919 -0.41843358]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4500\n",
      "entry_point=109.3000 exit_point=106.6600 max_point=110.1300 min_point=105.9300\n",
      "Episode: 10406 Total reward: -2.8400 Training loss: 0.0493 Explore P: 0.0000 Reward mean: -0.2567 Reward std: 2.6363\n",
      "Episode: 10407 Total reward: 0.0000 Training loss: 0.0493 Explore P: 0.0000 Reward mean: -0.2567 Reward std: 2.6363\n",
      "[-0.06937274  0.12707701 -0.4843325 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-18.1500\n",
      "entry_point=109.3000 exit_point=90.3000 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10408 Total reward: -18.5400 Training loss: 0.0486 Explore P: 0.0000 Reward mean: -0.4421 Reward std: 3.2028\n",
      "Episode: 10409 Total reward: 0.0000 Training loss: 0.0486 Explore P: 0.0000 Reward mean: -0.4421 Reward std: 3.2028\n",
      "Episode: 10410 Total reward: 0.0000 Training loss: 0.0486 Explore P: 0.0000 Reward mean: -0.4421 Reward std: 3.2028\n",
      "Episode: 10411 Total reward: 5.9800 Training loss: 0.0561 Explore P: 0.0000 Reward mean: -0.3823 Reward std: 3.2657\n",
      "Episode: 10412 Total reward: 0.0000 Training loss: 0.0561 Explore P: 0.0000 Reward mean: -0.3823 Reward std: 3.2657\n",
      "Episode: 10413 Total reward: 5.8300 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.3240 Reward std: 3.3236\n",
      "Episode: 10414 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.3240 Reward std: 3.3236\n",
      "[-0.06405925  0.12847364 -0.48466092]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.7100\n",
      "entry_point=109.3000 exit_point=98.7400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10415 Total reward: -12.1000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "Episode: 10416 Total reward: 0.0000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "Episode: 10417 Total reward: 0.0000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "Episode: 10418 Total reward: 0.0000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "Episode: 10419 Total reward: 0.0000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "Episode: 10420 Total reward: 0.0000 Training loss: 0.0513 Explore P: 0.0000 Reward mean: -0.4450 Reward std: 3.5238\n",
      "[-0.09003449  0.11537784 -0.47983098]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-14.6300\n",
      "entry_point=109.3000 exit_point=97.3200 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10421 Total reward: -15.0200 Training loss: 0.0497 Explore P: 0.0000 Reward mean: -0.5952 Reward std: 3.8101\n",
      "[-0.10286103  0.03395358 -0.42940336]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-8.1900\n",
      "entry_point=109.3000 exit_point=99.8600 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10422 Total reward: -8.5800 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6810 Reward std: 3.8915\n",
      "Episode: 10423 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6514 Reward std: 3.8853\n",
      "Episode: 10424 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.6514 Reward std: 3.8853\n",
      "Episode: 10425 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.5133 Reward std: 3.6536\n",
      "Episode: 10426 Total reward: 0.0000 Training loss: 0.0416 Explore P: 0.0000 Reward mean: -0.3624 Reward std: 3.3472\n",
      "[ 0.00440733  0.19829813 -0.43015444]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.2800\n",
      "entry_point=94.2800 exit_point=90.1800 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10427 Total reward: -4.2500 Training loss: 0.0474 Explore P: 0.0000 Reward mean: -0.4778 Reward std: 3.2797\n",
      "Episode: 10428 Total reward: 0.0000 Training loss: 0.0474 Explore P: 0.0000 Reward mean: -0.4778 Reward std: 3.2797\n",
      "[-0.15763217  0.04134205 -0.45129195]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-5.9600\n",
      "entry_point=106.4600 exit_point=101.1200 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10429 Total reward: -5.0300 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4860 Reward std: 3.2900\n",
      "Episode: 10430 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4860 Reward std: 3.2900\n",
      "Episode: 10431 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10432 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10433 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10434 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10435 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10436 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.4580 Reward std: 3.2821\n",
      "Episode: 10437 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5358 Reward std: 3.1764\n",
      "Episode: 10438 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5080 Reward std: 3.1688\n",
      "Episode: 10439 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5080 Reward std: 3.1688\n",
      "Episode: 10440 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5080 Reward std: 3.1688\n",
      "Episode: 10441 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5080 Reward std: 3.1688\n",
      "[-0.00377744  0.1851215  -0.4270136 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.0100\n",
      "entry_point=94.2800 exit_point=93.4000 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10442 Total reward: -1.9800 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5278 Reward std: 3.1717\n",
      "Episode: 10443 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5514 Reward std: 3.1589\n",
      "Episode: 10444 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5514 Reward std: 3.1589\n",
      "Episode: 10445 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5514 Reward std: 3.1589\n",
      "Episode: 10446 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5396 Reward std: 3.1588\n",
      "Episode: 10447 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5213 Reward std: 3.1565\n",
      "Episode: 10448 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5213 Reward std: 3.1565\n",
      "Episode: 10449 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5213 Reward std: 3.1565\n",
      "Episode: 10450 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5213 Reward std: 3.1565\n",
      "Episode: 10451 Total reward: 0.0000 Training loss: 0.0467 Explore P: 0.0000 Reward mean: -0.5213 Reward std: 3.1565\n",
      "[-0.15626436  0.05340669 -0.45896143]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.3800\n",
      "entry_point=106.4600 exit_point=104.1500 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10452 Total reward: -3.4500 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.5558 Reward std: 3.1695\n",
      "Episode: 10453 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.5558 Reward std: 3.1695\n",
      "Episode: 10454 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.5558 Reward std: 3.1695\n",
      "Episode: 10455 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.5558 Reward std: 3.1695\n",
      "[ 0.10107058  0.13782373 -0.44209027]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.2600\n",
      "entry_point=97.5300 exit_point=93.9500 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10456 Total reward: -3.7500 Training loss: 0.0497 Explore P: 0.0000 Reward mean: -0.5481 Reward std: 3.1608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10053965  0.1377221  -0.44214123]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.0900\n",
      "entry_point=97.5300 exit_point=94.0300 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10457 Total reward: -3.5800 Training loss: 0.0460 Explore P: 0.0000 Reward mean: -0.5796 Reward std: 3.1751\n",
      "Episode: 10458 Total reward: 0.0000 Training loss: 0.0460 Explore P: 0.0000 Reward mean: -0.5722 Reward std: 3.1756\n",
      "Episode: 10459 Total reward: 7.7800 Training loss: 0.0479 Explore P: 0.0000 Reward mean: -0.4944 Reward std: 3.2821\n",
      "[-0.00542853  0.17241916 -0.42157844]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-5.0500\n",
      "entry_point=94.2800 exit_point=88.5700 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10460 Total reward: -6.0200 Training loss: 0.0464 Explore P: 0.0000 Reward mean: -0.5546 Reward std: 3.3274\n",
      "Episode: 10461 Total reward: 0.0000 Training loss: 0.0464 Explore P: 0.0000 Reward mean: -0.5546 Reward std: 3.3274\n",
      "Episode: 10462 Total reward: 0.0000 Training loss: 0.0464 Explore P: 0.0000 Reward mean: -0.5546 Reward std: 3.3274\n",
      "Episode: 10463 Total reward: 0.0000 Training loss: 0.0464 Explore P: 0.0000 Reward mean: -0.5546 Reward std: 3.3274\n",
      "Episode: 10464 Total reward: 0.0000 Training loss: 0.0464 Explore P: 0.0000 Reward mean: -0.5922 Reward std: 3.2996\n",
      "Episode: 10465 Total reward: 6.5000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5272 Reward std: 3.3738\n",
      "Episode: 10466 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5272 Reward std: 3.3738\n",
      "Episode: 10467 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5272 Reward std: 3.3738\n",
      "Episode: 10468 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5272 Reward std: 3.3738\n",
      "Episode: 10469 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5739 Reward std: 3.3336\n",
      "Episode: 10470 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5739 Reward std: 3.3336\n",
      "Episode: 10471 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5739 Reward std: 3.3336\n",
      "Episode: 10472 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5739 Reward std: 3.3336\n",
      "Episode: 10473 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5739 Reward std: 3.3336\n",
      "Episode: 10474 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5319 Reward std: 3.3141\n",
      "Episode: 10475 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.5319 Reward std: 3.3141\n",
      "[-0.09281711  0.11369118 -0.4797224 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.2400\n",
      "entry_point=109.3000 exit_point=100.4600 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10476 Total reward: -11.6300 Training loss: 0.0531 Explore P: 0.0000 Reward mean: -0.6482 Reward std: 3.4926\n",
      "[-0.00535999  0.18176375 -0.4263925 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.9400\n",
      "entry_point=94.2800 exit_point=92.9300 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10477 Total reward: -1.9100 Training loss: 0.0512 Explore P: 0.0000 Reward mean: -0.7259 Reward std: 3.4329\n",
      "Episode: 10478 Total reward: 7.0100 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.6558 Reward std: 3.5175\n",
      "Episode: 10479 Total reward: 0.0000 Training loss: 0.0450 Explore P: 0.0000 Reward mean: -0.6558 Reward std: 3.5175\n",
      "[-0.05747093  0.13053659 -0.48583406]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-12.3600\n",
      "entry_point=109.3000 exit_point=96.9900 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10480 Total reward: -12.7500 Training loss: 0.0589 Explore P: 0.0000 Reward mean: -0.7833 Reward std: 3.7169\n",
      "Episode: 10481 Total reward: 0.0000 Training loss: 0.0589 Explore P: 0.0000 Reward mean: -0.7833 Reward std: 3.7169\n",
      "Episode: 10482 Total reward: 3.7400 Training loss: 0.0498 Explore P: 0.0000 Reward mean: -0.7459 Reward std: 3.7433\n",
      "Episode: 10483 Total reward: 0.0000 Training loss: 0.0498 Explore P: 0.0000 Reward mean: -0.7459 Reward std: 3.7433\n",
      "Episode: 10484 Total reward: 4.9300 Training loss: 0.0494 Explore P: 0.0000 Reward mean: -0.6966 Reward std: 3.7850\n",
      "Episode: 10485 Total reward: 0.0000 Training loss: 0.0494 Explore P: 0.0000 Reward mean: -0.6966 Reward std: 3.7850\n",
      "Episode: 10486 Total reward: 0.0000 Training loss: 0.0494 Explore P: 0.0000 Reward mean: -0.6966 Reward std: 3.7850\n",
      "[-0.06122538  0.12960237 -0.4858871 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-8.5300\n",
      "entry_point=109.3000 exit_point=100.3300 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10487 Total reward: -8.9200 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7858 Reward std: 3.8717\n",
      "Episode: 10488 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7858 Reward std: 3.8717\n",
      "Episode: 10489 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7858 Reward std: 3.8717\n",
      "Episode: 10490 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7858 Reward std: 3.8717\n",
      "Episode: 10491 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7858 Reward std: 3.8717\n",
      "Episode: 10492 Total reward: 3.7400 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10493 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10494 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10495 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10496 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10497 Total reward: 0.0000 Training loss: 0.0449 Explore P: 0.0000 Reward mean: -0.7484 Reward std: 3.8971\n",
      "Episode: 10498 Total reward: 3.7100 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10499 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10500 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10501 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10502 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10503 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10504 Total reward: 0.0000 Training loss: 0.0417 Explore P: 0.0000 Reward mean: -0.7113 Reward std: 3.9216\n",
      "Episode: 10505 Total reward: 0.9800 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.7015 Reward std: 3.9246\n",
      "Episode: 10506 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.6731 Reward std: 3.9193\n",
      "Episode: 10507 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.6731 Reward std: 3.9193\n",
      "Episode: 10508 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.4877 Reward std: 3.4840\n",
      "Episode: 10509 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.4877 Reward std: 3.4840\n",
      "Episode: 10510 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.4877 Reward std: 3.4840\n",
      "Episode: 10511 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.5475 Reward std: 3.4233\n",
      "Episode: 10512 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.5475 Reward std: 3.4233\n",
      "Episode: 10513 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.6058 Reward std: 3.3633\n",
      "Episode: 10514 Total reward: 0.0000 Training loss: 0.0453 Explore P: 0.0000 Reward mean: -0.6058 Reward std: 3.3633\n",
      "[-0.10714916  0.03359836 -0.43088293]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-10.0800\n",
      "entry_point=109.3000 exit_point=99.8600 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10515 Total reward: -10.4700 Training loss: 0.0514 Explore P: 0.0000 Reward mean: -0.5895 Reward std: 3.3111\n",
      "[-0.06404602  0.12907454 -0.4861411 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-8.2800\n",
      "entry_point=109.3000 exit_point=100.7800 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10516 Total reward: -8.6700 Training loss: 0.0481 Explore P: 0.0000 Reward mean: -0.6762 Reward std: 3.4067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10517 Total reward: 0.0000 Training loss: 0.0481 Explore P: 0.0000 Reward mean: -0.6762 Reward std: 3.4067\n",
      "Episode: 10518 Total reward: 7.4900 Training loss: 0.0456 Explore P: 0.0000 Reward mean: -0.6013 Reward std: 3.5017\n",
      "[-0.16096686  0.05047491 -0.45877796]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.5800\n",
      "entry_point=106.4600 exit_point=105.2100 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10519 Total reward: -1.6500 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.6178 Reward std: 3.5027\n",
      "Episode: 10520 Total reward: 0.0000 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.6178 Reward std: 3.5027\n",
      "Episode: 10521 Total reward: 0.0000 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.4676 Reward std: 3.1900\n",
      "Episode: 10522 Total reward: 0.0000 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.3818 Reward std: 3.0843\n",
      "Episode: 10523 Total reward: 0.0000 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.3818 Reward std: 3.0843\n",
      "Episode: 10524 Total reward: 7.8100 Training loss: 0.0488 Explore P: 0.0000 Reward mean: -0.3037 Reward std: 3.1901\n",
      "Episode: 10525 Total reward: 0.0000 Training loss: 0.0488 Explore P: 0.0000 Reward mean: -0.3037 Reward std: 3.1901\n",
      "Episode: 10526 Total reward: 0.0000 Training loss: 0.0488 Explore P: 0.0000 Reward mean: -0.3037 Reward std: 3.1901\n",
      "Episode: 10527 Total reward: 0.2400 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.2588 Reward std: 3.1657\n",
      "Episode: 10528 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.2588 Reward std: 3.1657\n",
      "Episode: 10529 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.2085 Reward std: 3.1292\n",
      "Episode: 10530 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.2085 Reward std: 3.1292\n",
      "[-0.09795843  0.10408053 -0.47501782]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-5.2800\n",
      "entry_point=109.3000 exit_point=106.9100 max_point=110.2800 min_point=97.0400\n",
      "Episode: 10531 Total reward: -5.6700 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.2652 Reward std: 3.1760\n",
      "Episode: 10532 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.2652 Reward std: 3.1760\n",
      "Episode: 10533 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.2652 Reward std: 3.1760\n",
      "Episode: 10534 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.2652 Reward std: 3.1760\n",
      "Episode: 10535 Total reward: 0.0000 Training loss: 0.0477 Explore P: 0.0000 Reward mean: -0.2652 Reward std: 3.1760\n",
      "[-0.1634641   0.04198092 -0.4538924 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.6300\n",
      "entry_point=106.4600 exit_point=103.4500 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10536 Total reward: -2.7000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10537 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10538 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10539 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10540 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10541 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2922 Reward std: 3.1851\n",
      "Episode: 10542 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2724 Reward std: 3.1807\n",
      "Episode: 10543 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2724 Reward std: 3.1807\n",
      "Episode: 10544 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.2724 Reward std: 3.1807\n",
      "Episode: 10545 Total reward: 2.0800 Training loss: 0.0529 Explore P: 0.0000 Reward mean: -0.2516 Reward std: 3.1892\n",
      "Episode: 10546 Total reward: 0.0000 Training loss: 0.0529 Explore P: 0.0000 Reward mean: -0.2516 Reward std: 3.1892\n",
      "Episode: 10547 Total reward: 0.0000 Training loss: 0.0529 Explore P: 0.0000 Reward mean: -0.2516 Reward std: 3.1892\n",
      "Episode: 10548 Total reward: 0.0000 Training loss: 0.0529 Explore P: 0.0000 Reward mean: -0.2516 Reward std: 3.1892\n",
      "[-0.01025408  0.15491855 -0.4157818 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.6900\n",
      "entry_point=94.2800 exit_point=90.0700 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10549 Total reward: -3.6600 Training loss: 0.0567 Explore P: 0.0000 Reward mean: -0.2882 Reward std: 3.2070\n",
      "[-0.00979507  0.16649729 -0.4214005 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-6.5100\n",
      "entry_point=94.2800 exit_point=86.4200 max_point=100.7800 min_point=86.3100\n",
      "Episode: 10550 Total reward: -7.4800 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3630 Reward std: 3.2857\n",
      "Episode: 10551 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3630 Reward std: 3.2857\n",
      "Episode: 10552 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3285 Reward std: 3.2712\n",
      "Episode: 10553 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3285 Reward std: 3.2712\n",
      "Episode: 10554 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3285 Reward std: 3.2712\n",
      "Episode: 10555 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.3285 Reward std: 3.2712\n",
      "[-0.10518711  0.06661785 -0.4522911 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.9100\n",
      "entry_point=109.3000 exit_point=107.3700 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10556 Total reward: -1.3000 Training loss: 0.0479 Explore P: 0.0000 Reward mean: -0.3040 Reward std: 3.2546\n",
      "Episode: 10557 Total reward: 0.0000 Training loss: 0.0479 Explore P: 0.0000 Reward mean: -0.2682 Reward std: 3.2380\n",
      "Episode: 10558 Total reward: 1.8600 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.2496 Reward std: 3.2448\n",
      "Episode: 10559 Total reward: 0.0000 Training loss: 0.0516 Explore P: 0.0000 Reward mean: -0.3274 Reward std: 3.1431\n",
      "Episode: 10560 Total reward: 3.4600 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2326 Reward std: 3.1127\n",
      "Episode: 10561 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2326 Reward std: 3.1127\n",
      "Episode: 10562 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2326 Reward std: 3.1127\n",
      "Episode: 10563 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2326 Reward std: 3.1127\n",
      "Episode: 10564 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2326 Reward std: 3.1127\n",
      "Episode: 10565 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2976 Reward std: 3.0385\n",
      "Episode: 10566 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2976 Reward std: 3.0385\n",
      "Episode: 10567 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2976 Reward std: 3.0385\n",
      "Episode: 10568 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2976 Reward std: 3.0385\n",
      "Episode: 10569 Total reward: 0.0000 Training loss: 0.0499 Explore P: 0.0000 Reward mean: -0.2976 Reward std: 3.0385\n",
      "Episode: 10570 Total reward: 3.7100 Training loss: 0.0523 Explore P: 0.0000 Reward mean: -0.2605 Reward std: 3.0644\n",
      "[-0.17178865  0.00375792 -0.43105072]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.9900\n",
      "entry_point=106.4600 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "Episode: 10571 Total reward: -4.0600 Training loss: 0.0581 Explore P: 0.0000 Reward mean: -0.3011 Reward std: 3.0875\n",
      "[-0.08942871  0.12340486 -0.48668975]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-18.0700\n",
      "entry_point=109.3000 exit_point=90.4300 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10572 Total reward: -18.4600 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.4857 Reward std: 3.5770\n",
      "Episode: 10573 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.4857 Reward std: 3.5770\n",
      "Episode: 10574 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.4857 Reward std: 3.5770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10575 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.4857 Reward std: 3.5770\n",
      "Episode: 10576 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.3694 Reward std: 3.3973\n",
      "Episode: 10577 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.3503 Reward std: 3.3940\n",
      "Episode: 10578 Total reward: 0.0000 Training loss: 0.0483 Explore P: 0.0000 Reward mean: -0.4204 Reward std: 3.3127\n",
      "Episode: 10579 Total reward: 3.6300 Training loss: 0.0574 Explore P: 0.0000 Reward mean: -0.3841 Reward std: 3.3369\n",
      "Episode: 10580 Total reward: 6.9200 Training loss: 0.0544 Explore P: 0.0000 Reward mean: -0.1874 Reward std: 3.1781\n",
      "Episode: 10581 Total reward: 2.9100 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1583 Reward std: 3.1930\n",
      "Episode: 10582 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1957 Reward std: 3.1689\n",
      "Episode: 10583 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1957 Reward std: 3.1689\n",
      "Episode: 10584 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.2450 Reward std: 3.1269\n",
      "Episode: 10585 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.2450 Reward std: 3.1269\n",
      "Episode: 10586 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.2450 Reward std: 3.1269\n",
      "Episode: 10587 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1558 Reward std: 3.0029\n",
      "Episode: 10588 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1558 Reward std: 3.0029\n",
      "Episode: 10589 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1558 Reward std: 3.0029\n",
      "Episode: 10590 Total reward: 0.0000 Training loss: 0.0510 Explore P: 0.0000 Reward mean: -0.1558 Reward std: 3.0029\n",
      "[-0.09446905  0.12239945 -0.48688403]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-15.9600\n",
      "entry_point=109.3000 exit_point=92.7200 max_point=110.2800 min_point=92.5500\n",
      "Episode: 10591 Total reward: -16.3500 Training loss: 0.0531 Explore P: 0.0000 Reward mean: -0.3193 Reward std: 3.4078\n",
      "Episode: 10592 Total reward: 0.0000 Training loss: 0.0531 Explore P: 0.0000 Reward mean: -0.3567 Reward std: 3.3834\n",
      "[-0.08584642  0.12463143 -0.48715436]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-19.7400\n",
      "entry_point=109.3000 exit_point=88.9000 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10593 Total reward: -20.1300 Training loss: 0.0669 Explore P: 0.0000 Reward mean: -0.5580 Reward std: 3.9135\n",
      "Episode: 10594 Total reward: 0.0000 Training loss: 0.0669 Explore P: 0.0000 Reward mean: -0.5580 Reward std: 3.9135\n",
      "Episode: 10595 Total reward: 0.0000 Training loss: 0.0669 Explore P: 0.0000 Reward mean: -0.5580 Reward std: 3.9135\n",
      "Episode: 10596 Total reward: 0.0000 Training loss: 0.0669 Explore P: 0.0000 Reward mean: -0.5580 Reward std: 3.9135\n",
      "Episode: 10597 Total reward: 0.0000 Training loss: 0.0669 Explore P: 0.0000 Reward mean: -0.5580 Reward std: 3.9135\n",
      "Episode: 10598 Total reward: 1.4600 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10599 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10600 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10601 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10602 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10603 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10604 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5805 Reward std: 3.8954\n",
      "Episode: 10605 Total reward: 0.0000 Training loss: 0.0558 Explore P: 0.0000 Reward mean: -0.5903 Reward std: 3.8927\n",
      "[-0.11171179  0.04876944 -0.4423327 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-5.0800\n",
      "entry_point=109.3000 exit_point=104.8600 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10606 Total reward: -5.4700 Training loss: 0.0496 Explore P: 0.0000 Reward mean: -0.6450 Reward std: 3.9223\n",
      "Episode: 10607 Total reward: 0.0000 Training loss: 0.0496 Explore P: 0.0000 Reward mean: -0.6450 Reward std: 3.9223\n",
      "Episode: 10608 Total reward: 0.0000 Training loss: 0.0496 Explore P: 0.0000 Reward mean: -0.6450 Reward std: 3.9223\n",
      "Episode: 10609 Total reward: 4.2500 Training loss: 0.0540 Explore P: 0.0000 Reward mean: -0.6025 Reward std: 3.9520\n",
      "Episode: 10610 Total reward: 0.0000 Training loss: 0.0540 Explore P: 0.0000 Reward mean: -0.6025 Reward std: 3.9520\n",
      "[-0.175145    0.00389132 -0.4320581 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.9900\n",
      "entry_point=106.4600 exit_point=101.6300 max_point=110.9400 min_point=102.2500\n",
      "Episode: 10611 Total reward: -4.0600 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.6431 Reward std: 3.9664\n",
      "Episode: 10612 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.6431 Reward std: 3.9664\n",
      "Episode: 10613 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.6431 Reward std: 3.9664\n",
      "Episode: 10614 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.6431 Reward std: 3.9664\n",
      "Episode: 10615 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.5384 Reward std: 3.8418\n",
      "Episode: 10616 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.4517 Reward std: 3.7542\n",
      "Episode: 10617 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.4517 Reward std: 3.7542\n",
      "Episode: 10618 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.5266 Reward std: 3.6687\n",
      "Episode: 10619 Total reward: 0.0000 Training loss: 0.0638 Explore P: 0.0000 Reward mean: -0.5101 Reward std: 3.6674\n",
      "[-0.07869202  0.12673262 -0.48769045]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-10.5100\n",
      "entry_point=109.3000 exit_point=97.2400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10620 Total reward: -10.9000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6191 Reward std: 3.8098\n",
      "Episode: 10621 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6191 Reward std: 3.8098\n",
      "Episode: 10622 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6191 Reward std: 3.8098\n",
      "Episode: 10623 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6191 Reward std: 3.8098\n",
      "Episode: 10624 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6972 Reward std: 3.7151\n",
      "Episode: 10625 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6972 Reward std: 3.7151\n",
      "Episode: 10626 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6972 Reward std: 3.7151\n",
      "Episode: 10627 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6996 Reward std: 3.7145\n",
      "Episode: 10628 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6996 Reward std: 3.7145\n",
      "Episode: 10629 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6996 Reward std: 3.7145\n",
      "Episode: 10630 Total reward: 0.0000 Training loss: 0.0536 Explore P: 0.0000 Reward mean: -0.6996 Reward std: 3.7145\n",
      "[-0.11596692  0.02924782 -0.43063685]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-11.3800\n",
      "entry_point=109.3000 exit_point=101.0700 max_point=110.1300 min_point=101.6600\n",
      "Episode: 10631 Total reward: -11.7700 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7606 Reward std: 3.8435\n",
      "Episode: 10632 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7606 Reward std: 3.8435\n",
      "Episode: 10633 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7606 Reward std: 3.8435\n",
      "Episode: 10634 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7606 Reward std: 3.8435\n",
      "Episode: 10635 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7606 Reward std: 3.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10636 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7336 Reward std: 3.8393\n",
      "Episode: 10637 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7336 Reward std: 3.8393\n",
      "Episode: 10638 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7336 Reward std: 3.8393\n",
      "Episode: 10639 Total reward: 0.0000 Training loss: 0.0644 Explore P: 0.0000 Reward mean: -0.7336 Reward std: 3.8393\n",
      "[-0.17211527  0.03016651 -0.44857383]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-3.7800\n",
      "entry_point=106.4600 exit_point=103.9100 max_point=110.9400 min_point=99.8100\n",
      "Episode: 10640 Total reward: -2.8500 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7621 Reward std: 3.8443\n",
      "Episode: 10641 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7621 Reward std: 3.8443\n",
      "Episode: 10642 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7621 Reward std: 3.8443\n",
      "Episode: 10643 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7621 Reward std: 3.8443\n",
      "Episode: 10644 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7621 Reward std: 3.8443\n",
      "Episode: 10645 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7829 Reward std: 3.8345\n",
      "Episode: 10646 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7829 Reward std: 3.8345\n",
      "Episode: 10647 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7829 Reward std: 3.8345\n",
      "Episode: 10648 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7829 Reward std: 3.8345\n",
      "Episode: 10649 Total reward: 0.0000 Training loss: 0.0571 Explore P: 0.0000 Reward mean: -0.7463 Reward std: 3.8243\n",
      "[-0.01566691  0.14769995 -0.41468853]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4800\n",
      "entry_point=94.2800 exit_point=90.6800 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10650 Total reward: -3.4500 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.7060 Reward std: 3.7740\n",
      "Episode: 10651 Total reward: 0.0000 Training loss: 0.0546 Explore P: 0.0000 Reward mean: -0.7060 Reward std: 3.7740\n",
      "[ 0.08724336  0.1360282  -0.4444909 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.5800\n",
      "entry_point=97.5300 exit_point=96.1400 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10652 Total reward: -2.0700 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.7267 Reward std: 3.7758\n",
      "Episode: 10653 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.7267 Reward std: 3.7758\n",
      "Episode: 10654 Total reward: 0.0000 Training loss: 0.0522 Explore P: 0.0000 Reward mean: -0.7267 Reward std: 3.7758\n",
      "[-0.11946637  0.01467669 -0.42200345]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.4500\n",
      "entry_point=109.3000 exit_point=106.6600 max_point=110.1300 min_point=105.9300\n",
      "Episode: 10655 Total reward: -2.8400 Training loss: 0.0633 Explore P: 0.0000 Reward mean: -0.7551 Reward std: 3.7809\n",
      "Episode: 10656 Total reward: 0.0000 Training loss: 0.0633 Explore P: 0.0000 Reward mean: -0.7421 Reward std: 3.7812\n",
      "Episode: 10657 Total reward: 0.0000 Training loss: 0.0633 Explore P: 0.0000 Reward mean: -0.7421 Reward std: 3.7812\n",
      "Episode: 10658 Total reward: 0.0000 Training loss: 0.0633 Explore P: 0.0000 Reward mean: -0.7607 Reward std: 3.7729\n",
      "[-0.1118602   0.0667361  -0.45422348]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-0.9100\n",
      "entry_point=109.3000 exit_point=107.3700 max_point=110.1300 min_point=97.0400\n",
      "Episode: 10659 Total reward: -1.3000 Training loss: 0.0533 Explore P: 0.0000 Reward mean: -0.7737 Reward std: 3.7725\n",
      "Episode: 10660 Total reward: 0.0000 Training loss: 0.0533 Explore P: 0.0000 Reward mean: -0.8083 Reward std: 3.7493\n",
      "Episode: 10661 Total reward: 0.0000 Training loss: 0.0533 Explore P: 0.0000 Reward mean: -0.8083 Reward std: 3.7493\n",
      "Episode: 10662 Total reward: 0.0000 Training loss: 0.0533 Explore P: 0.0000 Reward mean: -0.8083 Reward std: 3.7493\n",
      "Episode: 10663 Total reward: 0.0000 Training loss: 0.0533 Explore P: 0.0000 Reward mean: -0.8083 Reward std: 3.7493\n",
      "[-0.17818972  0.00162849 -0.43142742]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.6000\n",
      "entry_point=106.4600 exit_point=104.6200 max_point=110.9400 min_point=103.8300\n",
      "Episode: 10664 Total reward: -1.6700 Training loss: 0.0545 Explore P: 0.0000 Reward mean: -0.8250 Reward std: 3.7494\n",
      "Episode: 10665 Total reward: 0.0000 Training loss: 0.0545 Explore P: 0.0000 Reward mean: -0.8250 Reward std: 3.7494\n",
      "Episode: 10666 Total reward: 0.0000 Training loss: 0.0545 Explore P: 0.0000 Reward mean: -0.8250 Reward std: 3.7494\n",
      "Episode: 10667 Total reward: 0.0000 Training loss: 0.0545 Explore P: 0.0000 Reward mean: -0.8250 Reward std: 3.7494\n",
      "[-0.01782184  0.13321164 -0.4083868 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.2500\n",
      "entry_point=94.2800 exit_point=90.6400 max_point=100.7800 min_point=87.4700\n",
      "Episode: 10668 Total reward: -3.2200 Training loss: 0.0509 Explore P: 0.0000 Reward mean: -0.8572 Reward std: 3.7560\n",
      "Episode: 10669 Total reward: 0.0000 Training loss: 0.0509 Explore P: 0.0000 Reward mean: -0.8572 Reward std: 3.7560\n",
      "[-0.08941699  0.12455526 -0.4881271 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-19.5800\n",
      "entry_point=109.3000 exit_point=88.9000 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10670 Total reward: -19.9700 Training loss: 0.0604 Explore P: 0.0000 Reward mean: -1.0940 Reward std: 4.1828\n",
      "[-0.17601523  0.02295324 -0.4448704 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-7.5800\n",
      "entry_point=106.4600 exit_point=100.6900 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10671 Total reward: -6.6500 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -1.1199 Reward std: 4.2091\n",
      "Episode: 10672 Total reward: 0.0000 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -0.9353 Reward std: 3.8325\n",
      "Episode: 10673 Total reward: 0.0000 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -0.9353 Reward std: 3.8325\n",
      "Episode: 10674 Total reward: 0.0000 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -0.9353 Reward std: 3.8325\n",
      "Episode: 10675 Total reward: 0.0000 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -0.9353 Reward std: 3.8325\n",
      "Episode: 10676 Total reward: 7.4900 Training loss: 0.0569 Explore P: 0.0000 Reward mean: -0.8604 Reward std: 3.9222\n",
      "Episode: 10677 Total reward: 0.0000 Training loss: 0.0569 Explore P: 0.0000 Reward mean: -0.8604 Reward std: 3.9222\n",
      "Episode: 10678 Total reward: 0.0000 Training loss: 0.0569 Explore P: 0.0000 Reward mean: -0.8604 Reward std: 3.9222\n",
      "Episode: 10679 Total reward: 0.0000 Training loss: 0.0569 Explore P: 0.0000 Reward mean: -0.8967 Reward std: 3.8971\n",
      "[-0.17827106  0.0128985  -0.43885723]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.6700\n",
      "entry_point=106.4600 exit_point=107.2800 max_point=110.9400 min_point=100.8300\n",
      "Episode: 10680 Total reward: -0.7400 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -0.9733 Reward std: 3.8172\n",
      "Episode: 10681 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10682 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10683 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10684 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10685 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10686 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10687 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10688 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10689 Total reward: 0.0000 Training loss: 0.0572 Explore P: 0.0000 Reward mean: -1.0024 Reward std: 3.7985\n",
      "Episode: 10690 Total reward: 6.3300 Training loss: 0.0519 Explore P: 0.0000 Reward mean: -0.9391 Reward std: 3.8668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10691 Total reward: 0.0000 Training loss: 0.0519 Explore P: 0.0000 Reward mean: -0.7756 Reward std: 3.5440\n",
      "[-0.10702322  0.116166   -0.48541582]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-10.3200\n",
      "entry_point=109.3000 exit_point=97.9400 max_point=110.2800 min_point=93.3100\n",
      "Episode: 10692 Total reward: -10.7100 Training loss: 0.0584 Explore P: 0.0000 Reward mean: -0.8827 Reward std: 3.6782\n",
      "Episode: 10693 Total reward: 0.0000 Training loss: 0.0584 Explore P: 0.0000 Reward mean: -0.6814 Reward std: 3.1292\n",
      "Episode: 10694 Total reward: 0.0000 Training loss: 0.0584 Explore P: 0.0000 Reward mean: -0.6814 Reward std: 3.1292\n",
      "[-0.12255859  0.0163531  -0.42396384]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-1.5300\n",
      "entry_point=109.3000 exit_point=106.9500 max_point=110.1300 min_point=105.9300\n",
      "Episode: 10695 Total reward: -1.9200 Training loss: 0.0574 Explore P: 0.0000 Reward mean: -0.7006 Reward std: 3.1308\n",
      "Episode: 10696 Total reward: 0.0000 Training loss: 0.0574 Explore P: 0.0000 Reward mean: -0.7006 Reward std: 3.1308\n",
      "[ 0.08846395  0.13695821 -0.4452916 ]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-4.9000\n",
      "entry_point=97.5300 exit_point=92.8200 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10697 Total reward: -5.3900 Training loss: 0.0534 Explore P: 0.0000 Reward mean: -0.7545 Reward std: 3.1645\n",
      "[-0.09672259  0.12319931 -0.48858857]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-16.8800\n",
      "entry_point=109.3000 exit_point=91.0100 max_point=110.2800 min_point=87.4700\n",
      "Episode: 10698 Total reward: -17.2700 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "Episode: 10699 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "Episode: 10700 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "Episode: 10701 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "Episode: 10702 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "Episode: 10703 Total reward: 0.0000 Training loss: 0.0508 Explore P: 0.0000 Reward mean: -0.9418 Reward std: 3.5578\n",
      "[-0.18217157  0.00115362 -0.43221185]\n",
      "entry_state_action=0, initial_action=0, action=1, position reward=-2.7700\n",
      "entry_point=106.4600 exit_point=103.8300 max_point=110.9400 min_point=104.7200\n",
      "Episode: 10704 Total reward: -1.8400 Training loss: 0.0498 Explore P: 0.0000 Reward mean: -0.9602 Reward std: 3.5576\n",
      "Episode: 10705 Total reward: 0.0000 Training loss: 0.0498 Explore P: 0.0000 Reward mean: -0.9602 Reward std: 3.5576\n",
      "Episode: 10706 Total reward: 0.0000 Training loss: 0.0498 Explore P: 0.0000 Reward mean: -0.9055 Reward std: 3.5298\n",
      "Episode: 10707 Total reward: -0.2600 Training loss: 0.0542 Explore P: 0.0000 Reward mean: -0.9081 Reward std: 3.5292\n",
      "Episode: 10708 Total reward: 0.0000 Training loss: 0.0542 Explore P: 0.0000 Reward mean: -0.9081 Reward std: 3.5292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-38b089a1c2ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Take action, get new state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/Documents/MachineLearning/ReinforcementLearning/FXTrading/envs/TradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/Documents/MachineLearning/ReinforcementLearning/FXTrading/envs/TradingEnv.py\u001b[0m in \u001b[0;36mcalc_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0maccelaration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdownfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mupfall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mrest_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'getitem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             raise TypeError('Indexing a Series with DataFrame is not '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             return self._constructor(self._data.get_slice(indexer),\n\u001b[0m\u001b[1;32m    978\u001b[0m                                      fastpath=True).__finalize__(self)\n\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mget_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         return self.__class__(self._block._slice(slobj),\n\u001b[0;32m-> 1511\u001b[0;31m                               self.index[slobj], fastpath=True)\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msuper_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRangeIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 raise IndexError(\"only integers, slices (`:`), \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_list = []\n",
    "opt = optim.Adam(mainQN.parameters(), learning_rate)\n",
    "\n",
    "outputs = np.empty([1,11])\n",
    "episode_reward_deque = deque(maxlen=100)\n",
    "\n",
    "count_stop = 0\n",
    "for ep in range(10000, train_episodes+1000):\n",
    "    total_reward = 0\n",
    "    current_position_reward = 0\n",
    "    last_action = 0\n",
    "    initial_action = 0\n",
    "    t = 0\n",
    "    # Start new episode\n",
    "    observation = env.reset()\n",
    "    state = make_state(observation, means, stds)\n",
    "    max_point = info\n",
    "    min_point = info\n",
    "            \n",
    "    for t in range(max_steps):\n",
    "        # Explore or Exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*(ep*max_steps+t))\n",
    "        if ep > train_episodes:\n",
    "            explore_p = 0.0\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            Qs = mainQN(Variable(torch.FloatTensor(state))).data.numpy()\n",
    "            action = np.argmax(Qs)\n",
    "            \n",
    "        result = np.hstack((state, mainQN(Variable(torch.FloatTensor(state))).data.numpy()))\n",
    "        outputs = np.vstack((outputs, result))\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        next_state = make_state(next_observation, means, stds)\n",
    "\n",
    "        total_reward += reward\n",
    "        current_position_reward += reward\n",
    "        \n",
    "        if last_action != action or done:\n",
    "            if last_action != 0:\n",
    "                clipped_reward = current_position_reward/(t-entry_t)\n",
    "                memory.add((entry_state, last_action, clipped_reward, next_state, done))\n",
    "                if current_position_reward > 0:\n",
    "                    for val in keep_list:\n",
    "                        memory.add((val[0], val[1], clipped_reward, val[2], done))\n",
    "                else:\n",
    "                    print(mainQN(Variable(torch.FloatTensor(entry_state))).data.numpy())\n",
    "                    print('entry_state_action={}, initial_action={}, action={}, position reward={:.4f}'.format(np.argmax(entry_state[0:3]), initial_action, last_action, current_position_reward))\n",
    "                    print('entry_point={:.4f} exit_point={:.4f} max_point={:.4f} min_point={:.4f}'.format(entry_point, info, max_point, min_point))\n",
    "                keep_list = [] \n",
    "                loss = optimize(memory, opt, batch_size)\n",
    "                \n",
    "            entry_state = state\n",
    "            initial_action = last_action\n",
    "            entry_t = t\n",
    "            current_position_reward = 0.0\n",
    "            max_point = info\n",
    "            min_point = info\n",
    "            entry_point = info\n",
    "        else:\n",
    "            keep_list.append([state, action, next_state])\n",
    "            max_point = info if info > max_point else max_point\n",
    "            min_point = info if info < min_point else min_point\n",
    "            \n",
    "        last_action = action\n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            episode_reward_deque.append(total_reward)\n",
    "            # the episode ends so no next state\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {:.4f}'.format(total_reward),\n",
    "                  'Training loss: {:.4f}'.format(loss.data.numpy()),\n",
    "                  'Explore P: {:.4f}'.format(explore_p),\n",
    "                  'Reward mean: {:.4f}'.format(np.array(list(episode_reward_deque)).mean()),\n",
    "                  'Reward std: {:.4f}'.format(np.array(list(episode_reward_deque)).std()))\n",
    "            rewards_list.append((ep, total_reward))\n",
    "            break\n",
    "df = pd.DataFrame(outputs)\n",
    "df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Total Reward')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0126,  0.1031,  0.9911], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainQN(Variable(torch.FloatTensor([0,0,1,0.2,0.1,0.1,0.4,0.9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999.7061111111111"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rewards_list[1990:2000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1991, 18.950000000000003),\n",
       " (1992, 20.279999999999987),\n",
       " (1993, 5.310000000000002),\n",
       " (1994, 3.039999999999992),\n",
       " (1995, -26.14),\n",
       " (1996, 0),\n",
       " (1997, 0),\n",
       " (1998, 18.269999999999996),\n",
       " (1999, 0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_list[1990:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
